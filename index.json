{
    "content/_includes/data-source-configuration.html":  {
                                                             "href":  "content/_includes/data-source-configuration.html",
                                                             "title":  "Data source configuration with required parameters",
                                                             "keywords":  "Data source configuration with required parameters { \"inputDirectory\": \"C:\\\\InputDirectory\", \"outputDirectory\": \"C:\\\\OutputDirectory\" } Data source configuration with optional parameters (Windows) { \"friendlyName\": \"Weather\", \"inputDirectory\": \"C:\\\\InputDirectory\", \"fileNameFilter\": \"*.csv\", \"outputDirectory\": \"C:\\\\OutputDirectory\", \"hasHeader\": true, \"culture\": \"fr-FR\", \"timeZone\": \"Europe/Paris\", \"Europe Paris\", \"compression\": \"Zip\", \"fieldSeparator\": \"|\", \"purgeDelay\": \"5.04:03:02.0000000\" } Data source configuration with optional parameters (Linux) { \"friendlyName\": \"NA-Pumps\", \"inputDirectory\": \"/usr/mnt/InputDir/\", \" usr mnt InputDir \", \"fileNameFilter\": \"*\", \"outputDirectory\": \"/usr/mnt/OutputDir/\", \" usr mnt OutputDir \", \"hasHeader\": true, \"format\": \"Csv\", \"compression\": \"None\", \"encoding\": \"UTF8\", \"fieldSeparator\": \",\", \"lineSeparator\": \"\\n\", \"defaultStreamIdPattern\": \"{FriendlyName}.{ValueField}\", \"purgeDelay\": \"5.04:03:02.0000000\" }"
                                                         },
    "content/configuration/configuration.html":  {
                                                     "href":  "content/configuration/configuration.html",
                                                     "title":  "Configuration",
                                                     "keywords":  "Configuration Before you can begin using the adapter, you must configure the data source and data selection using the adapter\u0027s REST API. \u003c!-- I would really like to see the configurations listed in order here, so that I have a road map of what I need to do. You could then indicate which ones are optional and get rid of the some are required and others are optional sentences. --\u003e The examples in the configuration topics use cURL, a commonly available tool on both Windows and Linux. You can configure the adapter with any programming language or tool that supports making REST calls or with the EdgeCmd utility. For more information, see the EdgeCmd utility documentation . To validate successful configurations, you can perform data retrieval ( GET commands) with a browser, if available, on your device. For more information on PI Adapter configuration tools, see Configuration tools . Quick Start This Quick Start guides you through setup of each configuration file available for the PI Adapter for Structured Data Files. As you complete each step, perform each configuration to establish a data flow from a data source to one or more endpoints. Some configurations are optional. Important: If you want to complete the optional configurations in each step below, complete those tasks before the required tasks. Configure system components. Configuration Description Required Optional system components Defines each component instance on the system host. ??? Configure egress endpoints. Configuration Description Required Optional network proxy If there is a proxy between the adapter and your egress endpoints, define it using this configuration. ??? egress endpoints Defines the final locations that the adapter sends OMF data. ??? Optional: Configure health endpoints. Configuration Description Required Optional general Defines whether diagnostic information is included in health data. Requires a health endpoint. ??? health endpoint Defines endpoint where PI adapters produce and store health data. ??? Optional: Complete buffering and logging configurations. Configuration Description Required Optional buffering Defines whether data buffering is enabled, the volume of data buffered, and the location of buffered files. ??? logging Defines custom logging options. ??? Complete data source configurations. The adapter starts collecting data immediately after you complete the data source configuration. Important: Complete all other adapter configuration steps before data source configuration. This practice ensures a complete history of data collection. Configuration Description Required Optional data filter A reusable file that defines what data within the data source files are received. Can be used in conjunction with data selection configurations. ??? data selection Defines what data within the data source files are received. ??? data source Defines the source that the adapter receives data files from. ???"
                                                 },
    "content/configuration/configuration-examples.html":  {
                                                              "href":  "content/configuration/configuration-examples.html",
                                                              "title":  "Configuration examples",
                                                              "keywords":  "Configuration examples The following JSON samples provide examples for all configurations available for PI Adapter for Structured Data Files. System components configuration with two Structured Data Files adapter instances [ { \"ComponentId\": \"StructuredDataFiles1\", \"ComponentType\": \"StructuredDataFiles\" }, { \"ComponentId\": \"StructuredDataFiles2\", \"ComponentType\": \"StructuredDataFiles\" }, { \"ComponentId\": \"OmfEgress\", \"ComponentType\": \"OmfEgress\" } ] Adapter configuration The following example is a complete configuration for the PI Adapter for Structured Data Files. { \"StructuredDataFiles1\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"DataSource\": { \"FriendlyName\": \"Weather\", \"InputDirectory\": \"C:\\\\InputDirectory\", \"FileNameFilter\": \"*.csv\", \"OutputDirectory\": \"C:\\\\OutputDirectory\", \"HasHeader\": true, \"Culture\": \"fr-FR\", \"TimeZone\": \"Europe/Paris\", \"Europe Paris\", \"Compression\": \"Zip\", \"FieldSeparator\": \"|\" }, \"DataSelection\": [ { \"Selected\": true, \"Name\": \"Name\", \"StreamId\": \"StreamId\", \"ValueField\": \"FanSpeed\", \"IndexField\": \"FileCreationTime\", \"IndexFormat\": \"mm/dd/yy\", \"mm dd yy\", \"DataType\": \"Int32\" } ] }, \"System\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"HealthEndpoints\": [], \"Diagnostics\": { \"enableDiagnostics\": true }, \"Components\": [ { \"componentId\": \"Egress\", \"componentType\": \"OmfEgress\" }, { \"componentId\": \"StructuredDataFiles1\", \"componentType\": \"StructuredDataFiles\" } ], \"Buffering\": { \"BufferLocation\": \"C:/ProgramData/OSIsoft/Adapters/StructuredDataFiles/Buffers\", \"C: ProgramData OSIsoft Adapters StructuredDataFiles Buffers\", \"MaxBufferSizeMB\": -1, \"EnablePersistentBuffering\": true }, \"General\": { \"enableDiagnostics\": true, \"metadataLevel\": \"Medium\" } }, \"OmfEgress\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"DataEndpoints\": [ { \"id\": \"WebAPI EndPoint\", \"endpoint\": \"https://PIWEBAPIServer/piwebapi/omf\", \"https:  PIWEBAPIServer piwebapi omf\", \"userName\": \"USERNAME\", \"password\": \"PASSWORD\" }, { \"id\": \"OCS Endpoint\", \"endpoint\": \"https://OCSEndpoint/omf\", \"https:  OCSEndpoint omf\", \"clientId\": \"CLIENTID\", \"clientSecret\": \"CLIENTSECRET\" } ] }, \"DataFilters\": [ { \"id\": \"DuplicateData\", \"absoluteDeadband\": 0, \"percentChange\": null, \"expirationPeriod\": \"1:00:00\" } ] } Data source configuration The following are representations of data source configurations for the Structured Data Files adapter. Data source configuration with required parameters { \"inputDirectory\": \"C:\\\\InputDirectory\", \"outputDirectory\": \"C:\\\\OutputDirectory\" } Data source configuration with optional parameters (Windows) { \"friendlyName\": \"Weather\", \"inputDirectory\": \"C:\\\\InputDirectory\", \"fileNameFilter\": \"*.csv\", \"outputDirectory\": \"C:\\\\OutputDirectory\", \"hasHeader\": true, \"culture\": \"fr-FR\", \"timeZone\": \"Europe/Paris\", \"Europe Paris\", \"compression\": \"Zip\", \"fieldSeparator\": \"|\", \"purgeDelay\": \"5.04:03:02.0000000\" } Data source configuration with optional parameters (Linux) { \"friendlyName\": \"NA-Pumps\", \"inputDirectory\": \"/usr/mnt/InputDir/\", \" usr mnt InputDir \", \"fileNameFilter\": \"*\", \"outputDirectory\": \"/usr/mnt/OutputDir/\", \" usr mnt OutputDir \", \"hasHeader\": true, \"format\": \"Csv\", \"compression\": \"None\", \"encoding\": \"UTF8\", \"fieldSeparator\": \",\", \"lineSeparator\": \"\\n\", \"defaultStreamIdPattern\": \"{FriendlyName}.{ValueField}\", \"purgeDelay\": \"5.04:03:02.0000000\" }"
                                                          },
    "content/configuration/data-selection.html":  {
                                                      "href":  "content/configuration/data-selection.html",
                                                      "title":  "Data selection",
                                                      "keywords":  "Data selection In addition to the data source configuration, you need to provide a data selection configuration to specify the data you want the adapter to collect from the data source. Note: This document uses cURL commands to demonstrate data selection configuration, but other options are available. For more information, see Configuration tools . Configure Structured Data Files data selection Complete the following steps to configure Structured Data Files data selection. Use the PUT method in conjunction with the api/v1/configuration/\u003cComponentId\u003e/DataSelection api v1 configuration \u003cComponentId\u003e DataSelection REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for data selection into the file. For sample JSON, see Structured Data Files data selection examples . Update the example JSON parameters for your environment. For a table of all available parameters, see Structured Data Files data selection parameters . Save the file as ConfigureDataSelection.json . Open a command line session and change the working directory to the location of ConfigureDataSelection.json . Enter the following cURL command (which uses the PUT method) to initialize the data selection configuration. curl -d \"@ConfigureDataSelection.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/StructuredDataFiles1/DataSelection\" \"http:  localhost:5590 api v1 configuration StructuredDataFiles1 DataSelection\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. If you use a component ID other than StructuredDataFiles1 , update the endpoint with the component ID. For a list of other REST operations you can perform, like updating or deleting a data selection configuration, see REST URLs . Structured Data Files data selection schema The full schema definition for the Structured Data Files data selection configuration is in the StructuredDataFiles_DataSelection_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\StructuredDataFiles\\Schemas Linux: /opt/OSIsoft/Adapters/StructuredDataFiles/Schemas  opt OSIsoft Adapters StructuredDataFiles Schemas Structured Data Files data selection parameters Parameter Required Type Description Name Optional string The optional friendly name of the data item collected from the data source. If not configured, data endpoints use the StreamId value as the Name . Default value: null DataFilterId Optional string The identifier of a data filter defined in the Data filters configuration . By default, no filter is applied. Note: If the specified DataFilterId does not exist, unfiltered data is sent until that DataFilterId is created. Selected Optional boolean If true , data for this item is collected and sent to one or more configured OMF endpoint. Allowed value: true or false Default value: true StreamId Optional string The custom identifier used to create the streams. If not specified or null , the adapter generates a default value based on the DefaultStreamIdPattern in the PI Adapter for Structured Data Files data source configuration . A properly configured custom stream ID follows these rules: Is not case-sensitive. Can contain spaces. Cannot start with two underscores __ . Can contain a maximum of 100 characters. For more information on how the adapter encodes special characters in the StreamId , see Egress endpoints . ValueField Required 2 string Name of the value field. JSONPath, XPath, and CSV are supported 1 . For CSV files without a header, the column index ( 1 being the first column) should be specified. Example: \"FanSpeed\". Allowed Values: Any name to represent the value. IndexField Optional string Name of the time field. JSONPath, XPath, and CSV are supported 1 . For CSV files without a header, the column index ( 1 being the first column) should be specified. Default value (also when empty string, or only whitespace characters): null - UTC time the file is processed by the adapter. Allowed values if no timestamp is provided in the file: - null - FileCreationTime - UTC time when the file was created. - FileModifiedTime - UTC time when the file was modified. - Any valid JSONPath, XPath, or CSV column number or header Example: \"FanSpeedTimeStamp\" DataType Required string Data type of the values specified in the ValueField parameter. Example: \"Int32\" Allowed values: Boolean, Int16, UInt16, Int32, UInt32, Int64, UInt64, Float32, Float64, String, Date-Time IndexFormat Optional string Time format of the timestamp specified in the IndexField parameter. When IndexField is null , IndexFormat is passed as Adapter . Default value: null . Example: MM/dd/yyyy MM dd yyyy H:mm:ss zzz . Note: For more examples of time format syntax, see Date and time processing . DataFields Required 2 dictionary\u003cstring, string\u003e A dictionary of values with key-value pairs. The keys are specific fields for a complex type and the values are a JSONPath, XPath, or CSV to the value field 1 . For CSV files without a header, the column index ( 1 being the first column) should be specified Allowed keys: Latitude , Longitude , x , y , z Default value: null 1 Note : For full examples of how to enter JSONPath, XPath, or CSV syntax, see the following topics: JSONPath syntax for value retrieval XPath and CSV syntax for value retrieval 2 ValueField and DataFields are mutually exclusive. For example, if you specify ValueField , you cannot specify DataFields and vice versa. Structured Data Files data selection examples The following are examples of valid Structured Data Files data selection configurations: Structured Data Files data selection configuration example with custom IndexFormat [ { \"Selected\": true, \"Name\": \"Name\", \"StreamId\": \"StreamId\", \"ValueField\": \"Pressure\", \"IndexField\": \"PressureTimeStamp\", \"IndexFormat\": \"mm/dd/yy\", \"mm dd yy\", \"DataType\": \"Int16\" } ] Structured Data Files data selection configuration example with complex data Example complex data (CSV file with header): timestamp, lat, long, x_col, y_col, z_col 2021-09-12, 15.294533, 20.479839, 1.1, 2.2, 3.3 2021-09-12, 36.33403, -82.4019478, 4.1, 5.2, 6.3 Example data selection configuration for complex data [ { \"Selected\": true, \"Name\": \"Name_XYZ\", \"StreamId\": \"StreamId_XYZ\", \"ValueField\": null, \"DataFields\": { \"X\": \"x_col\", \"Y\": \"y_col\", \"Z\": \"z_col\" }, \"IndexField\": \"timestamp\", \"DataType\": \"Float64\" }, { \"Selected\": true, \"Name\": \"Name_LatLong\", \"StreamId\": \"StreamId_LatLong\", \"ValueField\": null, \"DataFields\": { \"Latitude\": \"lat\", \"Longitude\": \"long\" }, \"IndexField\": \"timestamp\", \"DataType\": \"Float64\" } ] Structured Data Files data selection configuration example with unselected item [ { \"Selected\": false, \"Name\": \"Name\", \"StreamId\": \"StreamId\", \"ValueField\": \"FanSpeed\", \"IndexField\": \"FileCreationTime\", \"DataType\": \"Int32\" } ] REST URLs Relative URL HTTP verb Action api/v1/configuration/\u003cComponentId\u003e/DataSelection api v1 configuration \u003cComponentId\u003e DataSelection GET Retrieves the data selection configuration, including all data selection items. api/v1/configuration/\u003cComponentId\u003e/DataSelection api v1 configuration \u003cComponentId\u003e DataSelection PUT Configures or updates the data selection configuration. The adapter starts collecting data for each data selection item when the following conditions are met: ??? The data selection configuration PUT request is received. ??? A data source configuration is active. api/v1/configuration/\u003cComponentId\u003e/DataSelection api v1 configuration \u003cComponentId\u003e DataSelection DELETE Deletes the active data selection configuration. The adapter stops collecting data. api/v1/configuration/\u003cComponentId\u003e/DataSelection api v1 configuration \u003cComponentId\u003e DataSelection PATCH Allows partial updates of configured data selection items. Note: The request must be an array containing one or more data selection items. Each item in the array must include its StreamId . api/v1/configuration/\u003cComponentId\u003e/DataSelection/\u003cStreamId\u003e api v1 configuration \u003cComponentId\u003e DataSelection \u003cStreamId\u003e PUT Updates or creates a new data selection item by StreamId . For new items, the adapter starts collecting data after the request is received. api/v1/configuration/\u003cComponentId\u003e/DataSelection/\u003cStreamId\u003e api v1 configuration \u003cComponentId\u003e DataSelection \u003cStreamId\u003e DELETE Deletes a data selection item from the configuration by StreamId . The adapter stops collecting data for the deleted item. Note: Replace \u003cComponentId\u003e with the ID of your Structured Data Files component, for example StructuredDataFiles1 ."
                                                  },
    "content/configuration/data-source.html":  {
                                                   "href":  "content/configuration/data-source.html",
                                                   "title":  "Data source",
                                                   "keywords":  "Data source To use the adapter, you must configure it to receive data from a data source. Note: This document uses cURL commands to demonstrate data selection configuration, but other options are available. For more information, see Configuration tools . Configure Structured Data Files data source Complete the following steps to configure a Structured Data Files data source. Use the PUT method in conjunction with the api/v1/configuration/\u003cComponentId\u003e/DataSource api v1 configuration \u003cComponentId\u003e DataSource REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for a structured data files data source into the file. For sample JSON, see Structured Data Files data source examples . Update the example JSON parameters for your environment. For a table of all available parameters, see Structured Data Files data source parameters . Save the file as ConfigureDataSource.json . Open a command line session and change the working directory to the location of ConfigureDataSelection.json . Enter the following cURL command (which uses the PUT method) to initialize the data source configuration. curl -d \"@ConfigureDataSource.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/StructuredDataFiles1/DataSource\" \"http:  localhost:5590 api v1 configuration StructuredDataFiles1 DataSource\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. If you use a component ID other than StructuredDataFiles1 , update the endpoint with the component ID. For a list of other REST operations you can perform, like updating or deleting a data source configuration, see REST URLs . Postrequisite: Configure data selection. For more information, see PI Adapter for Structured Data Files data selection configuration . Structured Data Files data source schema The full schema definition for the Structured Data Files data source configuration is in the StructuredDataFiles_DataSource_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\StructuredDataFiles\\Schemas Linux: /opt/OSIsoft/Adapters/StructuredDataFiles/Schemas  opt OSIsoft Adapters StructuredDataFiles Schemas Structured Data Files data source parameters The following parameters are available for configuring a Structured Data Files data source: Parameter Required Type Description FriendlyName Optional string The label to use for the data source. InputDirectory Required string Location of the source files to process. FTP servers are not supported. Example: C:\\\\InputDirectory Note: For the adapter to process data, the adapter service account must have read, write, and delete permissions for the input directory. OutputDirectory Required string Location for the files to be moved to after being processed. FTP servers are not supported. Example: C:\\\\OutputDirectory Note: For the adapter to process data, the adapter service account must have read, write, and delete permissions for the output directory. DiscoveryDirectory Optional string Location for the files to be processed during discovery. FTP servers are not supported. Example: C:\\\\DiscoveryDirectory Default value: null Note: For the adapter to process data, the adapter service account must have read, write, and delete permissions for the discovery directory. FileNameFilter Optional string Pattern used to match files in the InputDirectory for processing. If no filter is specified, the adapter will attempt to process all files in the InputDirectory . Use * as the wildcard character. Example: *.csv HasHeader Optional bool Indicates if a header line is present in the file. Only applies to CSV files. Default value: false Culture Optional string Locale setting for the input files. Example: en-US Default value: local culture TimeZone Optional string Time zone of timestamps in the input files. If specified, the value must be a valid entry from the IANA time zone database. Example 1: America/Los_Angeles America Los_Angeles Default value: Etc/UTC Etc UTC Example 2: This example represents three dates and a value seperated by commas. Date1,Date2,Date3,Value 03-08-2020T02:59-05:00,03-08-2020T02:59,03-08-2020T02:59Z,12345.6789 Date1 = 03-08-2020T02:59-05:00 this date has a \"-05:00\" adjustment meaning the date will be UTC-5. Date2 = 03-08-2020T02:59 this date is displayed with no adjustment and will make use of the default timezone properly. Date3 = 03-08-2020T02:59Z this date is displayed with a \"Z\" so will be UTC. Value = 12345.6789 Format Optional string Input file format. Allowed values: Csv , Json , Xml Default value: Csv Compression Optional string Input file compression format. Allowed value: None , Zip , Tar , GZip , TarGZip Default value: None Encoding Optional string Character encoding used in the input files. Allowed value: UTF8 , ASCII , Unicode Default value: UTF8 FieldSeparator Optional string Character used to delineate fields in the input files. Only applies to CSV files. Default value: , LineSeparator Optional string Character(s) used to separate lines in the input files. Only applies to CSV files. Allowed values: \\n , \\r , \\r\\n Default value: \\n StreamIdPrefix Optional string The stream ID prefix applied to all data items collected from the data source. Note: If you change the StreamIdPrefix of a configured adapter, for example when you delete and add a data source, you need to restart the adapter for the changes to take place. New streams are created on adapter restart and pre-existing streams are no longer updated. Default value: {ComponentId} DefaultStreamIdPattern Optional string Specifies the default stream ID pattern to use. Possible parameters: {FriendlyName} , {ValueField} Default pattern: {FriendlyName}.{ValueField} Note: The {FriendlyName} parameter can contain any text, the {ValueField} parameter contains JSONPath/XPath JSONPath XPath symbols, which are replaced according to Special characters support . PurgeDelay Optional string Specifies a time after which processed files will be removed from the output directory to save disk space. Allowed value: A valid time span string (minimum of zero seconds: 0.00:00:00.0000000 ) or null Default value: 10 days: 10.00:00:00.0000000 Note: If PurgeDelay is set to null , files will never be purged from the output directory. Structured Data Files data source examples The following are examples of valid Structured Data Files data source configurations: Note: When copy and pasting the examples below, validate the InputDirectory and OutputDirectory path for the data source host operating system: Windows or Linux. Data source configuration with required parameters { \"inputDirectory\": \"C:\\\\InputDirectory\", \"outputDirectory\": \"C:\\\\OutputDirectory\" } Data source configuration with optional parameters (Windows) { \"friendlyName\": \"Weather\", \"inputDirectory\": \"C:\\\\InputDirectory\", \"fileNameFilter\": \"*.csv\", \"outputDirectory\": \"C:\\\\OutputDirectory\", \"hasHeader\": true, \"culture\": \"fr-FR\", \"timeZone\": \"Europe/Paris\", \"Europe Paris\", \"compression\": \"Zip\", \"fieldSeparator\": \"|\", \"purgeDelay\": \"5.04:03:02.0000000\" } Data source configuration with optional parameters (Linux) { \"friendlyName\": \"NA-Pumps\", \"inputDirectory\": \"/usr/mnt/InputDir/\", \" usr mnt InputDir \", \"fileNameFilter\": \"*\", \"outputDirectory\": \"/usr/mnt/OutputDir/\", \" usr mnt OutputDir \", \"hasHeader\": true, \"format\": \"Csv\", \"compression\": \"None\", \"encoding\": \"UTF8\", \"fieldSeparator\": \",\", \"lineSeparator\": \"\\n\", \"defaultStreamIdPattern\": \"{FriendlyName}.{ValueField}\", \"purgeDelay\": \"5.04:03:02.0000000\" } REST URLs Relative URL HTTP verb Action api/v1/configuration/\u003cComponentId\u003e/DataSource api v1 configuration \u003cComponentId\u003e DataSource GET Retrieves the data source configuration. api/v1/configuration/\u003cComponentId\u003e/DataSource api v1 configuration \u003cComponentId\u003e DataSource POST Creates the data source configuration. The adapter starts collecting data after the following conditions are met: ??? The data source configuration POST request is received. ??? A data selection configuration is active. api/v1/configuration/\u003cComponentId\u003e/DataSource api v1 configuration \u003cComponentId\u003e DataSource PUT Configures or updates the data source configuration. Overwrites any active data source configuration. If no configuration is active, the adapter starts collecting data after the following conditions are met: ??? The data source configuration PUT request is received. ??? A data selection configuration is active. api/v1/configuration/\u003cComponentId\u003e/DataSource api v1 configuration \u003cComponentId\u003e DataSource DELETE Deletes the data source configuration. After the request is received, the adapter stops collecting data. Note: Replace \u003cComponentId\u003e with the ID of your Structured Data Files component, for example StructuredDataFiles1 ."
                                               },
    "content/configuration/data-source-discovery.html":  {
                                                             "href":  "content/configuration/data-source-discovery.html",
                                                             "title":  "Data source discovery",
                                                             "keywords":  "Data source discovery A discovery against the data source of a Structured Data Files adapter allows you to specify the optional query parameter. The query discovers the contents of the data source and narrows the scope of the discovery. You can add the discovered items to the data selection. Structured Data Files query string The string of the query parameter may contain none, any, or all string items in the following form: INCLUDEDATATYPE=\u003cDATA_TYPE_1\u003e,\u003cDATA_TYPE_2\u003e;EXCLUDEDATATYPE=\u003cDATA_TYPE_1\u003e,\u003cDATA_TYPE_2\u003e;INCLUDEFIELD=\u003cFIELD_1\u003e,\u003cFIELD_2\u003e;EXCLUDEFIELD=\u003cFIELD_1\u003e,\u003cFIELD_2\u003e; String item Required Description INCLUDEDATATYPE Optional Supported data types to include during discovery. Discovery looks for the specified data types in the files found in the discovery directory in the data source configuration. EXCLUDEDATATYPE Optional Supported data types to exclude during discovery. Discovery ignores the specified data types in the files found in the discovery directory in the data source configuration. INCLUDEFIELD Optional Name of the fields to include during discovery. Discovery looks for the specified fields in the files found in the discovery directory in the data source configuration. EXCLUDEFIELD Optional Name of the fields to exclude during discovery. Discovery ignores the specified fields in the files found in the discovery directory in the data source configuration. Query rules The following rules apply for specifying the query string: The query is made up of key=value pairs. Pairs are separated with a semicolon ( ; ). Keys and values are separated with an equals ( = ). Multiple values per key are supported. These values are separated with a comma ( , ). Special characters are replaced by the text parser. For more information, see Text parser Note: The data source might contain tens of thousands of metrics. Ensure that the query will only return data for the selection items you are interested in. Discovery query example The query parameter must be specified in the following form: INCLUDEDATATYPE=\u003cDATA_TYPE_1\u003e,\u003cDATA_TYPE_2\u003e;EXCLUDEDATATYPE=\u003cDATA_TYPE_1\u003e,\u003cDATA_TYPE_2\u003e;INCLUDEFIELD=\u003cFIELD_1\u003e,\u003cFIELD_2\u003e;EXCLUDEFIELD=\u003cFIELD_1\u003e,\u003cFIELD_2\u003e; Structured Data Files data source discovery initiation { \"id\" : \"40\", \"query\" : \"INCLUDEDATATYPE=int16,float32;EXCLUDEFIELD=Volume\" } Structured Data Files data source discovery results [ { \"id\": \"40\", \"query\": \"INCLUDEDATATYPE=int16,float32;EXCLUDEFIELD=Volume\", \"startTime\": \"2020-12-14T14:19:01.4383791-08:00\", \"endTime\": \"2020-12-14T14:19:31.8549164-08:00\", \"progress\": 30, \"itemsFound\": 700, \"newItems\": 200, \"resultUri\": \"http://127.0.0.1:5590/api/v1/Configuration/SDFComponentId/Discoveries/40/result\", \"http:  127.0.0.1:5590 api v1 Configuration SDFComponentId Discoveries 40 result\", \"autoSelect\": false, \"status\": \"Complete\", \"errors\": null } ] Structured Data Files discovered selection items [ { \"Selected\": false, \"Name\": \"Name\", \"StreamId\": \"StreamId\", \"ValueField\": \"Pressure\", \"IndexField\": \"PressureTimeStamp\", \"IndexFormat\": null, \"DataType\": \"int16\" } ]"
                                                         },
    "content/configuration/security-structured-data-files.html":  {
                                                                      "href":  "content/configuration/security-structured-data-files.html",
                                                                      "title":  "Security",
                                                                      "keywords":  "Security When determining Structured Data Files security practices with regards to REST APIs, you should consider the following practice. To keep the adapter secure, only administrators should have access to machines where the adapter is installed. REST APIs are bound to localhost, meaning that only requests coming from within the machine will be accepted."
                                                                  },
    "content/configuration/system-components-configuration.html":  {
                                                                       "href":  "content/configuration/system-components-configuration.html",
                                                                       "title":  "System components configuration",
                                                                       "keywords":  "System components configuration PI adapters use JSON configuration files in a protected directory on Windows and Linux to store configuration that is read on startup. While the files are accessible to view, OSIsoft recommends that you use REST or the EdgeCmd utility for any changes you make to the files. As part of making adapters as secure as possible, any passwords or secrets that you configure are stored in encrypted form where cryptographic key material is stored separately in a secure location. If you edit the files directly, the adapter may not work as expected. Note: You can edit any single component or facet of the system individually using REST, but you can also configure the system as a whole with a single REST call. Configure system components Complete the following steps to configure system components. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/system/components http:  localhost:5590 api v1 configuration system components REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for system components into the file. For sample JSON, see Examples . Update the example JSON parameters for your environment. For a table of all available parameters, see System components parameters . Save the file. For example, as ConfigureComponents.json . Open a command line session. Change directory to the location of ConfigureComponents.json . Enter the following cURL command (which uses the PUT method) to initialize the system components configuration. curl -d \"@ConfigureComponents.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/system/components\" \"http:  localhost:5590 api v1 configuration system components\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or deleting a system components configuration, see REST URLs . System components schema The full schema definition for the system components configuration is in the System_Components_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux: /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas System components parameters You can configure the following parameters for system components: Parameters Required Type Description ComponentId Required string The ID of the component 1 . It can be any alphanumeric string. A properly configured ComponentID follows these rules: Cannot contain leading or trailing space Cannot use the following characters: \u003e \u003c /   : ? # [ ] @ ! $ \u0026 * \\ \" ( ) \\\\ + , ; = \\| ` { } ComponentType Required string The type of the component. There are two types of components: OmfEgress and the adapter. 1 1 Note: The OmfEgress component is required to run the adapter. Both its ComponentId and ComponentType are reserved and should not be modified. Examples Default system components configuration The default System_Components.json file for the System component contains the following information. [ { \"ComponentId\": \"OmfEgress\", \"ComponentType\": \"OmfEgress\" } ] System components configuration with two adapter instances [ { \"ComponentId\": \"StructuredDataFiles1\", \"ComponentType\": \"StructuredDataFiles\" }, { \"ComponentId\": \"StructuredDataFiles2\", \"ComponentType\": \"StructuredDataFiles\" }, { \"ComponentId\": \"OmfEgress\", \"ComponentType\": \"OmfEgress\" } ] REST URLs Relative URL HTTP verb Action api/v1/configuration/system/components api v1 configuration system components GET Retrieves the system components configuration api/v1/configuration/system/components api v1 configuration system components POST Adds a new component to the system configuration api/v1/configuration/system/components api v1 configuration system components PUT Updates the system components configuration api/v1/configuration/system/components/ api v1 configuration system components  ComponentId DELETE Deletes a specific component from the system components configuration api/v1/configuration/system/components/ api v1 configuration system components  ComponentId PUT Creates a new component with the specified ComponentId in the system configuration"
                                                                   },
    "content/configuration/text-parser/jsonpath-syntax-for-value-retrieval.html":  {
                                                                                       "href":  "content/configuration/text-parser/jsonpath-syntax-for-value-retrieval.html",
                                                                                       "title":  "JSONPath syntax for value retrieval",
                                                                                       "keywords":  "JSONPath syntax for value retrieval For information on which semantic is used for retrieving values from JSON files, see JSONPath Syntax . The following syntax is used to extract values from JSON documents. JSON - Simple JSONPath example [ { \"time\": \"2020-08-10T12:10:46.0928791Z\", \"value\": 1.234567890 }, { \"time\": \"2020-08-10T12:10:47.0928791Z\", \"value\": 12.34567890 }, { \"time\": \"2020-08-10T12:10:48.0928791Z\", \"value\": 123.4567890 }, { \"time\": \"2020-08-10T12:10:49.0928791Z\", \"value\": 1234.567890 }, { \"time\": \"2020-08-10T12:10:50.0928791Z\", \"value\": 12345.67890 }, { \"time\": \"2020-08-10T12:10:51.0928791Z\", \"value\": 123456.7890 }, { \"time\": \"2020-08-10T12:10:52.0928791Z\", \"value\": 12345678.90 }, { \"time\": \"2020-08-10T12:10:53.0928791Z\", \"value\": 123456789.0 } ] The following data selection item configuration with JSONPath reads a series of values: [ { \"valueField\": \"$[*].value\", \"indexField\": \"$[*].time\", \"dataType\": \"float64\", \"selected\": true } ] JSON - Complex JSONPath examples The following example reads specific values from a JSON array: { \"StreamData\": { \"TPPrototype.uflsample.value_time\": [ { \"StreamId\": \"TPPrototype.uflsample.value_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T06:00:00Z\", \"Value\": 339.0 }, { \"StreamId\": \"TPPrototype.uflsample.value_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T07:00:00Z\", \"Value\": 344.0 }, { \"StreamId\": \"TPPrototype.uflsample.value_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T17:00:00Z\", \"Value\": 341.0 } ], \"TPPrototype.uflsample.value_timeString\": [ { \"StreamId\": \"TPPrototype.uflsample.value_timeString\", \"DataType\": \"String\", \"Timestamp\": \"2013-12-01T06:00:00Z\", \"Value\": \"339.0\" }, { \"StreamId\": \"TPPrototype.uflsample.value_timeString\", \"DataType\": \"String\", \"Timestamp\": \"2013-12-01T07:00:00Z\", \"Value\": \"344.0\" }, { \"StreamId\": \"TPPrototype.uflsample.value_timeString\", \"DataType\": \"String\", \"Timestamp\": \"2013-12-01T17:00:00Z\", \"Value\": \"341.0\" } ], \"TPPrototype.uflsample.pressure_time\": [ { \"StreamId\": \"TPPrototype.uflsample.pressure_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T06:00:00Z\", \"Value\": 339.0 }, { \"StreamId\": \"TPPrototype.uflsample.pressure_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T07:00:00Z\", \"Value\": 344.0 }, { \"StreamId\": \"TPPrototype.uflsample.pressure_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T17:00:00Z\", \"Value\": 341.0 } ] } } The following data selection item configuration with JSONPath reads all the TPPrototype.uflsample.value_time values from the JSON above: [ { \"valueField\": \"$[\u0027StreamData\u0027].[\u0027TPPrototype.uflsample.value_time\u0027][*].Value\", \"indexField\": \"$[\u0027StreamData\u0027].[\u0027TPPrototype.uflsample.value_time\u0027][*].Timestamp\", \"dataType\": \"float64\", \"selected\": true } ] The following data selection item configuration with JSONPath uses a filter expression to read all the TPPrototype.uflsample.value_time values with a value greater than 340 from the JSON above: [ { \"valueField\": \"$.StreamData.[\u0027TPPrototype.uflsample.value_time\u0027][?(@.Value\u003e340)].Value\", \"indexField\": \"$.StreamData.[\u0027TPPrototype.uflsample.value_time\u0027][?(@.Value\u003e340)].Timestamp\", \"dataType\": \"float64\", \"selected\": true } ] The following example reads specific values from a JSON array: { \"powerDateValuesList\": { \"timeUnit\": \"QUARTER_OF_AN_HOUR\", \"unit\": \"Wh\", \"count\": 1, \"siteEnergyList\": [ { \"siteId\": 1338075, \"powerDataValueSeries\": { \"measuredBy\": \"METER\", \"values\": [ { \"date\": \"2021-09-23 06:00:00\", \"value\": 0.0 }, { \"date\": \"2021-09-23 06:15:00\", \"value\": 0.0 }, { \"date\": \"2021-09-23 06:30:00\", \"value\": null }, { \"date\": \"2021-09-23 06:45:00\", \"value\": null }, { \"date\": \"2021-09-23 07:00:00\", \"value\": 35.29568 }, { \"date\": \"2021-09-23 07:15:00\", \"value\": 280.98932 }, { \"date\": \"2021-09-23 07:30:00\", \"value\": 541.07477 }, { \"date\": \"2021-09-23 07:45:00\", \"value\": 508.92096 } ] } } ] } } The following data selection item configuration with JSONPath uses a filter expression to read all values that are not null from the JSON above: [ { \"valueField\": \"$..siteEnergyList[0].powerDataValueSeries.values[?(@.value!=null)].value\", \"indexField\": \"$..siteEnergyList[0].powerDataValueSeries.values[?(@.value!=null)].date\", \"dataType\": \"float64\", \"selected\": true } ] The following example reads specific value from complex nested JSON: { \"success\": true, \"error\": null, \"result\": { \"type\": \"runtime_history\", \"chart\": { \"chart\": { \"type\": \"column\" }, \"title\": { \"text\": \"\" }, \"subtitle\": { \"text\": \"Daily History\" }, \"colors\": [ \"#fee292\", \"#fdc152\", \"#f69638\", \"#f17130\", \"#9f2d26\", \"#8acadc\", \"#184c8e\" ], \"series\": [ { \"name\": \"Stage 3 Aux Heat\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"heat\", \"state\": \"heat_aux_stage3\" }, { \"name\": \"Stage 2 Aux Heat\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"heat\", \"state\": \"heat_aux_stage2\" }, { \"name\": \"Aux Heat\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"heat\", \"state\": \"heat_aux\" }, { \"name\": \"Stage 2 Heat\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"heat\", \"state\": \"heat_stage2\" }, { \"name\": \"Heat\", \"data\": [ 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.3, 0.2, 0.0 ], \"stack\": \"heat\", \"state\": \"heat\" }, { \"name\": \"Stage 2 Cool\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"cool\", \"state\": \"cool_stage2\" }, { \"name\": \"Cool\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"cool\", \"state\": \"cool\" } ], \"xAxis\": { \"categories\": [ \"Friday\", \"Saturday\", \"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\" ], \"labels\": { \"rotation\": -45 } }, \"yAxis\": { \"allowDecimals\": false, \"min\": 0, \"max\": 24, \"tickInternval\": 4, \"title\": { \"text\": \"Runtime (Hours)\" } }, \"legend\": { \"layout\": \"vertical\", \"align\": \"center\", \"floating\": false, \"shadow\": false, \"itemStyle\": { \"fontSize\": \"1em\" } }, \"tooltip\": { \"shared\": true, \"borderColor\": \"#000000\" }, \"credits\": { \"enabled\": false }, \"plotOptions\": { \"column\": { \"stacking\": \"normal\" }, \"series\": { \"shadow\": false } } }, \"table\": { \"headings\": [ \"Fri\", \"Sat\", \"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\" ], \"series\": [ { \"name\": \"Aux Heat\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"heat\", \"state\": \"heat_aux\" }, { \"name\": \"Outdoor High Temp.\", \"data\": [ 72.0, 64.0, 73.0, 72.0, 67.0, 73.0, 77.0, 62.0, 51.0 ], \"stack\": null, \"state\": \"outdoor_high_temperature\" }, { \"name\": \"Outdoor Low Temp.\", \"data\": [ 55.0, 60.0, 62.0, 61.0, 51.0, 43.0, 46.0, 44.0, 35.0 ], \"stack\": null, \"state\": \"outdoor_low_temperature\" }, { \"name\": \"Avg Indoor Temp.\", \"data\": [ 76.0, 77.0, 78.0, 78.0, 77.0, 73.0, 74.0, 75.0, 72.0 ], \"stack\": null, \"state\": \"average_indoor_temperature\" }, { \"name\": \"Avg Indoor Humidity\", \"data\": [ 66.0, 68.0, 70.0, 70.0, 69.0, 67.0, 67.0, 66.0, 61.0 ], \"stack\": null, \"state\": \"average_indoor_humidity\" }, { \"name\": \"Fan Only Runtime\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": null, \"state\": \"fan_only\" }, { \"name\": \"Vent\", \"data\": [], \"stack\": null, \"state\": \"vent\" } ] }, \"show_monthly_runtime_history\": true } } The following data selection item configuration with JSONPath reads Sunday Average Indoor Temperature from the JSON above. The timestamp comes from Adapter local time. [ { \"valueField\": \"$.result.table.series[3].data[2]\", \"indexField\": null, \"dataType\": \"float64\", \"selected\": true } ] Error handling If you encounter text parser related errors that is errors for the ValueField or IndexField , check the StreamId associated with the error message. Possible errors include the following: The JSONPath expression of ValueField or IndexField is pointing to a non-existing value The JSONPath expression of ValueField or IndexField is missing a value altogether DataType does not match the value"
                                                                                   },
    "content/configuration/text-parser/text-parser.html":  {
                                                               "href":  "content/configuration/text-parser/text-parser.html",
                                                               "title":  "Text parser",
                                                               "keywords":  "Text parser PI Adapter for Structured Data Files includes the text parser component which ensures consistent parsing of text from different files. This adapter supports parsing of .json, .xml, and .csv files. Designed to be a document parser, the text parser parses a semantically complete document in its entirety. The text parser produces OMF compatible output, which in turn is compatible with the OCS backing SDS (Sequential Data Store) that stores data in streams consisting of multiple values and indexes. Data types supported by the text parser The following data types are supported by the text parser: DateTime DateTimeOffset TimeSpan sbyte byte short ushort int uint long ulong float double decimal bool char string Note: Not all data types supported by the text parser are also supported by OMF. Special characters support As part of the default StreamId logic, the text parser replaces special characters as follows: Special character Replacement character * empty string \u0027 empty string ` empty string \" empty string ? empty string ; - \\| - \\ - { ( } ) [ ( ] ) Culture support Some numeric values and datetimes support cultures when they are being parsed. The default culture is en-US (US English) (InvariantCulture). OSIsoft recommends that you leave the adapter at the default unless you expect culturally variant input. Note: Installed cultures vary by machine with both Linux and Windows. If the specified culture is not installed, the text parser fails to parse input that requires that culture. Time zone support A time zone or offset specified by a time is always used to convert to UTC time. Time zones are only used if there is no offset or time zone specifier in a text date and time string. For time zones that support time changes between daylight and standard times, a text file may temporarily contain invalid or ambiguous datetimes during the time change, which are possible only for a two-hour period each year. When these time changes occur, the text parser logs them, but the datetime is parsed and passed to the callback. Ambiguous times are reported as standard times, which is the Microsoft recommendation. Date and time processing The text parser can use time zones, cultures, and custom formats to read dates and times from ingress data. You can specify date and time formats when you configure data selection. Set the date and time using the IndexFormat property. If you leave the IndexFormat property unset, the data selection configuration defaults to the ISO 8601 date format. If you are using a culture other than default en-US , use the name of day or month specific to the culture. For example, use \"Juni\" instead of \"June\" for the de-DE culture. The following date and time syntaxes have been tested and are supported. \"MM/dd/yyyy \"MM dd yyyy H:mm:ss zzz\" \"06/15/2018 \"06 15 2018 15:15:30 -05:00\" \"MM/dd/yyyy \"MM dd yyyy H:mm:ss.fff zzz\" \"06/15/2018 \"06 15 2018 15:15:30.123 -05:00\" \"dd/MM/yyyy \"dd MM yyyy H:mm:ss.fff K\" \"15/06/2018 \"15 06 2018 15:15:30.123 Z\" \"MMMM/dd/yyyy \"MMMM dd yyyy H:mm:ss.fff K\" \"June/15/2018 \"June 15 2018 15:15:30.123 Z\" (InvariantCulture/English) (InvariantCulture English) \"MMMM/dd/yyyy \"MMMM dd yyyy H:mm:ss.fff K\" \"Juni/15/2018 \"Juni 15 2018 15:15:30.123 Z\" (German) \"MMM/dd/yyyy \"MMM dd yyyy H:mm:ss.fff K\" \"Jun/15/2018 \"Jun 15 2018 15:15:30.123 Z\" \"MMM-dd-yyyy H:mm:ss.fff K\" \"Jun-15-2018 15:15:30.123 Z\" \"MMM-dd-yyyy H:mm:ss.fff K\" \"Jun-15-2018 15:15:30.123 Z\" \"MMM-dd-yyyy H:mm:ss.fff K\" \"Jun-15-2018 15:15:30.123 Z\" \"yyyy-MM-dd H:mm:ss.fff K\" \"2018-06-15 15:15:30.123 Z\" \"yyyy-M-d H:mm:ss.fff K\" \"2018-6-5 15:15:30.123 Z\" \"yyyy-M-d H:mm:ss.fff zzz\" \"2018-6-5 15:15:30.123 +05:00\" \"ddd dd MMM yyyy h:mm tt zzz\" \"Sun 15 Jun 2008 8:30 AM -06:00\" \"dddd dd MMM yyyy h:mm tt zzz\" \"Sunday 15 Jun 2008 8:30 AM -06:00\" \"dddd dd MMM yyyy h:mm tt zzz\" \"Sunday 15 Jun 2008 8:30 AM -06:00\" \"dddd dd MMMM yyyy h:mm tt zzz\" \"Sunday 15 June 2008 8:30 AM -06:00\" Adapter date and time processing uses Microsoft datetime parsing . For more documentation on standard datetime formats, which fit most use cases, see Standard date and time format strings . For documentation on custom datetime formation, see Custom date and time format strings ."
                                                           },
    "content/configuration/text-parser/xpath-and-csv-syntax-for-value-retrieval.html":  {
                                                                                            "href":  "content/configuration/text-parser/xpath-and-csv-syntax-for-value-retrieval.html",
                                                                                            "title":  "XPath and CSV syntax for value retrieval",
                                                                                            "keywords":  "XPath and CSV syntax for value retrieval For information on which semantics are used for retrieving values from XML and CSV files, see the following documentation: XML - XML Path Language (XPath) CSV - Column Index (1 based) or Header value (if header defined) The following syntaxes are used to extract values from XML or CSV documents. XML - Simple XPath example \u003cvalues\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:46.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:46.0928791Z\u003c time\u003e \u003cvalue\u003e1.234567890\u003c/value\u003e \u003cvalue\u003e1.234567890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:47.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:47.0928791Z\u003c time\u003e \u003cvalue\u003e12.34567890\u003c/value\u003e \u003cvalue\u003e12.34567890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:48.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:48.0928791Z\u003c time\u003e \u003cvalue\u003e123.4567890\u003c/value\u003e \u003cvalue\u003e123.4567890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:49.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:49.0928791Z\u003c time\u003e \u003cvalue\u003e1234.567890\u003c/value\u003e \u003cvalue\u003e1234.567890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:50.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:50.0928791Z\u003c time\u003e \u003cvalue\u003e12345.67890\u003c/value\u003e \u003cvalue\u003e12345.67890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:51.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:51.0928791Z\u003c time\u003e \u003cvalue\u003e123456.7890\u003c/value\u003e \u003cvalue\u003e123456.7890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:52.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:52.0928791Z\u003c time\u003e \u003cvalue\u003e12345678.90\u003c/value\u003e \u003cvalue\u003e12345678.90\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:53.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:53.0928791Z\u003c time\u003e \u003cvalue\u003e123456789.0\u003c/value\u003e \u003cvalue\u003e123456789.0\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003c/values\u003e \u003c values\u003e The following data selection item configuration with XPath reads a series of values: [ { \"valueField\": \"./values/value/value\", \". values value value\", \"indexField\": \"./values/value/time\", \". values value time\", \"dataType\": \"float64\", \"selected\": true } ] XML - Complex XPath example \u003csample\u003e \u003csuccess\u003etrue\u003c/success\u003e \u003csuccess\u003etrue\u003c success\u003e \u003cresult\u003e \u003cid\u003e964879\u003c/id\u003e \u003cid\u003e964879\u003c id\u003e \u003cname\u003eHome\u003c/name\u003e \u003cname\u003eHome\u003c name\u003e \u003clatitude\u003enull\u003c/latitude\u003e \u003clatitude\u003enull\u003c latitude\u003e \u003clongitude\u003enull\u003c/longitude\u003e \u003clongitude\u003enull\u003c longitude\u003e \u003cchild\u003e \u003cdata\u003e \u003citems\u003e \u003citem\u003e \u003cid\u003e1603836\u003c/id\u003e \u003cid\u003e1603836\u003c id\u003e \u003cname\u003eThermostat1\u003c/name\u003e \u003cname\u003eThermostat1\u003c name\u003e \u003cfeature\u003e \u003cname\u003ethermostat\u003c/name\u003e \u003cname\u003ethermostat\u003c name\u003e \u003ctemperature\u003e70\u003c/temperature\u003e \u003ctemperature\u003e70\u003c temperature\u003e \u003cscale\u003ef\u003c/scale\u003e \u003cscale\u003ef\u003c scale\u003e \u003cheat_min\u003e55\u003c/heat_min\u003e \u003cheat_min\u003e55\u003c heat_min\u003e \u003cheat_max\u003e90\u003c/heat_max\u003e \u003cheat_max\u003e90\u003c heat_max\u003e \u003ccool_min\u003e60\u003c/cool_min\u003e \u003ccool_min\u003e60\u003c cool_min\u003e \u003ccool_max\u003e99\u003c/cool_max\u003e \u003ccool_max\u003e99\u003c cool_max\u003e \u003cdatetime\u003e2021-09-06T17:00:00Z\u003c/datetime\u003e \u003cdatetime\u003e2021-09-06T17:00:00Z\u003c datetime\u003e \u003c/feature\u003e \u003c feature\u003e \u003c/item\u003e \u003c item\u003e \u003citem\u003e \u003cid\u003e1603836\u003c/id\u003e \u003cid\u003e1603836\u003c id\u003e \u003cname\u003eThermostat1\u003c/name\u003e \u003cname\u003eThermostat1\u003c name\u003e \u003cfeature\u003e \u003cname\u003ethermostat\u003c/name\u003e \u003cname\u003ethermostat\u003c name\u003e \u003ctemperature\u003e71\u003c/temperature\u003e \u003ctemperature\u003e71\u003c temperature\u003e \u003cscale\u003ef\u003c/scale\u003e \u003cscale\u003ef\u003c scale\u003e \u003cheat_min\u003e56\u003c/heat_min\u003e \u003cheat_min\u003e56\u003c heat_min\u003e \u003cheat_max\u003e91\u003c/heat_max\u003e \u003cheat_max\u003e91\u003c heat_max\u003e \u003ccool_min\u003e59\u003c/cool_min\u003e \u003ccool_min\u003e59\u003c cool_min\u003e \u003ccool_max\u003e99\u003c/cool_max\u003e \u003ccool_max\u003e99\u003c cool_max\u003e \u003cdatetime\u003e2021-09-06T17:01:00Z\u003c/datetime\u003e \u003cdatetime\u003e2021-09-06T17:01:00Z\u003c datetime\u003e \u003c/feature\u003e \u003c feature\u003e \u003c/item\u003e \u003c item\u003e \u003citem\u003e \u003cid\u003e1603836\u003c/id\u003e \u003cid\u003e1603836\u003c id\u003e \u003cname\u003eThermostat1\u003c/name\u003e \u003cname\u003eThermostat1\u003c name\u003e \u003cfeature\u003e \u003cname\u003ethermostat\u003c/name\u003e \u003cname\u003ethermostat\u003c name\u003e \u003ctemperature\u003e69\u003c/temperature\u003e \u003ctemperature\u003e69\u003c temperature\u003e \u003cscale\u003ef\u003c/scale\u003e \u003cscale\u003ef\u003c scale\u003e \u003cheat_min\u003e54\u003c/heat_min\u003e \u003cheat_min\u003e54\u003c heat_min\u003e \u003cheat_max\u003e92\u003c/heat_max\u003e \u003cheat_max\u003e92\u003c heat_max\u003e \u003ccool_min\u003e58\u003c/cool_min\u003e \u003ccool_min\u003e58\u003c cool_min\u003e \u003ccool_max\u003e99\u003c/cool_max\u003e \u003ccool_max\u003e99\u003c cool_max\u003e \u003cdatetime\u003e2021-09-06T17:02:00Z\u003c/datetime\u003e \u003cdatetime\u003e2021-09-06T17:02:00Z\u003c datetime\u003e \u003c/feature\u003e \u003c feature\u003e \u003c/item\u003e \u003c item\u003e \u003c/items\u003e \u003c items\u003e \u003c/data\u003e \u003c data\u003e \u003c/child\u003e \u003c child\u003e \u003c/result\u003e \u003c result\u003e \u003c/sample\u003e \u003c sample\u003e The following data selection item configuration with XPath reads all the heat_min values from the XML above: [ { \"valueField\": \"./sample/result/child/data/items/item/feature/heat_min\", \". sample result child data items item feature heat_min\", \"indexField\": \"./sample/result/child/data/items/item/feature/datetime\", \". sample result child data items item feature datetime\", \"dataType\": \"float64\", \"selected\": true } ] The following data selection item configuration with XPath uses a predicate to read all the heat_min values with a value greater than or equal to 55 from the XML above: [ { \"valueField\": \"./sample/result/child/data/items/item/feature[heat_min\u003e=55]/heat_min\", \". sample result child data items item feature[heat_min\u003e=55] heat_min\", \"indexField\": \"./sample/result/child/data/items/item/feature[heat_min\u003e=55]/datetime\", \". sample result child data items item feature[heat_min\u003e=55] datetime\", \"dataType\": \"float64\", \"selected\": true } ] CSV - Simple CSV column index example 2020-08-10T12:10:46.0928791Z,1.234567890 2020-08-10T12:10:47.0928791Z,12.34567890 2020-08-10T12:10:48.0928791Z,123.4567890 2020-08-10T12:10:49.0928791Z,1234.567890 2020-08-10T12:10:50.0928791Z,12345.67890 2020-08-10T12:10:51.0928791Z,123456.7890 2020-08-10T12:10:52.0928791Z,12345678.90 2020-08-10T12:10:53.0928791Z,123456789.0 The following data selection item configuration using the CSV column index requires the data source configuration be configured with HasHeader=false . The column indexes are 1 based and configured as strings. [ { \"valueField\": \"2\", \"indexField\": \"1\", \"dataType\": \"float64\", \"selected\": true } ] CSV - Simple CSV column header example Date,Value 2020-08-10T12:10:46.0928791Z,1.234567890 2020-08-10T12:10:47.0928791Z,12.34567890 2020-08-10T12:10:48.0928791Z,123.4567890 2020-08-10T12:10:49.0928791Z,1234.567890 2020-08-10T12:10:50.0928791Z,12345.67890 2020-08-10T12:10:51.0928791Z,123456.7890 2020-08-10T12:10:52.0928791Z,12345678.90 2020-08-10T12:10:53.0928791Z,123456789.0 The following data selection item configuration using the CSV column header requires the data source configuration be configured with HasHeader=true . [ { \"valueField\": \"Value\", \"indexField\": \"Date\", \"dataType\": \"float64\", \"selected\": true } ]"
                                                                                        },
    "content/health-and-diagnostics/health/device-status.html":  {
                                                                     "href":  "content/health-and-diagnostics/health/device-status.html",
                                                                     "title":  "Device status",
                                                                     "keywords":  "Device status The device status indicates the health of this component and if it is currently communicating properly with the data source. This time-series data is stored within a PI point or OCS stream, depending on the endpoint type. During healthy steady-state operation, a value of Good is expected. Property Type description Time string Timestamp of the event DeviceStatus string The value of the DeviceStatus The possible statuses: Status Meaning Good The component is connected to the data source and is successfully processing files. ConnectedNoData The component is connected to the data source, but a data file contained no data for selected items. Starting The component is currently in the process of starting up and is not yet connected to the data source. DeviceInError The component encountered an error while processing files. A file either cannot be opened or moved successfully. Shutdown The component is in the process of shutting down or has finished. Removed The component has been removed and will no longer collect data. NotConfigured The component has been created but is not yet configured."
                                                                 },
    "content/health-and-diagnostics/health/health.html":  {
                                                              "href":  "content/health-and-diagnostics/health/health.html",
                                                              "title":  "Health",
                                                              "keywords":  "Health PI Adapters produce different kinds of health data that can be egressed to different health endpoints. Available health data Dynamic data is sent every minute to configured health endpoints. The following health data are available: Device status Next Health Message Expected AF structure With a health endpoint configured to a PI server, you can use PI System Explorer to view the health of a given adapter. The element hierarchy is shown in the following image."
                                                          },
    "content/health-and-diagnostics/health/health-and-diagnostics.html":  {
                                                                              "href":  "content/health-and-diagnostics/health/health-and-diagnostics.html",
                                                                              "title":  "Health and diagnostics",
                                                                              "keywords":  "Health and diagnostics PI Adapters produce various types of health data. You can use health data to ensure that your adapters are running properly and that data flows to the configured OMF endpoints. For more information, see Adapter health . PI Adapters also produce diagnostic data. You can use diagnostic data to find more information about a particular adapter instance. Diagnostic data lives alongside the health data and you can egress it using a health endpoint and setting EnableDiagnostics to true . You can configure EnableDiagnostics in the system\u0027s General configuration . For more information on available diagnostics, see Adapter diagnostics . Health endpoint differences Two OMF endpoints are currently supported for adapter health data: PI Web API OSIsoft Cloud Services There are a few differences in how these two systems treat the associated health and diagnostics data. PI Web API parses the information and sends it to configured PI servers for the OMF endpoint. The static data is used to create a hierarchy on a PI AF server similar to the following example: The dynamic health data is time-series data that is stored in PI points on a PI Data Archive. You can see it in the AF hierarchy as PI point data reference attributes. OSIsoft Cloud Services does not currently provide a way to store the static metadata. For OCS-based adapter health endpoints, only the dynamic data is stored. Each value is its own stream with the timestamp property as the single index."
                                                                          },
    "content/health-and-diagnostics/health/next-health-message-expected.html":  {
                                                                                    "href":  "content/health-and-diagnostics/health/next-health-message-expected.html",
                                                                                    "title":  "Next health message expected",
                                                                                    "keywords":  "Next health message expected This property is similar to a heartbeat. A new value for NextHealthMessageExpected is sent by an individual adapter data component on a periodic basis while it is functioning properly. This value is a timestamp that indicates when the next value should be received. When monitoring, if the next value is not received by the indicated time, this likely means that there is an issue. It could be an issue with the adapter, adapter component, network connection between the health endpoint and the adapter, and so on. Property Type Description Time string Timestamp of the event NextHealthMessageExpected string Timestamp when next value is expected"
                                                                                },
    "content/index.html":  {
                               "href":  "content/index.html",
                               "title":  "Overview",
                               "keywords":  "Overview PI Adapter for Structured Data Files is a data-collection component that transfers time-series data from source files in a local or remote directory to OMF endpoints in OSIsoft Cloud Services or PI Servers. \u003c!--The conceptual information is very light. What type of files? Where do they come from? What sorts of scenarios would this be used in? I wouldn\u0027t expect to see installation and configuration information in the main overview page. It seems too detailed. I realize this is what is done on the other apater documents, but I would question it there, too.--\u003e Adapter installation You can install the adapter with a download kit that you can obtain from the OSIsoft Customer Portal. You can install the adapter on devices running either Windows or Linux operating systems. Adapter configuration Using REST API, you can configure all functions of the adapter. The configurations are stored in JSON files. For data ingress, you must define an adapter component in the system components configuration for each data source to which the adapter will connect. You configure each adapter component with the connection information for the data source and the data to collect. For data egress, you must specify destinations for the data, including security for the outgoing connection. Additional configurations are available to egress health and diagnostics data, add buffering configuration to protect against data loss, and record logging information for troubleshooting purposes. Once you have configured the adapter and it is sending data, you can use administration functions to manage the adapter or individual ingress components of the adapter. Health and diagnostics functions monitor the status of connected devices, adapter system functions, the number of active data streams, the rate of data ingress, the rate of errors, and the rate of data egress. EdgeCmd utility OSIsoft also provides the EdgeCmd utility, a proprietary command line tool to configure and administer an adapter on both Linux and Windows operating systems. EdgeCmd utility is installed separately from the adapter."
                           },
    "content/main/shared-content/_includes/inline/component-id.html":  {
                                                                           "href":  "content/main/shared-content/_includes/inline/component-id.html",
                                                                           "title":  "",
                                                                           "keywords":  "StructuredDataFiles1"
                                                                       },
    "content/main/shared-content/_includes/inline/component-type.html":  {
                                                                             "href":  "content/main/shared-content/_includes/inline/component-type.html",
                                                                             "title":  "",
                                                                             "keywords":  "StructuredDataFiles"
                                                                         },
    "content/main/shared-content/_includes/inline/docker-image.html":  {
                                                                           "href":  "content/main/shared-content/_includes/inline/docker-image.html",
                                                                           "title":  "",
                                                                           "keywords":  "sdfadapter"
                                                                       },
    "content/main/shared-content/_includes/inline/framework-version.html":  {
                                                                                "href":  "content/main/shared-content/_includes/inline/framework-version.html",
                                                                                "title":  "",
                                                                                "keywords":  "1.4"
                                                                            },
    "content/main/shared-content/_includes/inline/installer-name.html":  {
                                                                             "href":  "content/main/shared-content/_includes/inline/installer-name.html",
                                                                             "title":  "",
                                                                             "keywords":  "PI-Adapter-for-StructuredDataFiles_1.0.0.138"
                                                                         },
    "content/main/shared-content/_includes/inline/product-name.html":  {
                                                                           "href":  "content/main/shared-content/_includes/inline/product-name.html",
                                                                           "title":  "",
                                                                           "keywords":  "PI Adapter for Structured Data Files"
                                                                       },
    "content/main/shared-content/_includes/inline/product-protocol.html":  {
                                                                               "href":  "content/main/shared-content/_includes/inline/product-protocol.html",
                                                                               "title":  "",
                                                                               "keywords":  ""
                                                                           },
    "content/main/shared-content/_includes/inline/product-version.html":  {
                                                                              "href":  "content/main/shared-content/_includes/inline/product-version.html",
                                                                              "title":  "",
                                                                              "keywords":  "1.0.0.138"
                                                                          },
    "content/main/shared-content/_includes/inline/startup-script.html":  {
                                                                             "href":  "content/main/shared-content/_includes/inline/startup-script.html",
                                                                             "title":  "",
                                                                             "keywords":  "sdfdockerstart.sh"
                                                                         },
    "content/main/shared-content/_includes/inline/symantic-version.html":  {
                                                                               "href":  "content/main/shared-content/_includes/inline/symantic-version.html",
                                                                               "title":  "",
                                                                               "keywords":  "1.0.0.138"
                                                                           },
    "content/main/shared-content/administration/administration.html":  {
                                                                           "href":  "content/main/shared-content/administration/administration.html",
                                                                           "title":  "Administration",
                                                                           "keywords":  "Administration With the PI adapter administration level functions, you can start and stop an adapter service and the individual adapter ingress components. You can also retrieve product version information and delete an adapter. The examples in the administration topics use curl , a commonly available tool on both Windows and Linux. You can use the same operations with any programming language or tool that supports making REST calls. You can also configure PI adapters with the EdgeCmd utility. For more information, see the EdgeCmd utility documentation . To validate successful configurations, you can accomplish data retrieval steps ( GET commands) using a browser, if available on your device. For more information on PI adapter configuration tools, see Configuration tools ."
                                                                       },
    "content/main/shared-content/administration/delete-an-adapter-component.html":  {
                                                                                        "href":  "content/main/shared-content/administration/delete-an-adapter-component.html",
                                                                                        "title":  "Delete an adapter component",
                                                                                        "keywords":  "Delete an adapter component When you remove an adapter component, the configuration and log files are saved into a sub-directory in case they are needed later. Any associated types, streams, and data remain on the respective endpoints. Complete the following steps to delete an adapter component: Start any of the Configuration tools capable of making HTTP requests. Run a DELETE command to the following endpoint: http://localhost:5590/api/v1/configuration/system/components/\u003cComponentId\u003e http:  localhost:5590 api v1 configuration system components \u003cComponentId\u003e Note: You must make an empty DELETE command against the Id of the component you want to delete. 5590 is the default port number. If you selected a different port number, replace it with that value. Example using curl : Delete an adapter component curl -X DELETE \"http://localhost:5590/api/v1/configuration/system/components/\u003cComponentId\u003e\" \"http:  localhost:5590 api v1 configuration system components \u003cComponentId\u003e\" File relocation All configuration and log files are renamed and moved. The files are renamed according to the timestamp of removal, for example, FileName.json_removed_yyyy-MM-dd--hh-mm-ss . Configuration files are moved to the following location: Windows: %programdata%\\OSIsoft\\Adapters\\AdapterName\\Configuration\\Removed Linux: /usr/share/OSIsoft/Adapters/AdapterName/Configuration/Removed  usr share OSIsoft Adapters AdapterName Configuration Removed Log files are moved to the following location: Windows: %programdata%\\OSIsoft\\Adapters\\AdapterName\\Logs\\Removed Linux: /usr/share/OSIsoft/Adapters/AdapterName/Logs/Removed  usr share OSIsoft Adapters AdapterName Logs Removed In the following example, one adapter service is installed on a particular Windows node with the name \u003cAdapter\u003eService1 . An adapter component with the name \u003cAdapter\u003eDeviceX was added and configured to this adapter and later removed. Linux follows a similar behavior. This is the resulting relocation and renaming scheme after deletion: REST URLs Relative URL HTTP verb Action api/v1/configuration/system/components/ api v1 configuration system components  ComponentId DELETE Deletes specified component Note: Replace ComponentId with the Id of the component that you want to delete."
                                                                                    },
    "content/main/shared-content/administration/retrieve-product-version-information.html":  {
                                                                                                 "href":  "content/main/shared-content/administration/retrieve-product-version-information.html",
                                                                                                 "title":  "Retrieve product version information",
                                                                                                 "keywords":  "Retrieve product version information The product version information includes the adapter framework version, application version, the version of the underlying .NET Core framework, and the operating system that the adapter is running on. Complete the following steps to retrieve the product version information of a PI adapter: Use any of the Configuration tools capable of making HTTP requests. Run a GET command to the following endpoint: http://localhost:5590/api/v1/Diagnostics/ProductInformation http:  localhost:5590 api v1 Diagnostics ProductInformation Note: 5590 is the default port number. If you selected a different port number, replace it with that value. Example using curl : Get product information for adapter hosted on port 5590 curl -X GET \"http://localhost:5590/api/v1/Diagnostics/ProductInformation\" \"http:  localhost:5590 api v1 Diagnostics ProductInformation\" Example result: { \"Application Name\": \"PI Adapter for \u003cAdapterName\u003e\", \"Adapter Framework Version\": \"1.3.0.351\", \"Application Version\":\"1.2.0.37\", \".Net Core Version\":\".NET Core 3.1.5\", \"Operating System\":\"Linux 4.15.0-106-generic #107-Ubuntu SMP Thu Jun 4 11:27:52 UTC 2020\" }"
                                                                                             },
    "content/main/shared-content/administration/start-and-stop-an-adapter.html":  {
                                                                                      "href":  "content/main/shared-content/administration/start-and-stop-an-adapter.html",
                                                                                      "title":  "Start and stop an adapter",
                                                                                      "keywords":  "Start and stop an adapter Complete the procedure appropriate for your operating system to start or stop an adapter service: Windows Open Windows services. Select PI Adapter for \u003cAdapterName\u003e . Depending on whether your adapter is running or not, click either Start or Stop . Linux Open command line. Depending on whether your adapter is running or not, type one of the following commands: Example: Start PI Adapter for \u003cAdapterName\u003e sudo systemctl start pi.adapter.\u003cadapterName\u003e Example: Stop PI Adapter for \u003cAdapterName\u003e sudo systemctl stop pi.adapter.\u003cadapterName\u003e Press Enter."
                                                                                  },
    "content/main/shared-content/administration/start-and-stop-ingress-component.html":  {
                                                                                             "href":  "content/main/shared-content/administration/start-and-stop-ingress-component.html",
                                                                                             "title":  "Start and stop ingress component",
                                                                                             "keywords":  "Start and stop ingress component To control data ingress, you can start and stop the ingress components of an adapter whenever necessary. By default, all currently configured ingress components are started. Start an ingress component Complete the following steps to start an individual ingress component: Use any of the Configuration tools capable of making HTTP requests. Run a POST command to the following endpoint, replacing \u003cComponentId\u003e with the ingress component that you want to start: http://localhost:5590/api/v1/administration/\u003cComponentId\u003e/Start http:  localhost:5590 api v1 administration \u003cComponentId\u003e Start Note: 5590 is the default port number. If you selected a different port number, replace it with that value. Example using curl : Start the adapter ingress component curl -d \"\" -X POST \"http://localhost:5590/api/v1/Administration/\u003cComponentId\u003e/Start\" \"http:  localhost:5590 api v1 Administration \u003cComponentId\u003e Start\" Stop an ingress component Complete the following steps to stop an individual ingress component: Start any configuration tool capable of making HTTP requests. Run a POST command to the following endpoint, replacing \u003cComponentId\u003e with the ingress component that you want to stop: http://localhost:5590/api/v1/administration/\u003cComponentId\u003e/Stop http:  localhost:5590 api v1 administration \u003cComponentId\u003e Stop Note: 5590 is the default port number. If you selected a different port number, replace it with that value. Example using curl : Stop the adapter ingress component curl -d \"\" -X POST \"http://localhost:5590/api/v1/Administration/\u003cComponentId\u003e/Stop\" \"http:  localhost:5590 api v1 Administration \u003cComponentId\u003e Stop\""
                                                                                         },
    "content/main/shared-content/configuration/buffering.html":  {
                                                                     "href":  "content/main/shared-content/configuration/buffering.html",
                                                                     "title":  "Buffering",
                                                                     "keywords":  "Buffering You can configure PI adapters to buffer data egressed from the adapter to endpoints. Buffering is configured through the buffering configuration parameters in the system configuration. Note: OSIsoft recommends that you do not modify the default buffering location unless it is necessary. Changes to the buffering configuration parameters only take effect during adapter service startup. Configure buffering Complete the following steps to configure buffering. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/system/buffering http:  localhost:5590 api v1 configuration system buffering REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for buffering into the file. For sample JSON, see Examples - Retrieve the buffering configuration . Update the example JSON parameters for your environment. For a table of all available parameters, see Buffering parameters . Save the file. For example, as ConfigureBuffering.json . Open a command line session. Change directory to the location of ConfigureBuffering.json . Enter the following cURL command (which uses the PUT method) to initialize the buffering configuration. curl -d \"@ConfigureBuffering.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/system/buffering\" \"http:  localhost:5590 api v1 configuration system buffering\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or replacing a buffering configuration, see REST URLs . Buffering schema The full schema definition for the system buffering is in the System_Buffering_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux: /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas Buffering parameters The following parameters are available for configuring buffering: Parameter Required Type Description EnablePersistentBuffering Optional boolean Enables or disables on-disk buffering Allowed value: true or false Default value: true Note: If you disable persistent buffering, in-memory buffering is used. On-disk and in-memory buffering are limited by value in the MaxBufferSizeMB property. MaxBufferSizeMB Optional integer Defines the maximum size of the buffer that is persisted on disk 1 or used in memory 2 . The unit is specified in MB (1 Megabyte = 1048576 bytes). Consider the capacity and the type of storage medium to determine a suitable value for this parameter. Minimum value: 1 Maximum value: 2147483647 Default value: 1024 Note: The MaxBufferSizeMB property is applied to each configured endpoint. For example, if you set the MaxBufferSizeMB to 1024 and you configured the adapter to send data to two endpoints (for example, PI Server and OCS), the total maximum resources used for buffering will be 2048 . The health endpoint is an exception fixed at 20 MB. BufferLocation Required string Defines the location of the buffer files. Absolute paths are required. Consider the access-control list (ACL) when you set this parameter. BufferLocation is used to buffer files when EnablePersistentBuffering is true . Allowed value: Valid path to a folder location in the file system Default value: Windows: %ProgramData%\\OSIsoft\\Adapters\\{AdapterInstance}\\Buffers Linux: /usr/share/OSIsoft/Adapters/{AdapterInstance}/Buffers  usr share OSIsoft Adapters {AdapterInstance} Buffers 1 Buffering to disk - disk is only used if required; Data is only written to the disk buffer if queued in the memory buffer for more than 5 seconds. The MaxBufferSizeMB is applied per configured endpoint except the health endpoint. An adapter creates 20 MB buffer files that are stored in BufferLocation . When MaxBufferSizeMB is reached, the oldest buffer file is deleted and a new buffer file is created. The health endpoint is fixed at 20 MB. When the health endpoint buffer file becomes full, a new buffer file is created and the previous buffer file is deleted. Note: The following rules apply in case of an error when creating a new buffer file: Attempt to delete oldest buffer file and retry. If unable to buffer, errors are logged to indicate data loss. If a buffer file is corrupted, an attempt is made to recover individual records and any failure to recover records is logged. 2 Buffering only to memory : The MaxBufferSizeMB is applied per configured endpoint except the health endpoint. When MaxBufferSizeMB is reached, the oldest messages in the memory buffer are removed. Depending on the size of a new message, several old messages may be removed. The health endpoint is fixed at 20 MB. When the health endpoint buffer file becomes full, the oldest messages in the memory buffer are removed and new messages are added. Examples The following examples are buffering configurations made through the curl REST client. Retrieve the buffering configuration curl -X GET \"http://localhost:5590/api/v1/configuration/system/buffering\" \"http:  localhost:5590 api v1 configuration system buffering\" Sample output: { \"bufferLocation\": \"C:/ProgramData/OSIsoft/Adapters/\u003cAdapterName\u003e/Buffers\", \"C: ProgramData OSIsoft Adapters \u003cAdapterName\u003e Buffers\", \"maxBufferSizeMB\": 1024, \"enablePersistentBuffering\": true } 200 OK response indicates success. Update MaxBufferSizeMb parameter curl -d \"{ \\\"MaxBufferSizeMB\\\": 100 }\" -H \"Content-Type: application/json\" application json\" -X PATCH \"http://localhost:5590/api/v1/configuration/system/buffering\" \"http:  localhost:5590 api v1 configuration system buffering\" 204 No Content response indicates success. REST URLs Relative URL HTTP verb Action api/v1/configuration/system/buffering api v1 configuration system buffering GET Gets the buffering configuration api/v1/configuration/system/buffering api v1 configuration system buffering PUT Replaces the existing buffering configuration api/v1/configuration/system/buffering api v1 configuration system buffering PATCH Update parameter, partial configuration"
                                                                 },
    "content/main/shared-content/configuration/configuration-tools.html":  {
                                                                               "href":  "content/main/shared-content/configuration/configuration-tools.html",
                                                                               "title":  "Configuration tools",
                                                                               "keywords":  "Configuration tools You can configure PI adapters with the EdgeCmd utility, OSIsoft\u0027s proprietary tool for configuring adapters, or a commonly-used REST tool. EdgeCmd utility The EdgeCmd utility enables adapter configuration on both Linux and Windows operating systems. For more information on using the EdgeCmd utility, see the EdgeCmd utility documentation . REST tools The following tools are available to make REST calls: curl curl is a command line tool used to make HTTP calls and is supported on both Windows and Linux operating systems. You can script curl with Bash or PowerShell on Linux or Windows and you can use it to perform adapter administrative and programming tasks. curl commands are used in configuration and management examples throughout this document. For more information, see curl (https://curl.haxx.se/) (https:  curl.haxx.se ) . Postman Postman is a REST tool for systems with GUI components. PI adapters are supported on platforms without GUIs. Postman is particularly useful for learning more about PI Adapter REST APIs. For more information, see Postman (https://www.postman.com/) (https:  www.postman.com ) ."
                                                                           },
    "content/main/shared-content/configuration/data-filters.html":  {
                                                                        "href":  "content/main/shared-content/configuration/data-filters.html",
                                                                        "title":  "Data filters",
                                                                        "keywords":  "Data filters PI adapters can be configured to perform data filtering to save network bandwidth. Every data item in the data selection configuration can be assigned the Id of a data filter. The adapter will then filter data for those data items based on the data filter configuration. Note: If data filters are enabled and data quality changes, both the old and current data quality values are passed on. Configure data filters Complete the following steps to configure data filters. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/DataFilters http:  localhost:5590 api v1 configuration \u003cComponentId\u003e DataFilters REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for data filters into the file. For sample JSON, see Data filters example . Update the example JSON parameters for your environment. For a table of all available parameters, see Data filters parameters . Save the file. For example, as ConfigureDataFilters.json . Open a command line session. Change directory to the location of ConfigureDataFilters.json . Enter the following cURL command (which uses the PUT method) to initialize the data filters configuration. curl -d \"@ConfigureDataFilters.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/DataFilters\" \"http:  localhost:5590 api v1 configuration \u003cComponentId\u003e DataFilters\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or deleting a data filters configuration, see REST URLs . On successful execution, the change that you have made to data filters takes effect immediately during runtime. Data filters schema The full schema definition for the data filters configuration is in the AdapterName_DataFilters_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux: /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas Data filters parameters The following parameters are available for configuring data filters: Parameter Required Type Description Id Required string Unique identifier for the data filter. Allowed value: any string identifier AbsoluteDeadband Optional double Specifies the absolute change in data value that should cause the current value to pass the filter test. Note: You must specify AbsoluteDeadband or PercentChange . Allowed value: double value representing absolute deadband number Default value: null PercentChange Optional double Specifies the percent change from previous value that should cause the current value to pass the filter test. Note: You must specify AbsoluteDeadband or PercentChange . Allowed value: double value representing percent change Default value: null ExpirationPeriod Optional timespan The length in time that can elapse after an event before automatically sending the next event, regardless of whether the next event passes the filter or not. The expected format is HH:MM:SS.### or SSS.* Allowed value: any timespan Default value: null * Note: For example, \"ExpirationPeriod\": 5:00 and \"ExpirationPeriod\": 300 both specify an expiration period of 5 minutes and 0 seconds. Data filters example [ { \"Id\": \"DuplicateData\", \"AbsoluteDeadband\": 0, \"PercentChange\": null, \"ExpirationPeriod\": \"01:00:00\" } ] REST URLs Relative URL HTTP verb Action api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters GET Gets all configured data filters. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters DELETE Deletes all configured data filters. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters POST Adds an array of data filters or a single data filter. Fails if any data filter already exists. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters PUT Replaces all data. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters PATCH Allows partial updating of configured data filter. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters/  DataFilters  id GET Gets configured data filter by id . api/v1/configuration/ api v1 configuration  ComponentId /DataFilters/  DataFilters  id DELETE Deletes configured data filter by id . api/v1/configuration/ api v1 configuration  ComponentId /DataFilters/  DataFilters  id PUT Replaces data filter by id . Fails if data filter does not exist. Note: Replace ComponentId with the Id of your adapter component."
                                                                    },
    "content/main/shared-content/configuration/diagnostics-and-metadata.html":  {
                                                                                    "href":  "content/main/shared-content/configuration/diagnostics-and-metadata.html",
                                                                                    "title":  "Diagnostics and metadata",
                                                                                    "keywords":  "Diagnostics and metadata You can configure PI adapters to produce and store diagnostics data at a designated health endpoint, and to send metadata for created streams. For more information about available diagnostics data, see Adapter diagnostics and Egress diagnostics . For more information about available metadata and what metadata are sent per metadata level, see Adapter Metadata . Configure general Start any of the Configuration tools capable of making HTTP requests. Run a PUT command to the following endpoint, setting EnableDiagnostics to either true or false , MetadataLevel to None , Low , Medium , or High and HealthPrefix to a string or null : http://localhost:5590/api/v1/configuration/system/general http:  localhost:5590 api v1 configuration system general Note: 5590 is the default port number. If you selected a different port number, replace it with that value. Example using curl : curl -d \"{ \\\"EnableDiagnostics\\\":true, \\\"MetadataLevel\\\":Medium, \\\"HealthPrefix\\\":\\\"Machine1\\\" }\" -X PUT \"http://localhost:5590/api/v1/configuration/system/general\" \"http:  localhost:5590 api v1 configuration system general\" General schema The full schema definition for the general configuration is in the System_General_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux: /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas General parameters The following parameters are available for configuring general: Parameter Required Type Description EnableDiagnostics Optional boolean Determines if diagnostics are enabled Allowed value: true or false Default value: true MetadataLevel Optional reference Defines amount of metadata sent to OMF endpoints. Allowed value: None , Low , Medium , and High Default value: Medium HealthPrefix Optional reference Prefix to use for health and diagnostics stream and asset IDs. Default value: null Example Retrieve the general configuration Example using curl : curl -X GET \"http://localhost:{port}/api/v1/configuration/system/general\" \"http:  localhost:{port} api v1 configuration system general\" Sample output: { \"EnableDiagnostics\": true, \"MetadataLevel\": \"Medium\", \"HealthPrefix\": \"Machine1\" } REST URLs Relative URL HTTP verb Action api/v1/configuration/system/General api v1 configuration system General GET Gets the general configuration api/v1/configuration/system/General api v1 configuration system General PUT Replaces the existing general configuration api/v1/configuration/system/General api v1 configuration system General PATCH Allows partial updating of general configuration"
                                                                                },
    "content/main/shared-content/configuration/discovery.html":  {
                                                                     "href":  "content/main/shared-content/configuration/discovery.html",
                                                                     "title":  "Discovery",
                                                                     "keywords":  "Discovery You can perform a data discovery for existing data items on demand. Data discovery is initiated through REST calls and it is tied to a specific discovery Id, which you can either specify or let the adapter generate it. Data discovery includes different routes. For example, you can choose to do the following: Retrieve the discovery results Query the discovery status Cancel or delete discoveries Merge discovery results with the data selection configuration Retrieve results from a current discovery and compare it with results from a previous or discovery Retrieve results from a current discovery and compare it with results from a current data selection configuration Configure discovery Start any of the Configuration tools capable of making HTTP requests. Run a POST command with the Id of the discovery and autoSelect set to either true or false to the following endpoint: http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Discoveries http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Discoveries . Notes: Including an Id is optional. If you do not include one, the adapter automatically generates one. 5590 is the default port number. If you selected a different port number, replace it with that value. Example using curl : curl -d \"{ \\\"Id\\\":\\\"TestDiscovery\\\", \\\"autoSelect\\\":true }\" -H \"Content-Type:application/json\" \"Content-Type:application json\" -X POST \"http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Discoveries\" \"http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Discoveries\" Discovery parameters Parameter Type Description id string The Id of the discovery Notes: ??? You cannot run multiple discoveries with the same Id. ??? Including an id is optional. If you do not include one, the adapter automatically generates one. query string A filter that is specific to the data source. The query filter can limit the scope of the discovery. For more information, see the Data source configuration topic. startTime datetime Time when the discovery started endTime datetime Time when the discovery ended progress double Progress of the discovery itemsFound integer Number of data items that the discovery found on the data source newItems integer Number of new data items that the discovery found in comparison to the previous discovery resultUri integer URL at which you can access the results of the discovery autoSelect boolean When set to true , the result of the discovery gets pushed to the data selection configuration. status reference Status of the discovery, for example Active or Complete errors string Errors encountered during the discovery Discoveries status example The following example shows the status of all discoveries. The discovery id in this example was auto-generated. [ { \"id\": \"8ff855f1-a636-490a-bb31-207410a6e607\", \"query\": null, \"startTime\": \"2020-09-30T19:34:01.8180401+02:00\", \"endTime\": \"2020-09-30T19:34:01.8368776+02:00\", \"progress\": 30, \"itemsFound\": 4, \"newItems\": 0, \"resultUri\": \"http://127.0.0.1:5590/api/v1/Configuration/\u003cComponentId\u003e/Discoveries/8ff855f1-a636-490a-bb31-207410a6e607/result\", \"http:  127.0.0.1:5590 api v1 Configuration \u003cComponentId\u003e Discoveries 8ff855f1-a636-490a-bb31-207410a6e607 result\", \"autoSelect\": false, \"status\": \"Complete\", \"errors\": null } ] REST URLs Relative URL HTTP verb Action api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries  discoveries GET Returns status of all discoveries api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries  discoveries POST Initiates a new discovery and returns its Id api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries  discoveries DELETE Cancels and deletes all saved discoveries api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries/  discoveries  \u003cdiscoveryId\u003e GET Gets the status of an individual discovery Note: If a discovery with the specified Id does not exist, you will get an error message api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries/  discoveries  \u003cdiscoveryId\u003e DELETE Cancels and deletes discovery and result api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries/  discoveries  \u003cdiscoveryId\u003e /result  result GET Returns the result of a discovery api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries/  discoveries  \u003cdiscoveryId\u003e /result?diff=  result?diff= previousId GET Returns the difference between the result and the previous result api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /dataselection?diff=  dataselection?diff= \u003cdiscoveryId\u003e GET Returns the difference between the data selection configuration and the discovery results api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries/  discoveries  \u003cdiscoveryId\u003e /result  result DELETE Cancels and deletes discovery result Note: The discovery Id is still valid, but a query will contain a status of canceled Only the Status property will contain a canceled status, but not the query api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /discoveries/  discoveries  \u003cdiscoveryId\u003e /cancel  cancel POST Cancels the on-demand data source discovery api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /dataselection/select?discoveryid=  dataselection select?discoveryid= \u003cdiscoveryId\u003e POST Adds the discovered items to data selection with selected set to true api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /dataselection/unselect?discoveryid=  dataselection unselect?discoveryid= \u003cdiscoveryId\u003e POST Adds the discovered items to data selection with selected set to false Note: Replace \u003ccomponentId\u003e with the Id of your adapter component. Replace \u003cdiscoveryId\u003e with the Id of the discovery for which you want to perform the action."
                                                                 },
    "content/main/shared-content/configuration/egress-endpoints.html":  {
                                                                            "href":  "content/main/shared-content/configuration/egress-endpoints.html",
                                                                            "title":  "Egress endpoints",
                                                                            "keywords":  "Egress endpoints PI adapters collect time series data, which they can send to a permanent data store (endpoint). This operation is called data egress. The following endpoints are available for data egress: OSIsoft Cloud Services (OCS) PI servers through PI Web API For long term storage and analysis, you can configure any adapter to send time series data to one or several of these endpoints in any combination. An egress endpoint is comprised of the properties specified under Egress endpoint parameters . Data egress to a PI server creates a PI point in the PI adapter configuration. Data egress to OCS creates a stream in the PI adapter configuration. The name of the PI point or OCS stream is a combination of the StreamIdPrefix specified in the adapter data source configuration and the StreamId specified in the adapter data selection configuration. Configure egress endpoints Complete the following steps to configure egress endpoints. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/OmfEgress/dataendpoints http:  localhost:5590 api v1 configuration OmfEgress dataendpoints REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for egress endpoints into the file. For sample JSON, see Examples . Update the example JSON parameters for your environment. For a table of all available parameters, see Egress endpoint parameters . Save the file. For example, as ConfigureEgressEndpoints.json . Open a command line session. Change directory to the location of ConfigureEgressEndpoints.json . Enter the following cURL command (which uses the PUT method) to initialize the egress endpoints configuration. curl -d \"@ConfigureEgressEndpoints.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/OmfEgress/dataendpoints\" \"http:  localhost:5590 api v1 configuration OmfEgress dataendpoints\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or replacing an egress endpoints configuration, see REST URLs . Egress endpoint configuration schema The full schema definition for the egress endpoint configuration is in the OmfEgress_DataEndpoints_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux: /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas Egress endpoint parameters The following parameters are available for configuring egress endpoints: Parameter Required Type Description Id Optional string Unique identifier Allowed value: any string identifier Default value: new GUID Endpoint Required string Destination that accepts OMF v1.2 messages. Supported destinations include OCS and PI Server. Allowed value: well-formed http or https endpoint string Default: null Username Required for PI server endpoint string Basic authentication to the PI Web API OMF endpoint PI server: Allowed value: any string Default: null Note: If your username contains a backslash, you must add an escape character, for example, type OilCompany\\TestUser as OilCompany\\\\TestUser . Password Required for PI server endpoint string Basic authentication to the PI Web API OMF endpoint PI server: Allowed value: any string Default: null ClientId Required for OCS endpoint string Authentication with the OCS OMF endpoint Allowed value: any string, can be null if the endpoint URL schema is HTTP Default: null ClientSecret Required for OCS endpoint string Authentication with the OCS OMF endpoint Allowed value: any string, can be null if the endpoint URL schema is HTTP Default: null TokenEndpoint Optional for OCS endpoint string Retrieves an OCS token from an alternative endpoint Allowed value: well-formed http or https endpoint string Default value: null ValidateEndpointCertificate Optional boolean Disables verification of destination certificate. Note: Only use for testing with self-signed certificates. Allowed value: true or false Default value: true Special characters encoding The adapter encodes special characters used in the data selection StreamId parameter string before sending it to configured endpoints. The encoded characters look as follows: Special character Encoded character * %2a \u0027 %27 ` %60 \" %22 ? %3f ; %3b | %7c \\ %5c { %7b } %7d [ %5b ] %5d Examples The following examples are valid egress configurations: Egress data to OCS [{ \"Id\": \"OCS\", \"Endpoint\": \"https://\u003cOCS \"https:  \u003cOCS OMF endpoint\u003e\", \"ClientId\": \"\u003cclientid\u003e\", \"ClientSecret\": \"\u003cclientsecret\u003e\" }] Egress data to PI Web API [{ \"Id\": \"PI Web API\", \"Endpoint\": \"https://\u003cpi \"https:  \u003cpi web api server\u003e:\u003cport\u003e/piwebapi/omf/\", server\u003e:\u003cport\u003e piwebapi omf \", \"UserName\": \"\u003cusername\u003e\", \"Password\": \"\u003cpassword\u003e\" }] REST URLs Relative URL HTTP verb Action api/v1/configuration/omfegress/DataEndpoints api v1 configuration omfegress DataEndpoints GET Gets all configured egress endpoints api/v1/configuration/omfegress/DataEndpoints api v1 configuration omfegress DataEndpoints DELETE Deletes all configured egress endpoints api/v1/configuration/omfegress/DataEndpoints api v1 configuration omfegress DataEndpoints POST Adds an array of egress endpoints or a single endpoint. Fails if any endpoint already exists api/v1/configuration/omfegress/DataEndpoints api v1 configuration omfegress DataEndpoints PUT Replaces all egress endpoints api/v1/configuration/omfegress/DataEndpoints api v1 configuration omfegress DataEndpoints PATCH Allows partial updating of configured endpoints. Note: The request must be an array containing one or more endpoints. Each endpoint in the array must include its Id . api/v1/configuration/omfegress/DataEndpoints/{Id} api v1 configuration omfegress DataEndpoints {Id} GET Gets configured endpoint by Id api/v1/configuration/omfegress/DataEndpoints/{Id} api v1 configuration omfegress DataEndpoints {Id} DELETE Deletes configured endpoint by Id api/v1/configuration/omfegress/DataEndpoints/{Id} api v1 configuration omfegress DataEndpoints {Id} PUT Updates or creates a new endpoint with the specified Id api/v1/configuration/omfegress/DataEndpoints/{Id} api v1 configuration omfegress DataEndpoints {Id} PATCH Allows partial updating of configured endpoint by Id Egress execution details After configuring an egress endpoint, egress is immediately run for that endpoint. Egress is handled individually per configured endpoint. When data is egressed for the first time, types and containers are egressed to the configured endpoint. After that only new or changed types or containers are egressed. Type creation must be successful in order to create containers. Container creation must be successful in order to egress data. If you delete an egress endpoint, data flow immediately stops for that endpoint. Buffered data in a deleted endpoint is permanently lost. Type, container, and data items are batched into one or more OMF messages when egressing. As per the requirements defined in OMF, a single message payload will not exceed 192KB in size. Compression is automatically applied to outbound egress messages. On the egress destination, failure to add a single item results in the message failing. Types, containers, and data are egressed as long as the destination continues to respond to HTTP requests."
                                                                        },
    "content/main/shared-content/configuration/egress-endpoints/configure-a-network-proxy.html":  {
                                                                                                      "href":  "content/main/shared-content/configuration/egress-endpoints/configure-a-network-proxy.html",
                                                                                                      "title":  "Configure a network proxy",
                                                                                                      "keywords":  "Configure a network proxy Some network architectures may need a network proxy between the PI adapter and the egress endpoint. The process for configuring the adapter to egress data through a network proxy varies depending on the proxy type. HTTPS forward proxy For the adapter to use an HTTPS forward proxy while egressing, configure the https_proxy environment variable. For information on how to configure system environment variables, refer to your platform specific documentation: Windows: setx Ubuntu: EnvironmentVariables Debian: EnvironmentVariables Docker: Environment variables in Compose The value of this environment variable must contain the URL of the proxy server, beginning with http . The format of the string is [user[:password]@]http://hostname[:port] [user[:password]@]http:  hostname[:port] . HTTPS proxy environment variable Parameter Required Description user Optional The user name for the HTTPS forward proxy. password Optional The password for the HTTPS forward proxy specified user name. If you specify user , password remains optional. port Optional If you do not specify port , the default 80 is used. Note: Usage of the https_proxy environment variable may affect other .NET or .NET Core applications. If you set this environment variable, it will affect the adapter egress endpoints and the adapter health endpoints. Examples: myUser@http://192.168.2.2 myUser@http:  192.168.2.2 myUser:myPassword@http://proxymachine.domain:3128 myUser:myPassword@http:  proxymachine.domain:3128 http://proxymachine.domain http:  proxymachine.domain In Windows, this may look something like: Example of an architecture with an https forward proxy: Reverse proxy For the adapter to use a reverse proxy while egressing, you must configure the reverse proxy as an egress endpoint. For information on how to configure an egress endpoint, see Egress endpoints configuration . Example: [{ \"Id\": \"PI Web API Through Proxy\", \"Endpoint\": \"https://\u003creverseProxy\u003e:\u003cport\u003e/piwebapi/omf/\", \"https:  \u003creverseProxy\u003e:\u003cport\u003e piwebapi omf \", \"UserName\": \"\u003cpiWebApiUser\u003e\", \"Password\": \"\u003cpiWebApiPassword\u003e\" }] Example of an architecture with a reverse proxy:"
                                                                                                  },
    "content/main/shared-content/configuration/egress-endpoints/prepare-egress-destinations.html":  {
                                                                                                        "href":  "content/main/shared-content/configuration/egress-endpoints/prepare-egress-destinations.html",
                                                                                                        "title":  "Prepare egress destinations",
                                                                                                        "keywords":  "Prepare egress destinations OCS and PI Server destinations may require additional configuration to receive Open Message Format (OMF) messages. OCS To prepare OCS to receive OMF messages from the adapter, create an OMF connection in OCS. Creating an OMF connection results in an available OMF endpoint that can be used by the adapter egress mechanism. Complete the following steps to create an OMF connection: Create a Client . The Client Id and Client Secret will be used for the corresponding properties in the egress configuration. Create an OMF type Connection . The connection should link the created client to an existing namespace where the data will be stored. The OMF Endpoint URL for the connection will be used as the egress configuration Endpoint property. PI Server To prepare a PI Server to receive OMF messages from the adapter, a PI Web API OMF endpoint must be available. Complete the following steps: Install PI Web API and enable the Open Message Format (OMF) Services feature. During configuration, choose an AF database and PI Data Archive where metadata and data will be stored. The account used in an egress configuration needs permissions to create AF elements, element templates, and PI points. Configure PI Web API to use Basic authentication. For complete steps, as well as best practices and recommendations, see the following topic in the PI Web API User Guide: Authentication methods . Notes: The certificate used by PI Web API must be trusted by the device running the adapter, otherwise the egress configuration ValidateEndpointCertificate property needs to be set to false (this can be the case with a self-signed certificate but should only be used for testing purposes). To continue to send OMF egress messages to the PI Web API endpoint after upgrading PI Web API, restart the adapter service."
                                                                                                    },
    "content/main/shared-content/configuration/health-endpoints.html":  {
                                                                            "href":  "content/main/shared-content/configuration/health-endpoints.html",
                                                                            "title":  "Health endpoints",
                                                                            "keywords":  "Health endpoints You can configure PI adapters to produce and store health data at a designated health endpoint. You can use health data to ensure that your adapters are running properly and that data flows to the configured OMF endpoints. For more information about adapter health, see Adapter health . Configure health endpoint A health endpoint designates an OMF endpoint where adapter health information is sent. You can configure multiple health endpoints. Complete the following steps to configure health endpoints. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/system/healthendpoints http:  localhost:5590 api v1 configuration system healthendpoints REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for health endpoints into the file. For sample JSON, see Examples . Update the example JSON parameters for your environment. For a table of all available parameters, see Health endpoint parameters . Save the file. For example, as ConfigureHealthEndpoints.json . Open a command line session. Change directory to the location of ConfigureHealthEndpoints.json . Enter the following cURL command (which uses the PUT method) to initialize the health endpoint configuration. curl -d \"@ConfigureHealthEndpoints.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/system/healthendpoints\" \"http:  localhost:5590 api v1 configuration system healthendpoints\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or replacing a health endpoints configuration, see REST URLs . Health endpoints schema The full schema definition for the health endpoint configuration is in the System_HealthEndpoints_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux: /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas Health endpoint parameters The following parameters are available for configuring health endpoints: Parameter Required Type Description Id Optional string Uniquely identifies the endpoint. This can be any alphanumeric string. If left blank, a unique value is generated automatically. Allowed value: any string identifier Default value: new GUID Endpoint Required string The URL of the OMF endpoint to receive this health data Allowed value: well-formed http or https endpoint string Default: null Username Required for PI Web API endpoints string The username used to authenticate with a PI Web API OMF endpoint PI server: Allowed value: any string Default: null Password Required for PI Web API endpoints string The password used to authenticate with a PI Web API OMF endpoint PI server: Allowed value: any string Default: null ClientId Required for OCS endpoints string The client ID used for authentication with an OSIsoft Cloud Services OMF endpoint Allowed value: any string Default: null ClientSecret Required for OCS endpoints string The client secret used for authentication with an OSIsoft Cloud Services OMF endpoint Allowed value: any string Default: null TokenEndpoint Optional for OCS endpoints string Retrieves an OCS token from an alternative endpoint Allowed value: well-formed http or https endpoint string Default value: null ValidateEndpointCertificate Optional boolean Disables verification of destination security certificate. Use for testing only with self-signed certificates; OSIsoft recommends keeping this set to the default, true, in production environments. Allowed value: true or false Default value: true Examples OCS endpoint { \"Id\": \"OCS\", \"Endpoint\": \"https://\u003cOCS \"https:  \u003cOCS OMF endpoint\u003e\", \"ClientId\": \"\u003cclientid\u003e\", \"ClientSecret\": \"\u003cclientsecret\u003e\" } PI Web API endpoint { \"Id\": \"PI Web API\", \"Endpoint\": \"https://\u003cpi \"https:  \u003cpi web api server\u003e:\u003cport\u003e/piwebapi/omf/\", server\u003e:\u003cport\u003e piwebapi omf \", \"UserName\": \"\u003cusername\u003e\", \"Password\": \"\u003cpassword\u003e\" } Note: When you use an adapter with a PI Web API health endpoint, the AF structure is required. If the elements are deleted, the adapter recreates the elements; if the account used to authenticate to the PI Web API has its permissions removed on the AF Server, the adapter retries sending health data to the PI Web API until the permissions are restored. REST URLs Relative URL HTTP verb Action api/v1/configuration/system/healthEndpoints api v1 configuration system healthEndpoints GET Gets all configured health endpoints api/v1/configuration/system/healthEndpoints api v1 configuration system healthEndpoints DELETE Deletes all configured health endpoints api/v1/configuration/system/healthEndpoints api v1 configuration system healthEndpoints POST Adds an array of health endpoints or a single endpoint. Fails if any endpoint already exists api/v1/configuration/system/healthEndpoints api v1 configuration system healthEndpoints PUT Replaces all health endpoints. Note: Requires an array of endpoints api/v1/configuration/system/healthEndpoints api v1 configuration system healthEndpoints PATCH Allows partial updating of configured health endpoints Note: The request must be an array containing one or more health endpoints. Each health endpoint in the array must include its Id . api/v1/configuration/system/healthEndpoints/ api v1 configuration system healthEndpoints  Id GET Gets configured health endpoint by Id api/v1/configuration/system/healthEndpoints/ api v1 configuration system healthEndpoints  Id DELETE Deletes configured health endpoint by Id api/v1/configuration/system/healthEndpoints/ api v1 configuration system healthEndpoints  Id PUT Updates or creates a new health endpoint with the specified Id api/v1/configuration/system/healthEndpoints/ api v1 configuration system healthEndpoints  Id PATCH Allows partial updating of configured health endpoint by Id Note: Replace Id with the Id of the health endpoint."
                                                                        },
    "content/main/shared-content/configuration/history-recovery.html":  {
                                                                            "href":  "content/main/shared-content/configuration/history-recovery.html",
                                                                            "title":  "History recovery",
                                                                            "keywords":  "History recovery The adapter you are using supports the following data collection modes which you configure in the DataCollectionMode parameter of your adapter\u0027s data source configuration: CurrentOnly : The adapter component operates normally. History recovery is disabled. CurrentWithBackfill (Default): The adapter component operates normally, but disconnections and shutdown events are recorded in the form of recovery intervals. When the adapter is reconnected to a data source, it automatically backfills data for the recorded intervals. HistoryOnly : The adapter component does not get started. The adapter is able to start collecting historical data on demand. History recovery for adapters supports the following two operations related to the data collection mode: On demand history recovery : Recovers data from a specified start time or start and end time. If end time is not specified, the default is utcnow . On demand history recovery is available only when the adapter is in HistoryOnly data collection mode. Limited automatic history recovery : Backfills data gaps that originated from connection disruptions, data source issues, or PI adapter shutdown or both. This is limited to a maximum time-range of four days. Limited automatic history recovery is available only when the adapter is in CurrentWithBackfill data collection mode."
                                                                        },
    "content/main/shared-content/configuration/history-recovery/automatic-history-recovery.html":  {
                                                                                                       "href":  "content/main/shared-content/configuration/history-recovery/automatic-history-recovery.html",
                                                                                                       "title":  "Automatic history recovery",
                                                                                                       "keywords":  "Automatic history recovery Besides on-demand history recovery, the PI adapter also supports automatic history recovery. For automatic history recovery, the adapter tracks changes to the DeviceStatus of each component. When the DeviceStatus changes to DeviceInError or Shutdown , the adapter starts a new History recovery interval . When the issue resolves or if the adapter is restarted and the DeviceStatus changes to Good , the adapter closes any current intervals for that component. The adapter tracks these intervals for each component and, when DeviceStatus has a value of Good , it performs history recovery for these intervals starting from oldest to newest. For more information, see also Device status . Note: If the data collection mode is set to CurrentWithBackfill , the adapter clears periods not recovered for the component and stops keeping track of them. Only if the data collection mode is set to HistoryOnly , an automatic history recovery operation in progress will be canceled, otherwise it will be finished. History recovery intervals Automatic history intervals cannot be longer than four days. If an interval is longer than four days, the adapter automatically changes the start time of the interval to be no earlier than four days before the end time prior to starting a recovery. If a current outage lasts longer than four days, when the device status finally improves the adapter recovers up to four days before the current time. This avoids introducing additional data gaps."
                                                                                                   },
    "content/main/shared-content/configuration/history-recovery/on-demand-history-recovery-configuration.html":  {
                                                                                                                     "href":  "content/main/shared-content/configuration/history-recovery/on-demand-history-recovery-configuration.html",
                                                                                                                     "title":  "On-demand history recovery configuration",
                                                                                                                     "keywords":  "On-demand history recovery configuration The PI adapter supports performing history recovery on-demand by specifying start and end time. Configure history recovery Start any of the Configuration tools capable of making HTTP requests. Run a POST command with the Id of the history recovery, and the startTime and endTime to the following endpoint: http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/HistoryRecoveries http:  localhost:5590 api v1 configuration \u003cComponentId\u003e HistoryRecoveries . Example using curl : curl -d \"{ \\\"Id\\\":\\\"TestRecovery\\\", \\\"startTime\\\":\\\"2021-03-29T14:00:30Z\\\", \\\"endTime\\\":\\\"2021-03-29T15:00:15Z\\\" }\" -X POST \"http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/HistoryRecoveries\" \"http:  localhost:5590 api v1 configuration \u003cComponentId\u003e HistoryRecoveries\" Note: 5590 is the default port number. If you selected a different port number, replace it with that value. If you do not specify an Id, the endpoint generates a unique Id. History recovery parameters Parameter Type Description Id string The Id of the history recovery Note: You cannot run multiple history recoveries with the same Id. StartTime datetime Time when the the first data items are collected. EndTime datetime Time when the last data items are collected. Checkpoint datetime The latest timestamp that the history recovery has completed with the range being between startTime and endTime . Items double The amount of data selection items in the history recovery operation. RecoveredEvents double Number of events that the history recovery found on the data source. Progress double Progress of the history recovery (number of data items found through the history recovery). Status enum Status of the history recovery. The following statuses are available: - Active - The operation is currently in progress - Complete - The operation has been completed - Canceled - The operation has been canceled - Failed - The operation failed Errors string Errors encountered during the history recovery. History recovery status example [ { \"Id\": \"HistoryRecovery1\", \"StartTime\": \"2021-01-09T05:55:00.0\", \"EndTime\": \"2021-01-26T13:20:00.0\", \"CheckPoint\": \"2021-01-13T14:55:00.0\", \"Items\": 7000, \"RecoveredEvents\": 800000, \"Progress\": 20, \"Status\": \"Active\", \"Errors\": null } ] Note: The result of the history recovery operation is added to the \u003ccomponentId\u003e_historyRecoveries.json file. REST URLs Relative URL HTTP verb Action api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /historyRecoveries  historyRecoveries GET Returns all history recoveries statuses api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /historyRecoveries  historyRecoveries POST Initiates a new history recovery, returns the id of the operation api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /historyRecoveries  historyRecoveries DELETE Cancels all active history recovery operations and removes states api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /historyRecoveries/  historyRecoveries  \u003coperationId\u003e GET Gets the status of an individual history recovery api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /historyRecoveries/  historyRecoveries  \u003coperationId\u003e DELETE Cancels history recovery and removes the state api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /historyRecoveries/  historyRecoveries  \u003coperationId\u003e /cancel  cancel POST Cancels history recovery api/v1/configuration/ api v1 configuration  \u003ccomponentId\u003e /historyRecoveries/  historyRecoveries  \u003coperationId\u003e /resume  resume POST Resumes canceled or failed history recovery operation ( 202 ) from the checkpoint Note: If the \u003coperationId\u003e is not found, a 404 HTTP error message will be returned Note: Replace \u003ccomponentId\u003e with the Id of your adapter component. Replace \u003coperationId\u003e with the Id of the history recovery operation for which you want to perform the action."
                                                                                                                 },
    "content/main/shared-content/configuration/logging.html":  {
                                                                   "href":  "content/main/shared-content/configuration/logging.html",
                                                                   "title":  "Logging",
                                                                   "keywords":  "Logging PI adapters write daily log messages for the adapter, the system, and OMF egress to flat text files in the following locations: ??? Windows: %ProgramData%\\OSIsoft\\Adapters{AdapterInstance}\\Logs ??? Linux: /usr/share/OSIsoft/Adapters/{AdapterInstance}/Logs  usr share OSIsoft Adapters {AdapterInstance} Logs Each message in the log displays the message severity level, timestamp, and the message itself. Configure logging Complete the following steps to configure logging. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Logging http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Logging REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for logging into the file. For sample JSON, see Example . Update the example JSON parameters for your environment. For a table of all available parameters, see Logging parameters . Save the file. For example, as ConfigureLogging.json . Open a command line session. Change directory to the location of ConfigureLogging.json . Enter the following cURL command (which uses the PUT method) to initialize the logging configuration. curl -d \"@ConfigureLogging.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Logging\" \"http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Logging\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or retrieving a logging configuration, see REST URLs . Any parameter not specified in the updated configuration file reverts to the default schema value On successful execution, the log-level change takes effect immediately during runtime. The other configurations (log file size and file count) are updated after the adapter is restarted. Logging schema The full schema definition for the logging configuration is in the component specific logging file: AdapterName_Logging_schema.json , OmfEgress_Logging_schema.json , or System_Logging_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux: /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas Logging parameters The following parameters are available for configuring logging: Parameter Required Type Description LogLevel Optional reference The logLevel sets the minimum severity for messages to be included in the logs. Messages with a severity below the level set are not included. The log levels in their increasing order of severity are as follows: Trace , Debug , Information , Warning , Error , Critical , and None . Default log level: Information For detailed information about the log levels, see LogLevel . LogFileSizeLimitBytes Optional integer The maximum size (in bytes) of log files that the service will create for the component. The value must be a positive integer. Minimum value: 1000 Maximum value: 9223372036854775807 Default value: 34636833 LogFileCountLimit Optional integer The maximum number of log files that the service will create for the component. The value must be a positive integer. Minimum value: 1 Maximum value: 2147483647 Default value: 31 LogLevel Level Description Trace Logs that contain the most detailed messages. These messages may contain sensitive application data like actual received values, which is why these messages should not be enabled in production environment. Note: Trace is translated to Verbose in the log file. Debug Logs that can be used to troubleshoot data flow issues by recording metrics and detailed flow related information. Information Logs that track the general flow of the application. Any non-repetitive general information like the following can be useful for diagnosing potential application errors: - Version information related to the software at startup - External services used - Data source connection string - Number of measurements - Egress URL - Change of state ???Starting??? or ???Stopping??? - Configuration Warning Logs that highlight an abnormal or unexpected event in the application flow that does not otherwise cause the application execution to stop. Warning messages can indicate an unconfigured data source state, an insecure communication channel in use, or any other event that could require attention but that does not impact data flow. Error Logs that highlight when the current flow of execution is stopped due to a failure. These should indicate a failure in the current activity and not an application-wide failure. It can indicate an invalid configuration, unavailable external endpoint, internal flow error, and so on. Critical Logs that describe an unrecoverable application or system crash or a catastrophic failure that requires immediate attention. This can indicate application wide failures like beta timeout expired, unable to start self-hosted endpoint, unable to access vital resource (for example, Data Protection key file), and so on. Note: Critical is translated to Fatal in the log file. None Logging is disabled for the given component. Example Default logging configuration By default, logging captures Information, Warning, Error, and Critical messages in the message logs. The following logging configuration is the installation default for a component: { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 } REST URLs Relative URL HTTP verb Action api/v1/configuration/System/Logging api v1 configuration System Logging GET Retrieves the system logging configuration api/v1/configuration/System/Logging api v1 configuration System Logging PUT Updates the system logging configuration api/v1/configuration/ api v1 configuration  ComponentId /Logging  Logging GET Retrieves the logging configuration of the specified adapter component api/v1/configuration/ api v1 configuration  ComponentId /Logging  Logging PUT Updates the logging configuration of the specified adapter component Note: Replace ComponentId with the Id of your adapter component."
                                                               },
    "content/main/shared-content/configuration/schedules.html":  {
                                                                     "href":  "content/main/shared-content/configuration/schedules.html",
                                                                     "title":  "Schedules",
                                                                     "keywords":  "Schedules You can configure the adapter to run scans based on a schedule. Each data item can be assigned to a schedule in the data selection configuration. The adapter samples data for those data items at the scheduled time. Note: You start an ingress component without a schedule configuration, a default schedule configuration is added to use as an example. Note: When the adapter framework scheduler misses or skips a scan for any reason, either one of the following messages is printed: Scan skipped for schedule id \u003cId\u003e or Scan missed for schedule \u003cid\u003e . Configure schedules Complete the following steps to change the schedules configuration: Using any text editor, create a file that contains the schedules configuration in the JSON format. For content structure, see the example schedule configuration . For all available parameters, see the schedules parameters . Save the file. For example, ConfigureSchedules.json . Use any of the Configuration tools capable of making HTTP requests to run a PUT command with the contents of the file to the following endpoint: http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Schedules http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Schedules . Note: Replace \u003cComponentId\u003e with the ComponentId of the adapter. 5590 is the default port number. If you selected a different port number, replace it with that value. Example using curl : Note: Run this command from the same directory where the file is located. curl -d \"@ConfigureSchedules.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Schedules\" \"http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Schedules\" On successful execution, the schedules change takes effect immediately during runtime. Schedules schema The full schema definition for the schedules configuration is in the AdapterName_Schedules_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux: /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas Schedules parameters The following parameters are available for configuring schedules: Parameter Required Type Description Id Required string Unique identifier for the schedule Allowed value: any string identifier Period Required string The data sampling rate of the schedule. The expected format is HH:MM:SS.###. Invalid inputs: null , negative timespan, or zero A default value must be specified. Offset Optional string The offset from the midnight when the schedule starts. The expected format is HH:MM:SS.### Invalid input: negative timespan A default value must be specified. Note: You can also specify timespans as numbers in seconds. For example, \"Period\": 25 specifies 25 seconds, or \"Period\": 125 specifies 2 minutes and 5 seconds. Example schedule configuration The following is an example of a complete schedule configuration: [ { \"Id\": \"schedule1\", \"Period\": \"00:00:01.500\", \"Offset\": \"00:02:03\" } ] Default schedule configuration If no schedule is configured, the adapter uses the following default schedule configuration: [ { \"Id\": \"1\", \"Period\": \"0:00:05\", \"Offset\": \"0:00:00\" } ] REST URLs Relative URL HTTP verb Action api/v1/configuration/ api v1 configuration  ComponentId /Schedules  Schedules GET Gets all configured schedules api/v1/configuration/ api v1 configuration  ComponentId /Schedules  Schedules DELETE Deletes all configured schedules api/v1/configuration/ api v1 configuration  ComponentId /Schedules  Schedules POST Adds an array of schedules or a single schedule. Fails if any schedule already exists api/v1/configuration/ api v1 configuration  ComponentId /Schedules  Schedules PUT Replaces all schedules api/v1/configuration/ api v1 configuration  ComponentId /Schedules/  Schedules  id GET Gets configured schedule by id api/v1/configuration/ api v1 configuration  ComponentId /Schedules/  Schedules  id DELETE Deletes configured schedule by id api/v1/configuration/ api v1 configuration  ComponentId /Schedules/  Schedules  id PUT Replaces schedule by id . Fails if schedule does not exist api/v1/configuration/ api v1 configuration  ComponentId /Schedules/  Schedules  id PATCH Allows partial updating of configured schedule by id Note: Replace ComponentId with the Id of your adapter component."
                                                                 },
    "content/main/shared-content/configuration/system-and-adapter.html":  {
                                                                              "href":  "content/main/shared-content/configuration/system-and-adapter.html",
                                                                              "title":  "System and adapter",
                                                                              "keywords":  "System and adapter You can configure the system component and adapter component together using a single file. Change system and adapter configuration Complete the following steps to configure system and adapter. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for system and adapter into the file. For sample JSON, see the corresponding adapter configuration examples topic. Save the file. For example, as ConfigureSystemAndAdapter.json . Open a command line session. Change directory to the location of ConfigureSystemAndAdapter.json . Enter the following cURL command (which uses the PUT method) to initialize the system and adapter configuration. curl -d \"@ConfigureSystemAndAdapter.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration\" \"http:  localhost:5590 api v1 configuration\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. In order for some of the adapter specific configurations to take effect, you have to restart the adapter. Discoveries and HistoryRecoveries facet details are not required to be supplied as part of the configuration and supplied values will be ignored. Their results will be restored from the previous states. If the operation fails due to errors in the configuration, the count of the error and suitable error messages are returned in the result. REST URLs Relative URL HTTP verb Action api/v1/configuration/ api v1 configuration  PUT Replaces the configuration for the entire adapter"
                                                                          },
    "content/main/shared-content/configuration/system-components.html":  {
                                                                             "href":  "content/main/shared-content/configuration/system-components.html",
                                                                             "title":  "System components",
                                                                             "keywords":  "System components PI adapters use JSON configuration files in a protected directory on Windows and Linux to store configuration that is read on startup. While the files are accessible to view, OSIsoft recommends that you use REST or the EdgeCmd utility for any changes you make to the files. As part of making adapters as secure as possible, any passwords or secrets that you configure are stored in encrypted form where cryptographic key material is stored separately in a secure location. If you edit the files directly, the adapter may not work as expected. Note: You can edit any single component or facet of the system individually using REST, but you can also configure the system as a whole with a single REST call. Configure system components Complete the following steps to configure system components. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/system/components http:  localhost:5590 api v1 configuration system components REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for system components into the file. For sample JSON, see Examples . Update the example JSON parameters for your environment. For a table of all available parameters, see System components parameters . Save the file. For example, as ConfigureComponents.json . Open a command line session. Change directory to the location of ConfigureComponents.json . Enter the following cURL command (which uses the PUT method) to initialize the system components configuration. curl -d \"@ConfigureComponents.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/system/components\" \"http:  localhost:5590 api v1 configuration system components\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or deleting a system components configuration, see REST URLs . System components schema The full schema definition for the system components configuration is in the System_Components_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\AdapterName\\Schemas Linux: /opt/OSIsoft/Adapters/AdapterName/Schemas  opt OSIsoft Adapters AdapterName Schemas System components parameters You can configure the following parameters for system components: Parameters Required Type Description ComponentId Required string The ID of the component 1 . It can be any alphanumeric string. A properly configured ComponentID follows these rules: Cannot contain leading or trailing space Cannot use the following characters: \u003e \u003c /   : ? # [ ] @ ! $ \u0026 * \" ( ) \\\\ + , ; = ` ComponentType Required string The type of the component. There are two types of components: OmfEgress and the adapter. 1 1 Note: The OmfEgress component is required to run the adapter. Both its ComponentId and ComponentType are reserved and should not be modified. Examples Default system components configuration The default System_Components.json file for the System component contains the following information. [ { \"ComponentId\": \"OmfEgress\", \"ComponentType\": \"OmfEgress\" } ] System components configuration with two adapter instances [ { \"ComponentId\": \"\u003cAdapterName\u003e1\", \"ComponentType\": \"\u003cAdapterName\u003e\" }, { \"ComponentId\": \"\u003cAdapterName\u003e2\", \"ComponentType\": \"\u003cAdapterName\u003e\" }, { \"ComponentId\": \"OmfEgress\", \"ComponentType\": \"OmfEgress\" } ] REST URLs Relative URL HTTP verb Action api/v1/configuration/system/components api v1 configuration system components GET Retrieves the system components configuration api/v1/configuration/system/components api v1 configuration system components POST Adds a new component to the system configuration api/v1/configuration/system/components api v1 configuration system components PUT Updates the system components configuration api/v1/configuration/system/components/ api v1 configuration system components  ComponentId DELETE Deletes a specific component from the system components configuration api/v1/configuration/system/components/ api v1 configuration system components  ComponentId PUT Creates a new component with the specified ComponentId in the system configuration"
                                                                         },
    "content/main/shared-content/configuration/text-parser/jsonpath-syntax-for-value-retrieval.html":  {
                                                                                                           "href":  "content/main/shared-content/configuration/text-parser/jsonpath-syntax-for-value-retrieval.html",
                                                                                                           "title":  "JSONPath syntax for value retrieval",
                                                                                                           "keywords":  "JSONPath syntax for value retrieval For information on which semantic is used for retrieving values from JSON files, see JSONPath Syntax . The following syntax is used to extract values from JSON documents. JSON - Simple JSONPath example [ { \"time\": \"2020-08-10T12:10:46.0928791Z\", \"value\": 1.234567890 }, { \"time\": \"2020-08-10T12:10:47.0928791Z\", \"value\": 12.34567890 }, { \"time\": \"2020-08-10T12:10:48.0928791Z\", \"value\": 123.4567890 }, { \"time\": \"2020-08-10T12:10:49.0928791Z\", \"value\": 1234.567890 }, { \"time\": \"2020-08-10T12:10:50.0928791Z\", \"value\": 12345.67890 }, { \"time\": \"2020-08-10T12:10:51.0928791Z\", \"value\": 123456.7890 }, { \"time\": \"2020-08-10T12:10:52.0928791Z\", \"value\": 12345678.90 }, { \"time\": \"2020-08-10T12:10:53.0928791Z\", \"value\": 123456789.0 } ] The following JSONPath configuration reads a series of values: { \"Id\": \"DoubleValue\", \"FieldDefinition\": \"value\", \"DataType\": \"Double\" }, { \"Id\": \"Timestamp\", \"FieldDefinition\": \"time\", \"DataType\": \"DateTime\", \"IsIndex\": true } JSON - Complex JSONPath examples The following example reads specific values from a JSON array: { \"StreamData\": { \"TPPrototype.uflsample.value_time\": [ { \"StreamId\": \"TPPrototype.uflsample.value_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T06:00:00Z\", \"Value\": 339.0 }, { \"StreamId\": \"TPPrototype.uflsample.value_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T07:00:00Z\", \"Value\": 344.0 }, { \"StreamId\": \"TPPrototype.uflsample.value_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T17:00:00Z\", \"Value\": 341.0 } ], \"TPPrototype.uflsample.value_timeString\": [ { \"StreamId\": \"TPPrototype.uflsample.value_timeString\", \"DataType\": \"String\", \"Timestamp\": \"2013-12-01T06:00:00Z\", \"Value\": \"339.0\" }, { \"StreamId\": \"TPPrototype.uflsample.value_timeString\", \"DataType\": \"String\", \"Timestamp\": \"2013-12-01T07:00:00Z\", \"Value\": \"344.0\" }, { \"StreamId\": \"TPPrototype.uflsample.value_timeString\", \"DataType\": \"String\", \"Timestamp\": \"2013-12-01T17:00:00Z\", \"Value\": \"341.0\" } ], \"TPPrototype.uflsample.pressure_time\": [ { \"StreamId\": \"TPPrototype.uflsample.pressure_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T06:00:00Z\", \"Value\": 339.0 }, { \"StreamId\": \"TPPrototype.uflsample.pressure_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T07:00:00Z\", \"Value\": 344.0 }, { \"StreamId\": \"TPPrototype.uflsample.pressure_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T17:00:00Z\", \"Value\": 341.0 } ] } } The following JSONPath configuration reads all the TPPrototype.uflsample.value_time values from the JSON above: { \"Id\": \"Value\", \"DataType\": \"Double\", \"FieldDefinition\": \"$[\u0027StreamData\u0027].[\u0027TPPrototype.uflsample.value_time\u0027][*].Value\" }, { \"Id\": \"Time\", \"DataType\": \"DateTime\", \"FieldDefinition\": \"$[\u0027StreamData\u0027].[\u0027TPPrototype.uflsample.value_time\u0027][*].Timestamp\", \"IsIndex\": true } The following example reads specific value from complex nested JSON: { \"success\": true, \"error\": null, \"result\": { \"type\": \"runtime_history\", \"chart\": { \"chart\": { \"type\": \"column\" }, \"title\": { \"text\": \"\" }, \"subtitle\": { \"text\": \"Daily History\" }, \"colors\": [ \"#fee292\", \"#fdc152\", \"#f69638\", \"#f17130\", \"#9f2d26\", \"#8acadc\", \"#184c8e\" ], \"series\": [ { \"name\": \"Stage 3 Aux Heat\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"heat\", \"state\": \"heat_aux_stage3\" }, { \"name\": \"Stage 2 Aux Heat\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"heat\", \"state\": \"heat_aux_stage2\" }, { \"name\": \"Aux Heat\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"heat\", \"state\": \"heat_aux\" }, { \"name\": \"Stage 2 Heat\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"heat\", \"state\": \"heat_stage2\" }, { \"name\": \"Heat\", \"data\": [ 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.3, 0.2, 0.0 ], \"stack\": \"heat\", \"state\": \"heat\" }, { \"name\": \"Stage 2 Cool\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"cool\", \"state\": \"cool_stage2\" }, { \"name\": \"Cool\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"cool\", \"state\": \"cool\" } ], \"xAxis\": { \"categories\": [ \"Friday\", \"Saturday\", \"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\" ], \"labels\": { \"rotation\": -45 } }, \"yAxis\": { \"allowDecimals\": false, \"min\": 0, \"max\": 24, \"tickInternval\": 4, \"title\": { \"text\": \"Runtime (Hours)\" } }, \"legend\": { \"layout\": \"vertical\", \"align\": \"center\", \"floating\": false, \"shadow\": false, \"itemStyle\": { \"fontSize\": \"1em\" } }, \"tooltip\": { \"shared\": true, \"borderColor\": \"#000000\" }, \"credits\": { \"enabled\": false }, \"plotOptions\": { \"column\": { \"stacking\": \"normal\" }, \"series\": { \"shadow\": false } } }, \"table\": { \"headings\": [ \"Fri\", \"Sat\", \"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\" ], \"series\": [ { \"name\": \"Aux Heat\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"heat\", \"state\": \"heat_aux\" }, { \"name\": \"Outdoor High Temp.\", \"data\": [ 72.0, 64.0, 73.0, 72.0, 67.0, 73.0, 77.0, 62.0, 51.0 ], \"stack\": null, \"state\": \"outdoor_high_temperature\" }, { \"name\": \"Outdoor Low Temp.\", \"data\": [ 55.0, 60.0, 62.0, 61.0, 51.0, 43.0, 46.0, 44.0, 35.0 ], \"stack\": null, \"state\": \"outdoor_low_temperature\" }, { \"name\": \"Avg Indoor Temp.\", \"data\": [ 76.0, 77.0, 78.0, 78.0, 77.0, 73.0, 74.0, 75.0, 72.0 ], \"stack\": null, \"state\": \"average_indoor_temperature\" }, { \"name\": \"Avg Indoor Humidity\", \"data\": [ 66.0, 68.0, 70.0, 70.0, 69.0, 67.0, 67.0, 66.0, 61.0 ], \"stack\": null, \"state\": \"average_indoor_humidity\" }, { \"name\": \"Fan Only Runtime\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": null, \"state\": \"fan_only\" }, { \"name\": \"Vent\", \"data\": [], \"stack\": null, \"state\": \"vent\" } ] }, \"show_monthly_runtime_history\": true } } The following JSONPath configuration reads Sunday Average Indoor Temperature. The timestamp comes from Adapter local time. { \"Id\": \"Temperature\", \"DataType\": \"Double\", \"FieldDefinition\": \"$.result.table.series[3].data[2]\" }, { \"Id\": \"Timestamp\", \"DataType\": \"DateTime\", \"Format\": \"Adapter\", \"IsIndex\": true } Error handling If you encounter text parser related errors that is errors for the ValueField or IndexField , check the StreamId associated with the error message. Possible errors include the following: The JSONPath expression of ValueField or IndexField is pointing to a non-existing value The JSONPath expression of ValueField or IndexField is missing a value altogether DataType does not match the value"
                                                                                                       },
    "content/main/shared-content/configuration/text-parser/text-parser.html":  {
                                                                                   "href":  "content/main/shared-content/configuration/text-parser/text-parser.html",
                                                                                   "title":  "Text parser",
                                                                                   "keywords":  "Text parser The adapter you are using includes the text parser component which ensures consistent parsing of text from different files. For more information on which file types are supported for your adapter, see the topics in this chapter. Designed to be a document parser, the text parser parses a semantically complete document in its entirety. The text parser produces OMF compatible output, which in turn is compatible with the OCS backing SDS (Sequential Data Store) that stores data in streams consisting of multiple values and indexes. Data types supported by the text parser The following data types are supported by the text parser: DateTime DateTimeOffset TimeSpan sbyte byte short ushort int uint long ulong float double decimal bool char string Note: Not all data types supported by the text parser are also supported by OMF. Special characters support As part of the default StreamId logic, the text parser replaces special characters as follows: Special character Replacement character * empty string \u0027 empty string ` empty string \" empty string ? empty string ; - \\| - \\ - { ( } ) [ ( ] ) Culture support Some numeric values and datetimes support cultures when they are being parsed. The default culture is en-US (US English) (InvariantCulture). OSIsoft recommends that you leave the adapter at the default unless you expect culturally variant input. Note: Installed cultures vary by machine with both Linux and Windows. If the specified culture is not installed, the text parser fails to parse input that requires that culture. Time zone support A time zone or offset specified by a time is always used to convert to UTC time. Time zones are only used if there is no offset or time zone specifier in a text date and time string. For time zones that support time changes between daylight and standard times, a text file may temporarily contain invalid or ambiguous datetimes during the time change, which are possible only for a two-hour period each year. When these time changes occur, the text parser logs them, but the datetime is parsed and passed to the callback. Ambiguous times are reported as standard times, which is the Microsoft recommendation. Date and time processing The text parser can use time zones, cultures, and custom formats to read dates and times from ingress data. You can specify date and time formats when you configure data selection. Set the date and time using the IndexFormat property. If you leave the IndexFormat property unset, the data selection configuration defaults to the ISO 8601 date format. If you are using a culture other than default en-US , use the name of day or month specific to the culture. For example, use \"Juni\" instead of \"June\" for the de-DE culture. The following date and time syntaxes have been tested and are supported. \"MM/dd/yyyy \"MM dd yyyy H:mm:ss zzz\" \"06/15/2018 \"06 15 2018 15:15:30 -05:00\" \"MM/dd/yyyy \"MM dd yyyy H:mm:ss.fff zzz\" \"06/15/2018 \"06 15 2018 15:15:30.123 -05:00\" \"dd/MM/yyyy \"dd MM yyyy H:mm:ss.fff K\" \"15/06/2018 \"15 06 2018 15:15:30.123 Z\" \"MMMM/dd/yyyy \"MMMM dd yyyy H:mm:ss.fff K\" \"June/15/2018 \"June 15 2018 15:15:30.123 Z\" (InvariantCulture/English) (InvariantCulture English) \"MMMM/dd/yyyy \"MMMM dd yyyy H:mm:ss.fff K\" \"Juni/15/2018 \"Juni 15 2018 15:15:30.123 Z\" (German) \"MMM/dd/yyyy \"MMM dd yyyy H:mm:ss.fff K\" \"Jun/15/2018 \"Jun 15 2018 15:15:30.123 Z\" \"MMM-dd-yyyy H:mm:ss.fff K\" \"Jun-15-2018 15:15:30.123 Z\" \"MMM-dd-yyyy H:mm:ss.fff K\" \"Jun-15-2018 15:15:30.123 Z\" \"MMM-dd-yyyy H:mm:ss.fff K\" \"Jun-15-2018 15:15:30.123 Z\" \"yyyy-MM-dd H:mm:ss.fff K\" \"2018-06-15 15:15:30.123 Z\" \"yyyy-M-d H:mm:ss.fff K\" \"2018-6-5 15:15:30.123 Z\" \"yyyy-M-d H:mm:ss.fff zzz\" \"2018-6-5 15:15:30.123 +05:00\" \"ddd dd MMM yyyy h:mm tt zzz\" \"Sun 15 Jun 2008 8:30 AM -06:00\" \"dddd dd MMM yyyy h:mm tt zzz\" \"Sunday 15 Jun 2008 8:30 AM -06:00\" \"dddd dd MMM yyyy h:mm tt zzz\" \"Sunday 15 Jun 2008 8:30 AM -06:00\" \"dddd dd MMMM yyyy h:mm tt zzz\" \"Sunday 15 June 2008 8:30 AM -06:00\" Adapter date and time processing uses Microsoft datetime parsing . For more documentation on standard datetime formats, which fit most use cases, see Standard date and time format strings . For documentation on custom datetime formation, see Custom date and time format strings ."
                                                                               },
    "content/main/shared-content/configuration/text-parser/xpath-and-csv-syntax-for-value-retrieval.html":  {
                                                                                                                "href":  "content/main/shared-content/configuration/text-parser/xpath-and-csv-syntax-for-value-retrieval.html",
                                                                                                                "title":  "XPath and CSV syntax for value retrieval",
                                                                                                                "keywords":  "XPath and CSV syntax for value retrieval For information on which semantics are used for retrieving values from XML and CSV files, see the following documentation: XML - XML Path Language (XPath) CSV - Column Index (1 based) or Header value (if header defined) The following syntaxes are used to extract values from XML or CSV documents. XML - Simple XPath example \u003cvalues\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:46.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:46.0928791Z\u003c time\u003e \u003cvalue\u003e1.234567890\u003c/value\u003e \u003cvalue\u003e1.234567890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:47.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:47.0928791Z\u003c time\u003e \u003cvalue\u003e12.34567890\u003c/value\u003e \u003cvalue\u003e12.34567890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:48.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:48.0928791Z\u003c time\u003e \u003cvalue\u003e123.4567890\u003c/value\u003e \u003cvalue\u003e123.4567890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:49.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:49.0928791Z\u003c time\u003e \u003cvalue\u003e1234.567890\u003c/value\u003e \u003cvalue\u003e1234.567890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:50.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:50.0928791Z\u003c time\u003e \u003cvalue\u003e12345.67890\u003c/value\u003e \u003cvalue\u003e12345.67890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:51.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:51.0928791Z\u003c time\u003e \u003cvalue\u003e123456.7890\u003c/value\u003e \u003cvalue\u003e123456.7890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:52.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:52.0928791Z\u003c time\u003e \u003cvalue\u003e12345678.90\u003c/value\u003e \u003cvalue\u003e12345678.90\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:53.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:53.0928791Z\u003c time\u003e \u003cvalue\u003e123456789.0\u003c/value\u003e \u003cvalue\u003e123456789.0\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003c/values\u003e \u003c values\u003e The following XPath configuration reads a series of values: { \"Id\": \"DoubleValue\", \"FieldDefinition\": \"./values/value/value\", \". values value value\", \"DataType\": \"Double\" }, { \"Id\": \"Timestamp\", \"FieldDefinition\": \"./values/value/time\", \". values value time\", \"DataType\": \"DateTime\", \"IsIndex\": true } CSV - Simple CSV column index example 2020-08-10T12:10:46.0928791Z,1.234567890 2020-08-10T12:10:47.0928791Z,12.34567890 2020-08-10T12:10:48.0928791Z,123.4567890 2020-08-10T12:10:49.0928791Z,1234.567890 2020-08-10T12:10:50.0928791Z,12345.67890 2020-08-10T12:10:51.0928791Z,123456.7890 2020-08-10T12:10:52.0928791Z,12345678.90 2020-08-10T12:10:53.0928791Z,123456789.0 The following CSV column index configuration requires the text parser be configured with HasHeader=false . The column indexes are 1 based and configured as strings. { \"Id\": \"DoubleValue\", \"FieldDefinition\": \"2\", \"DataType\": \"Double\" }, { \"Id\": \"Timestamp\", \"FieldDefinition\": \"1\", \"DataType\": \"DateTime\", \"IsIndex\": true } CSV - Simple CSV column header example Date,Value 2020-08-10T12:10:46.0928791Z,1.234567890 2020-08-10T12:10:47.0928791Z,12.34567890 2020-08-10T12:10:48.0928791Z,123.4567890 2020-08-10T12:10:49.0928791Z,1234.567890 2020-08-10T12:10:50.0928791Z,12345.67890 2020-08-10T12:10:51.0928791Z,123456.7890 2020-08-10T12:10:52.0928791Z,12345678.90 2020-08-10T12:10:53.0928791Z,123456789.0 The following CSV column header configuration requires the text parser be configured with HasHeader=true . { \"Id\": \"DoubleValue\", \"FieldDefinition\": \"Value\", \"DataType\": \"Double\" }, { \"Id\": \"Timestamp\", \"FieldDefinition\": \"Date\", \"DataType\": \"DateTime\", \"IsIndex\": true }"
                                                                                                            },
    "content/main/shared-content/diagnostics/diagnostics.html":  {
                                                                     "href":  "content/main/shared-content/diagnostics/diagnostics.html",
                                                                     "title":  "Diagnostics",
                                                                     "keywords":  "Diagnostics The adapter and its components produce various kinds of diagnostics data that is sent to all health endpoints. The System_Diagnostics.json file contains a flag that determines whether diagnostics are enabled. You can change this at runtime through REST calls or the EdgeCmd utility. Diagnostics data are collected by default. To egress diagnostics related data, you have to configure an adapter health endpoint first. See Health endpoint configuration . Available diagnostics data Every minute, dynamic data is sent to configured health endpoints. The following diagnostics data are available: System Stream count IO rate Error rate"
                                                                 },
    "content/main/shared-content/diagnostics/egress.html":  {
                                                                "href":  "content/main/shared-content/diagnostics/egress.html",
                                                                "title":  "Egress",
                                                                "keywords":  "Egress The Egress component of the adapter produces the following diagnostics stream: IO rate The Diagnostics.Egress.IORate dynamic type includes the following values, which are logged in a stream with the Id {machineName}.{serviceName}.OmfEgress.{EndpointId}.IORate . IORate includes only sequential data successfully sent to an egress endpoint. Property Type Description timestamp string Timestamp of event IORate double One-minute rolling average of data rate (streams/second) (streams second)"
                                                            },
    "content/main/shared-content/diagnostics/error-rate.html":  {
                                                                    "href":  "content/main/shared-content/diagnostics/error-rate.html",
                                                                    "title":  "Error rate",
                                                                    "keywords":  "Error rate The Diagnostics.Adapter.ErrorRate dynamic type includes the following values, which are logged in a stream with the Id {componentid}.ErrorRate . Property Type Description timestamp string Timestamp of event ErrorRate double One-minute rolling average of error rate (streams/second) (streams second)"
                                                                },
    "content/main/shared-content/diagnostics/io-rate.html":  {
                                                                 "href":  "content/main/shared-content/diagnostics/io-rate.html",
                                                                 "title":  "IO rate",
                                                                 "keywords":  "IO rate The Diagnostics.Adapter.IORate dynamic type includes the following values, which are logged in a stream with the Id {componentid}.IORate . IORate includes only sequential data collected from a data source. Property Type Description timestamp string Timestamp of event IORate double One-minute rolling average of data rate (streams/second) (streams second)"
                                                             },
    "content/main/shared-content/diagnostics/stream-count.html":  {
                                                                      "href":  "content/main/shared-content/diagnostics/stream-count.html",
                                                                      "title":  "Stream count",
                                                                      "keywords":  "Stream count The Diagnostics.StreamCountEvent dynamic type includes the following values, which are logged in a stream with the Id {componentid}.StreamCount . The StreamCount and TypeCount include only types and streams created for sequential data received from a data source. Property Type Description timestamp string Timestamp of event StreamCount int Number of streams created by the adapter instance TypeCount int Number of types created by the adapter instance"
                                                                  },
    "content/main/shared-content/diagnostics/system.html":  {
                                                                "href":  "content/main/shared-content/diagnostics/system.html",
                                                                "title":  "System",
                                                                "keywords":  "System The Diagnostics.System dynamic type includes the following values which are logged in a stream with the Id System.Diagnostics . This diagnostic stream contains system level information related to the host platform that the adapter is running on. Property Type Description timestamp string Timestamp of event ProcessIdentifier int Process Id of the host process StartTime string Time at which the host process started WorkingSet long Amount of physical memory in bytes, allocated for the host process TotalProcessorTime double Total processor time for the host process expressed in seconds TotalUserProcessorTime double User processor time for the host process expressed in seconds TotalPrivilegedProcessorTime double Privileged processor time for the host process expressed in seconds ThreadCount int Number of threads in the host process HandleCount int Number of handles opened by the host process ManagedMemorySize double Number of bytes currently thought to be allocated in managed memory Unit of Measure = megabytes PrivateMemorySize double Amount of paged memory in bytes allocated for the host process Unit of Measure = megabytes PeakPagedMemorySize double Maximum amount of memory in the virtual memory paging file in bytes used by the host process. Unit of Measure = megabytes StorageTotalSize double Total size of the storage medium in use by the system Unit of Measure = megabytes StorageFreeSpace double Free space available Unit of Measure = megabytes Each adapter component produces its own diagnostics streams."
                                                            },
    "content/main/shared-content/health/device-status.html":  {
                                                                  "href":  "content/main/shared-content/health/device-status.html",
                                                                  "title":  "Device status",
                                                                  "keywords":  "Device status The device status indicates the health of this component and if it is currently communicating properly with the data source. This time-series data is stored within a PI point or OCS stream, depending on the endpoint type. During healthy steady-state operation, a value of Good is expected. Property Type Description Time string Timestamp of the event DeviceStatus string The value of the DeviceStatus The possible statuses are: Status Meaning Good The component is connected to the data source and it is collecting data. ConnectedNoData The component is connected to the data source but it is not receiving data from it. Starting The component is currently in the process of starting up and is not yet connected to the data source. DeviceInError The component encountered an error either while connecting to the data source or attempting to collect data. Shutdown The component is either in the process of shutting down or has finished. Removed The adapter component has been removed and will no longer collect data. NotConfigured The adapter component has been created but is not yet configured."
                                                              },
    "content/main/shared-content/health/health.html":  {
                                                           "href":  "content/main/shared-content/health/health.html",
                                                           "title":  "Health",
                                                           "keywords":  "Health PI Adapters produce various kinds of health data that can be egressed to different health endpoints. To egress health related data, you have to configure an adapter health endpoint first. See Health endpoint configuration . Available health data Dynamic data is sent every minute to configured health endpoints. The following health data is available: Device status Next Health Message Expected"
                                                       },
    "content/main/shared-content/health/health-and-diagnostics.html":  {
                                                                           "href":  "content/main/shared-content/health/health-and-diagnostics.html",
                                                                           "title":  "Health and Diagnostics",
                                                                           "keywords":  "Health and Diagnostics PI Adapters produce various types of health data. You can use health data to ensure that your adapters are running properly and that data flows to the configured OMF endpoints. For more information on available adapter health data, see health . PI Adapters also produce diagnostic data. You can use diagnostic data to find more information about a particular adapter instance. Diagnostic data lives alongside the health data and you can egress it using a health endpoint and setting EnableDiagnostics to true . You can configure EnableDiagnostics in the system\u0027s General configuration . For more information on available adapter diagnostics data, see diagnostics . In OSIsoft Cloud Services (OCS), both health and diagnostics data are created as assets. The data are available in the Asset Explorer and you can use them in the OCS Trend feature. For more information, see the OCS documentation Assets . Health endpoint differences Two OMF endpoints are currently supported for adapter health data: PI Web API OSIsoft Cloud Services (OCS) There are a few differences in how these two systems treat the associated health and diagnostics data. PI Web API parses the information and sends it to configured PI servers for the OMF endpoint. The static data is used to create an AF structure on a PI AF server. The dynamic health data is time-series data that is stored in PI points on a PI Data Archive. You can see it in the AF structure as PI point data reference attributes. OCS does not currently provide a way to store the static metadata. For OCS-based adapter health endpoints, only the dynamic data is stored. Each value is its own stream with the timestamp property as the single index. AF structure With a health endpoint configured to a PI server, you can use PI System Explorer to view the health and diagnostics of an adapter. The element hierarchy is shown in the following image. The Elements root contains a link to an Adapters node. This is the root node for all adapter instances. Below Adapters , you will find one or more adapter nodes. Each node\u0027s title is defined by the node\u0027s corresponding computer name and service name in this format: {ComputerName}.{ServiceName} . For example, in the following image, MachineName is the computer name and OpcUa is the service name. To see the health and diagnostics values, click on an adapter node and select Attributes ."
                                                                       },
    "content/main/shared-content/health/next-health-message-expected.html":  {
                                                                                 "href":  "content/main/shared-content/health/next-health-message-expected.html",
                                                                                 "title":  "Next health message expected",
                                                                                 "keywords":  "Next health message expected This property is similar to a heartbeat. A new value for NextHealthMessageExpected is sent by an individual adapter data component on a periodic basis while it is functioning properly. This value is a timestamp that indicates when the next value should be received. When monitoring, if the next value is not received by the indicated time, this likely means that there is an issue. It could be an issue with the adapter, adapter component, network connection between the health endpoint and the adapter, and so on. Property Type Description Time string Timestamp of the event NextHealthMessageExpected string Timestamp when next value is expected"
                                                                             },
    "content/main/shared-content/installation/installation.html":  {
                                                                       "href":  "content/main/shared-content/installation/installation.html",
                                                                       "title":  "Installation",
                                                                       "keywords":  "Installation Adapters are installed on a local machine using an install kit downloaded from the OSIsoft Customer Portal. For instructions on downloading and installing adapters, see Install the adapter . Alternatively, you can build custom installers or containers for Linux. For more information, see the Docker instructions in the documentation of the respective adapter."
                                                                   },
    "content/main/shared-content/installation/install-the-adapter.html":  {
                                                                              "href":  "content/main/shared-content/installation/install-the-adapter.html",
                                                                              "title":  "Install the adapter",
                                                                              "keywords":  "Install the adapter You can install adapters on either a Windows or Linux operating system. Before installing the adapter, see the respective system requirements to ensure your machine is properly configured to provide optimum adapter operation. Windows Complete the following steps to install a PI adapter on a Windows computer: Download PI-Adapter-for-StructuredDataFiles_1.0.0.138-x64_.msi from the OSIsoft Customer portal . Note: Customer login credentials are required to access the portal. Run PI-Adapter-for-StructuredDataFiles_1.0.0.138-x64_.msi file. Follow the setup wizard. You can change the installation folder or port number during setup. The default port number is 5590 . Optional: To verify the installation, run the following curl command with the port number that you specified during installation: curl http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration If you receive an error, wait a few seconds and try the script again. If the installation was successful, a JSON copy of the default system configuration is returned. Linux Complete the following steps to install an adapter on a Linux computer: Download the appropriate Linux distribution file ( PI-Adapter-for-StructuredDataFiles_1.0.0.138- platform _.deb ) from the OSIsoft Customer portal . Note: Customer login credentials are required to access the portal. Open a terminal. Run the sudo apt update command to update available packages information. Run the sudo apt install command against the Linux distribution file ( PI-Adapter-for-StructuredDataFiles_1.0.0.138- platform _.deb ) selected in step 1 of this procedure. For example: sudo apt install ./PI-Adapter-for-StructuredDataFiles_1.0.0.138-x64_.deb . PI-Adapter-for-StructuredDataFiles_1.0.0.138-x64_.deb Optional: To verify the installation, run the following curl command with the port number that you specified during installation: curl http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration If you receive an error, wait a few seconds and run the command again. If the installation was successful, a JSON copy of the default system configuration is returned."
                                                                          },
    "content/main/shared-content/installation/install-using-docker.html":  {
                                                                               "href":  "content/main/shared-content/installation/install-using-docker.html",
                                                                               "title":  "Installation using Docker",
                                                                               "keywords":  "Installation using Docker Docker is a set of tools that you can use on Linux to manage application deployments. This topic provides examples of how to create a Docker container with the adapter. Note: The use of Docker is only recommended if your environment requires it. Only users proficient with Docker should use it to install the adapter. Docker is not required to use the adapter. Create a startup script To create a startup script for the adapter, follow the instructions below. Use a text editor to create a script similar to one of the following examples: Note: The script varies slightly by processor. \u003c!-- PRERELEASE REMINDER: Update {adapter} and {version} placeholders. Example: bacnet, 1.1.0.192 --\u003e ARM32 #!/bin/sh #! bin sh if [ -z $portnum ] ; then exec /PI-Adapter-for-StructuredDataFiles_1.0.0.138-arm_/OSIsoft.Data.System.Host  PI-Adapter-for-StructuredDataFiles_1.0.0.138-arm_ OSIsoft.Data.System.Host else exec /PI-Adapter-for-StructuredDataFiles_1.0.0.138-arm_/OSIsoft.Data.System.Host  PI-Adapter-for-StructuredDataFiles_1.0.0.138-arm_ OSIsoft.Data.System.Host --port:$portnum fi ARM64 #!/bin/sh #! bin sh if [ -z $portnum ] ; then exec /PI-Adapter-for-StructuredDataFiles_1.0.0.138-arm64_/OSIsoft.Data.System.Host  PI-Adapter-for-StructuredDataFiles_1.0.0.138-arm64_ OSIsoft.Data.System.Host else exec /PI-Adapter-for-StructuredDataFiles_1.0.0.138-arm64_/OSIsoft.Data.System.Host  PI-Adapter-for-StructuredDataFiles_1.0.0.138-arm64_ OSIsoft.Data.System.Host --port:$portnum fi AMD64 #!/bin/sh #! bin sh if [ -z $portnum ] ; then exec /PI-Adapter-for-StructuredDataFiles_1.0.0.138-x64_/OSIsoft.Data.System.Host  PI-Adapter-for-StructuredDataFiles_1.0.0.138-x64_ OSIsoft.Data.System.Host else exec /PI-Adapter-for-StructuredDataFiles_1.0.0.138-x64_/OSIsoft.Data.System.Host  PI-Adapter-for-StructuredDataFiles_1.0.0.138-x64_ OSIsoft.Data.System.Host --port:$portnum fi Name the script sdfdockerstart.sh and save it to the directory where you plan to create the container. Create a Docker container To create a Docker container that runs the adapter, follow the instructions below. Create the following Dockerfile in the directory where you want to create and run the container. Note: Dockerfile is the required name of the file. Use the variation according to your operating system: ARM32 FROM ubuntu:20.04 WORKDIR /   RUN apt-get update \u0026\u0026 DEBIAN_FRONTEND=noninteractive apt-get install -y ca-certificates libicu66 libssl1.1 curl COPY sdfdockerstart.sh /   RUN chmod +x /sdfdockerstart.sh  sdfdockerstart.sh ADD ./PI-Adapter-for-StructuredDataFiles_1.0.0.138-arm_.tar.gz . PI-Adapter-for-StructuredDataFiles_1.0.0.138-arm_.tar.gz . ENTRYPOINT [\"/sdfdockerstart.sh\"] [\" sdfdockerstart.sh\"] ARM64 FROM ubuntu:20.04 WORKDIR /   RUN apt-get update \u0026\u0026 DEBIAN_FRONTEND=noninteractive apt-get install -y ca-certificates libicu66 libssl1.1 curl COPY sdfdockerstart.sh /   RUN chmod +x /sdfdockerstart.sh  sdfdockerstart.sh ADD ./PI-Adapter-for-StructuredDataFiles_1.0.0.138-arm64_.tar.gz . PI-Adapter-for-StructuredDataFiles_1.0.0.138-arm64_.tar.gz . ENTRYPOINT [\"/sdfdockerstart.sh\"] [\" sdfdockerstart.sh\"] AMD64 (x64) FROM ubuntu:20.04 WORKDIR /   RUN apt-get update \u0026\u0026 DEBIAN_FRONTEND=noninteractive apt-get install -y ca-certificates libicu66 libssl1.1 curl COPY sdfdockerstart.sh /   RUN chmod +x /sdfdockerstart.sh  sdfdockerstart.sh ADD ./PI-Adapter-for-StructuredDataFiles_1.0.0.138-x64_.tar.gz . PI-Adapter-for-StructuredDataFiles_1.0.0.138-x64_.tar.gz . ENTRYPOINT [\"/sdfdockerstart.sh\"] [\" sdfdockerstart.sh\"] Copy the PI-Adapter-for-StructuredDataFiles_1.0.0.138- platform _.tar.gz file to the same directory as the Dockerfile . Copy the sdfdockerstart.sh script to the same directory as the Dockerfile . Run the following command line in the same directory ( sudo may be necessary): docker build -t sdfadapter . Docker container startup The following procedures contain instructions on how to run the adapter inside a Docker container with different options enabled. Run the Docker container with REST access enabled To run the adapter inside a Docker container with access to its REST API from the local host, complete the following steps: Use the docker container image sdfadapter created previously. Type the following in the command line ( sudo may be necessary): docker run -d --network host sdfadapter Port 5590 is accessible from the host and you can make REST calls to the adapter from applications on the local host computer. In this example, all data stored by the adapter is stored in the container itself. When you delete the container, the stored data is also deleted. Run the Docker container with persistent storage If you have a file share directory /sdf/InputDirectory  sdf InputDirectory and you want to move the files to /sdf/OutputDirectory  sdf OutputDirectory after processing, for the Docker container to access these directories and the storage on the host machine, complete the following steps to run the container: Use the docker container image sdfadapter created previously. Enter the following command line (you may need to use the sudo command): docker run -d --network host -v /sdf:/usr/share/OSIsoft/  sdf: usr share OSIsoft  sdfadapter Update the InputDirectory and OutputDirectory of your data source configuration to following settings: { \"InputDirectory\": \"/usr/share/OSIsoft/InputDirectory\", \" usr share OSIsoft InputDirectory\", \"OutputDirectory\": \"/usr/share/OSIsoft/OutputDirectory\" \" usr share OSIsoft OutputDirectory\" } Note: /sdf  sdf is replaced by /usr/share/OSIsoft  usr share OSIsoft , the target directory inside the container. The default port 5590 is accessible from the host and you can make REST calls to the adapter from applications on the local host computer. The data is written to a host directory on the local machine /sdf  sdf rather than the container. Change port number To use a different port other than 5590 , you can specify a portnum variable on the docker run command line. For example, to start the adapter using port 6000 instead of 5590 , use the following command: docker run -d -e portnum=6000 --network host sdfadapter This command accesses the REST API with port 6000 instead of port 5590 . The following curl command returns the configuration for the container. curl http://localhost:6000/api/v1/configuration http:  localhost:6000 api v1 configuration Remove REST access If you remove the --network host option from the docker run command, REST access is not possible from outside the container. This may be of value where you want to host an application in the same container as the adapter but do not want to have external REST access enabled."
                                                                           },
    "content/main/shared-content/installation/system-requirements.html":  {
                                                                              "href":  "content/main/shared-content/installation/system-requirements.html",
                                                                              "title":  "System requirements",
                                                                              "keywords":  "System requirements PI Adapter for Structured Data Files is supported on a variety of platforms and processors. Install kits are available for the following platforms: Operating System Platform Installation Kit Processor(s) Windows 10 Enterprise Windows 10 IoT Enterprise x64 PI-Adapter-for-StructuredDataFiles_1.0.0.138-x64_.msi Intel/AMD Intel AMD 64-bit processors Debian 9, 10 Ubuntu 18.04, 20.04 x64 PI-Adapter-for-StructuredDataFiles_1.0.0.138-x64_.deb Intel/AMD Intel AMD 64-bit processors Debian 9, 10 Ubuntu 20.04 ARM32 PI-Adapter-for-StructuredDataFiles_1.0.0.138-arm_.deb ARM 32-bit processors Debian 10 Ubuntu 20.04 ARM64 PI-Adapter-for-StructuredDataFiles_1.0.0.138-arm64_.deb ARM 64-bit processors Alternatively, you can use tar.gz files with binaries to build your own custom installers or containers for Linux. For more information on installing the adapter with Docker containers, see Installation using Docker . Note: Any file exceeding 100MB requires more system memory. PI Web API compatibility This version of PI Adapter for Structured Data Files is compatible with PI Web API 2021 and later."
                                                                          },
    "content/main/shared-content/installation/uninstall-the-adapter.html":  {
                                                                                "href":  "content/main/shared-content/installation/uninstall-the-adapter.html",
                                                                                "title":  "Uninstall the adapter",
                                                                                "keywords":  "Uninstall the adapter Complete the procedure corresponding to your specific operating system to uninstall the adapter: Windows To delete the PI adapter program files from a Windows device, use the Windows Control Panel uninstall application process. Note: The configuration, data, and log files are not deleted by the uninstall process. Optional: To delete data, configuration, and log files, delete the directory: %ProgramData%\\OSIsoft\\Adapters\\StructuredDataFiles This deletes all data processed by the adapter, in addition to the configuration and log files. Linux To delete PI Adapter software from a Linux device, open a terminal window and run the following command: sudo apt remove pi.adapter.StructuredDataFiles Optional: To delete data, configuration, and log files, run the following command: sudo rm -r /usr/share/OSIsoft/Adapters/StructuredDataFiles  usr share OSIsoft Adapters StructuredDataFiles This deletes all data processed by the adapter, in addition to the configuration and log files."
                                                                            },
    "content/main/shared-content/installation/upgrade-the-adapter.html":  {
                                                                              "href":  "content/main/shared-content/installation/upgrade-the-adapter.html",
                                                                              "title":  "Upgrade the adapter",
                                                                              "keywords":  "Upgrade the adapter When a new version of the adapter is released, you can upgrade to the latest version by running the new installation package. Windows upgrade Complete the following steps to upgrade a PI adapter on a Windows computer: Download PI-Adapter-for-StructuredDataFiles_1.0.0.138-x64_.msi from the OSIsoft Customer Portal . Note: Customer login credentials are required to access the portal. Run PI-Adapter-for-StructuredDataFiles_1.0.0.138-x64_.msi . Complete the setup wizard. Optional: To verify the upgrade, run the following curl command with the port number that you specified when completing the wizard: curl -X GET \"http://localhost:5590/api/v1/Diagnostics/ProductInformation\" \"http:  localhost:5590 api v1 Diagnostics ProductInformation\" Upon successful upgrade, the JSON response lists the updated application version: { \"Application Version\": \"1.1.0.128\", //    upgraded version \".Net Core Version\": \".NET Core 3.1.15\", \"Operating System\": \"Microsoft Windows 10.0.19041\" } Linux upgrade Complete the following steps to upgrade a PI adapter on a Linux computer: Download the appropriate Linux distribution file from the OSIsoft Customer Portal . Note: Customer login credentials are required to access the portal. Open a terminal session. Move the Linux distribution file to the target host and run the sudo apt upgrade command. Platform Command Linux x64 sudo apt upgrade ./PI-Adapter-for-StructuredDataFiles_1.0.0.138-x64_.deb . PI-Adapter-for-StructuredDataFiles_1.0.0.138-x64_.deb Linux ARM32 Debian sudo apt upgrade ./PI-Adapter-for-StructuredDataFiles_1.0.0.138-arm_.deb . PI-Adapter-for-StructuredDataFiles_1.0.0.138-arm_.deb Linux ARM64 Debian sudo apt upgrade ./PI-Adapter-for-StructuredDataFiles_1.0.0.138-arm64_.deb . PI-Adapter-for-StructuredDataFiles_1.0.0.138-arm64_.deb Optional: To verify the upgrade, run the following curl command with the port number that you specified: curl -X GET \"http://localhost:5590/api/v1/Diagnostics/ProductInformation\" \"http:  localhost:5590 api v1 Diagnostics ProductInformation\" Upon successful upgrade, the JSON response lists the updated application version: { \"Application Version\": \"1.1.0.128\", //    upgraded version \".Net Core Version\": \".NET Core 3.1.15\", \"Operating System\": \"Microsoft Windows 10.0.19041\" }"
                                                                          },
    "content/main/shared-content/introduction/intro-to-pi-adapters.html":  {
                                                                               "href":  "content/main/shared-content/introduction/intro-to-pi-adapters.html",
                                                                               "title":  "PI Adapter for Structured Data Files 1.0.0.138",
                                                                               "keywords":  "PI Adapter for Structured Data Files 1.0.0.138 PI Adapter for Structured Data Files is a data collection technology that collects time-series operations data from a data source over the protocol and then sends it to a supported storage location in the Open Message Format (OMF). \u003c!-- Add content about the protocol here --\u003e PI Adapter for Structured Data Files data flow The following diagram depicts the collection and processing of data for an operational PI Adapter for Structured Data Files, collecting and processing data. Refer to the list below the diagram for more information on each callout depicted. \u003c!-- Mark Bishop 3/3/22: 3 3 22: The SVG file referenced below can be opened and edited using https://app.diagrams.net/ https:  app.diagrams.net  --\u003e The user installs and configures PI Adapter for Structured Data Files on a host system. You can configure the adapter using either a REST interface or EdgeCmd, a command line utility specifically designed for interfacing with edge systems. The adapter collects data from assets over the protocol, a process known as data ingress . The adapter converts ingress data to the Open Message Format (OMF), a format that supported storage locations understand. The adapter sends OMF data to a supported storage location in a process known as data egress . Supported egress endpoints include: PI Server OSIsoft Cloud Services"
                                                                           },
    "content/main/shared-content/metadata/metadata.html":  {
                                                               "href":  "content/main/shared-content/metadata/metadata.html",
                                                               "title":  "Metadata",
                                                               "keywords":  "Metadata If the metadataLevel is set to Low , Medium , or High in the General configuration , adapter streams created by the ingress components include the following metadata: Datasource: {ComponentId} AdapterType: {ComponentType} ComponentId corresponds to the adapter components\u0027 data source configured in the Components configuration . ComponentType corresponds to the adapter type. Metadata for health and diagnostics streams If you configure a health endpoint and enable metadata, they are included in the health streams ( Device status and Next health message expected ) together with ComponentId and ComponentType . If you enable diagnostics in General configuration , metadata are included in the diagnostics streams ( Stream count , IO rate , Error rate ) together with ComponentId and ComponentType . The adapter may also send its own stream metadata not including health and diagnostics streams. For more information about what custom metadata is included in each stream, see the user guide for your adapter. Note: Metadata is only sent for streams created by the ingress components. Currently, the only endpoint that persists sent metadata is OCS (OSIsoft Cloud Services)."
                                                           },
    "content/main/shared-content/technical-support-and-feedback.html":  {
                                                                            "href":  "content/main/shared-content/technical-support-and-feedback.html",
                                                                            "title":  "Technical support and feedback",
                                                                            "keywords":  "Technical support and feedback OSIsoft provides several ways to report issues and provide feedback on PI Adapters. Technical support For technical assistance with PI Adapters, contact OSIsoft Technical Support through the OSIsoft Customer Portal . We can help you identify the problem, provide workarounds and address any concerns you may have. Remote access to your facilities may be necessary during the session. Note: You must have an account set up in the OSIsoft Customer Portal before you can open a case. If you do not have a portal account, see How to Get a Login to OSIsoft Customer Portal . Alternatively, call OSIsoft Technical Support at +1 510-297-5828. When you contact OSIsoft Technical Support, be prepared to provide this information: Product name, version, and build numbers Details about your computer platform (CPU type, operating system, and version number) Time that the difficulty started Log files at that time Details of any environment changes prior to the start of the issue Summary of the issue, including any relevant log files during the time the issue occurred \u003c!--To view a brief primer on PI Adapters, see the [PI Adapters playbook](https://customers.osisoft.com/s/knowledgearticle?knowledgeArticleUrl=Playbook-PI-adapters) playbook](https:  customers.osisoft.com s knowledgearticle?knowledgeArticleUrl=Playbook-PI-adapters) in the OSIsoft Customer Portal.--\u003e Product feedback To submit product feedback for PI Adapters, visit the PI Adapters feedback page . The product team at OSIsoft regularly monitors the page. Documentation feedback To submit documentation feedback for PI Adapters, send an email to documentation@aveva.com . Be sure to include the following information with your feedback: Product name and version Documentation topic URL Details of the suggestion or error The technical documentation team will review and address your feedback in future documentation updates."
                                                                        },
    "content/main/shared-content/troubleshooting/troubleshooting.html":  {
                                                                             "href":  "content/main/shared-content/troubleshooting/troubleshooting.html",
                                                                             "title":  "Troubleshooting",
                                                                             "keywords":  "Troubleshooting PI adapters provide features for troubleshooting issues related to connectivity, data flow, and configuration. Resources include adapter logs and the Wireshark troubleshooting tool . If you are still unable to resolve issues or need additional guidance, contact OSIsoft Technical Support through the OSIsoft Customer Portal . Note: Make sure to also check the troubleshooting information specific to your adapter in this user guide. Logs Messages from the System and OmfEgress logs provide information on the status of the adapter. For example, they show if a connection from the adapter to an egress endpoint exists. Perform the following steps to view the System and OmfEgress logs: Navigate to the logs directory: Windows: %ProgramData%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Logs Linux: /usr/share/OSIsoft/Adapters/\u003cAdapterName\u003e/Logs  usr share OSIsoft Adapters \u003cAdapterName\u003e Logs . Example: A successful connection to a PI Web API egress endpoint displays the following message in the OmfEgress log: 2020-11-02 11:08:51.870 -06:00 [Information] Data will be sent to the following OMF endpoint: Id: \u003comfegress id\u003e Endpoint: \u003cpi web api URL\u003e (note: the pi web api default port is 443) ValidateEndpointCertificate: \u003ctrue or false\u003e Optional: Change the log level of the adapter to receive more information and context. For more information, see Logging configuration . ASP .NET Core platform log The ASP .NET Core platform log provides information from the Kestrel web server that hosts the application. The log could contain information that the adapter is overloaded with incoming data. Perform the following steps to spread the load among multiple adapters: Decrease the scan frequency. Lower the amount of data selection items. Wireshark Wireshark is a protocol-specific troubleshooting tool that supports all current adapter protocols. Perform the following steps if you want to use Wireshark to capture traffic from the data source to the adapter or from the adapter to the OMF destination. Download Wireshark . Familiarize yourself with the tool and read the Wireshark user guide . Health and diagnostics egress to PI Web API The adapter sends health and diagnostics data to PI Web API; in some cases, conflicts may occur that are due to changes or perceived changes in PI Web API. For example, a 409 - Conflict error message displays if you upgrade your adapter version and the PI points do not match in the upgraded version. However, data is continued to be sent as long as containers are created, so buffering only starts if no containers are created. To resolve the conflict, perform the following steps: Stop the adapter. Delete the Health folder inside of the Buffers folder. Stop PI Web API. Delete the relevant adapter created AF structure. Delete the associated health and diagnostics PI points on any or all PI Data Archives created by PI Web API. Start PI Web API. Start the adapter. Adapter connection to egress endpoint Certain egress health information in both PI Web API and OCS show if an adapter connection to an egress endpoint exists. To verify an active connection, perform one of the following procedures: PI Web API connection Perform the following steps to determine if a connection to the PI Web API endpoint exists: Open PI Web API. Select the OmfEgress component of your adapter, for example GVAdapterUbuntu.\u003cAdapterName\u003e.OmfEgress . Make sure that the following PI points have been created for your egress endpoint: DeviceStatus NextHealthMessageExpected IORate OCS connection Perform the following steps to determine if a connection to the OCS endpoint exists: Open OCS. Click Sequential Data Store \u003e Streams . Makes sure that the following streams have been created for your egress endpoint: DeviceStatus NextHealthMessageExpected IORate TCP connection Perform the following steps to see all established TCP sessions in Linux: Open a terminal. Type the following command: ss -o state established -t -p Press Enter."
                                                                         },
    "content/overview/principles-of-operation.html":  {
                                                          "href":  "content/overview/principles-of-operation.html",
                                                          "title":  "Principles of operation",
                                                          "keywords":  "Principles of operation The adapter\u0027s operations focus on data collection and stream creation. Adapter configuration For the adapter to start data collection, you need to configure the adapter by defining the following: Data source : Provide an input directory that the adapter collects data from. An output directory must also be provided, which is where the adapter moves processed files. Data selection : Select items for which the adapter should gather data from the files. For more information, see PI Adapter for Structured Data Files data source configuration and PI Adapter for Structured Data Files data selection configuration . Data collection When the adapter starts, it begins scanning for files in the input directory that match the configured file name filter. These files are processed in the order of their creation time starting with the oldest. After a file has been processed, it will be moved to the output directory. If the input directory is empty when scanned by the adapter, the adapter will wait five seconds before rescanning. Notes: You must assign the following permissions to the adapter service account to successfully process structured data files: Directory Read Write Delete input directory ??? ??? ??? output directory ??? ??? ??? discovery directory ??? ??? ??? To use discovery, you must also assign the read, write, and delete permissions to the adapter service account on the discovery directory. If the adapter is not able to open a file from the input directory or move a file to the output directory after processing it, new files will not be processed until the problem is resolved. Do not open a file once it has been placed in the input directory as this could prevent the adapter from opening or moving it. Supported file types The adapter supports files that are in CSV, JSON, or XML format. The raw data files can be uncompressed or compressed as a zip, gzip, tar, or tar.gzip files. The files can have UTF-8, ASCII, or Unicode encoding. Note: Compression is only supported at the individual file level. The adapter does not support compressed archives that contain multiple files. Supported data types The following table lists value data types that the adapter supports for data collection and types of streams that will be created. Value data type Stream data type Boolean Boolean Int16 Int16 UInt16 UInt16 Int32 Int32 UInt32 UInt32 Int64 Int64 UInt64 UInt64 Float32 Float32 Float64 Float64 String String Date-Time DateTime Stream creation The adapter creates a stream with two properties for a selected item. The properties are described in the following table: Property name Data type Description Timestamp DateTime Timestamp of the given item update. Value Specified on the type of incoming value Value of the item update. Note: If streams are deleted from an endpoint while the adapter is running, the ingress component must be restarted to recreate the streams. See Start and stop ingress component . Certain metadata are sent with each stream created. The following metadata are common for every adapter type: ComponentId : Specifies the data source, for example, StructuredDataFiles1 ComponentType : Specifies the type of adapter, for example, StructuredDataFiles Metadata specific to PI Adapter for Structured Data Files: InputDirectory : Location of the source files to process. The metadata level is set in General configuration . For PI Adapter for Structured Data Files, the following metadata is sent for the individual level: None : No metadata Low : AdapterType ( ComponentType ) and DataSource ( ComponentId ) Medium : AdapterType ( ComponentType ) and DataSource ( ComponentId ) High : AdapterType ( ComponentType ), DataSource ( ComponentId ), InputDirectory Each stream created for a given item has a unique identifier (stream ID). If you specify a custom stream ID for the item in data selection configuration, the adapter uses that stream ID to create the stream. Otherwise, the adapter constructs the stream ID with the following format constructed from the data source\u0027s FriendlyName and the item\u0027s ValueField : \u003cAdapter Component ID\u003e.\u003cFriendlyName\u003e.\u003cValueField\u003e Note: The naming convention is affected by StreamIdPrefix and DefaultStreamIdPattern settings in data source configuration. For more information, see PI Adapter for Structured Data Files data source configuration ."
                                                      },
    "content/release-notes/release-notes.html":  {
                                                     "href":  "content/release-notes/release-notes.html",
                                                     "title":  "Release notes",
                                                     "keywords":  "Release notes PI Adapter for Structured Data Files 1.0.0.138 Adapter Framework 1.4 Overview This represents the initial release for PI Adapter for Structured Data Files. This product collects time series data from source files in a local or remote directory to Open Message Format (OMF) endpoints in OSIsoft Cloud Services or PI Servers. PI Adapter for Structured Data Files can also collect health and diagnostics information. It supports buffering, static and event data collection, and various Windows and Linux-based operating systems as well as containerization. Note: FTP servers are not supported for this version. For more information, see the PI Adapter for Structured Data Files overview . Known issues On Linux installs, the adapter currently cannot use file shares as an input directory. Workaround : Use an external process or script to copy or move the files locally on the machine where the adapter is running. Setup System requirements Refer to System requirements . Installation and upgrade Refer to Install the adapter . Uninstallation Refer to Uninstall the adapter . Security information and guidance OSIsoft\u0027s commitment Because the PI System often serves as a barrier protecting control system networks and mission-critical infrastructure assets, OSIsoft is committed to (1) delivering a high-quality product and (2) communicating clearly what security issues have been addressed. This release of PI Adapter for Structured Data Files is the highest quality and most secure version of the PI Adapter for Structured Data Files released to date. OSIsoft\u0027s commitment to improving the PI System is ongoing, and each future version should raise the quality and security bar even further. Vulnerability communication The practice of publicly disclosing internally discovered vulnerabilities is consistent with the Common Industrial Control System Vulnerability Disclosure Framework developed by the Industrial Control Systems Joint Working Group (ICSJWG) . Despite the increased risk posed by greater transparency, OSIsoft is sharing this information to help you make an informed decision about when to upgrade to ensure your PI System has the best available protection. For more information, refer to OSIsoft\u0027s Ethical Disclosure Policy (https://www.osisoft.com/ethical-disclosure-policy) (https:  www.osisoft.com ethical-disclosure-policy) . To report a security vulnerability, refer to OSIsoft\u0027s Report a Security Vulnerability (https://www.osisoft.com/report-a-security-vulnerability) (https:  www.osisoft.com report-a-security-vulnerability) . Vulnerability scoring OSIsoft has selected the Common Vulnerability Scoring System (CVSS) to quantify the severity of security vulnerabilities for disclosure. To calculate the CVSS scores, OSIsoft uses the National Vulnerability Database (NVD) calculator maintained by the National Institute of Standards and Technology (NIST). OSIsoft uses Critical, High, Medium and Low categories to aggregate the CVSS Base scores. This removes some of the opinion related errors of CVSS scoring. As noted in the CVSS specification, Base score range from 0 for the lowest severity to 10 for the highest severity. Overview of new vulnerabilities found or fixed This section is intended to provide relevant security-related information to guide your installation or upgrade decision. OSIsoft is proactively disclosing aggregate information about the number and severity of PI Adapter for OPC UA security vulnerabilities that are fixed in this release. The Adapter\u0027s utilization of zlib through .NET 6 does not expose CVE-2018-25032 or CVE-2022-37434. Documentation overview EdgeCmd utility: Provides an overview on how to configure and administer PI adapters on Linux and Windows using command line arguments. Technical support and resources Refer to Technical support and feedback ."
                                                 },
    "content/troubleshooting/troubleshoot-pi-adapter-for-structured-data-files.html":  {
                                                                                           "href":  "content/troubleshooting/troubleshoot-pi-adapter-for-structured-data-files.html",
                                                                                           "title":  "Troubleshoot PI Adapter for Structured Data Files",
                                                                                           "keywords":  "Troubleshoot PI Adapter for Structured Data Files If the adapter is not working as expected, you can troubleshoot by viewing message logs and verifying adapter configuration and connectivity. If you are unable to resolve issues with the adapter or need additional guidance, contact OSIsoft Technical Support through the OSIsoft Customer Portal . Check configurations Incorrect configurations can interrupt data flow and cause errors in values and ranges. Perform the following steps confirm correct configuration for your adapter. Navigate to data source configuration and validate each parameter value. Navigate to data selection configuration and validate each parameter value. Navigate to egress endpoints configuration and verify each configured endpoint\u0027s Endpoint property and credentials are correct. For a PI server endpoint, verify UserName and Password . For an OCS endpoint, verify ClientId and ClientSecret . Check connectivity Perform the following steps to verify active connections to the data source and egress endpoints. Based on your egress endpoints, verify that data values are updating. For PI Server, send a request to the PI Web API to verify that PI point values are updating. Use Postman or a Web browser to send the request. For more information, see PI Web API Reference . Alternatively, use any PI Client software to read point values from the PI Data Archive directly. For OCS, view the OCS portal to verify that data streams are updating. For more information, see Getting started with trend data . Alternatively, you can use Postman to send an API request to verify data streams. For more information, see API calls for reading data . If configured, use a health endpoint to determine the status of the adapter. For more information, see Health and diagnostics . Check logs Perform the following steps to view the adapter and endpoint logs to isolate issues for resolution. Navigate to the logs directory: Windows: %ProgramData%\\OSIsoft\\Adapters\\StructuredDataFiles\\Logs Linux: /usr/share/OSIsoft/Adapters/StructuredDataFiles/Logs  usr share OSIsoft Adapters StructuredDataFiles Logs Optional: Change the log level of the adapter to receive more information and context. For more information, see Logging configuration ."
                                                                                       },
    "README.html":  {
                        "href":  "README.html",
                        "title":  "PI-Adapter-for-Structured-Data-Files-Docs",
                        "keywords":  "PI-Adapter-for-Structured-Data-Files-Docs PI Adapter for Structured Data Files is a data-collection component that transfers time-series data from source devices to Open Message Format (OMF) endpoints in OSIsoft Cloud Services or PI Servers. This repository contains the documentation for PI Adapter for Structured Data Files. Subtree This documentation repository consumes the PI-Adapter repository as a subtree. This repository contains a documentation framework for adapters. This subtree should be updated periodically. To update the subtree, enter the following command: git subtree pull --prefix content/main content main https://github.com/osisoft/PI-Adapter https:  github.com osisoft PI-Adapter main --squash License ?? 2020 - 2023 OSIsoft, LLC. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at: http://www.apache.org/licenses/LICENSE-2.0 http:  www.apache.org licenses LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
                    }
}
