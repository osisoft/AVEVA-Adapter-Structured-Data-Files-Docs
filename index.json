{
    "content/configuration/configuration.html":  {
                                                     "href":  "content/configuration/configuration.html",
                                                     "title":  "Configuration",
                                                     "keywords":  "Configuration Before you can begin using the adapter, you must configure the data source and data selection using the adapter\u0027s REST API. \u003c!---I would really like to see the configurations listed in order here, so that I have a road map of what I need to do. You could then indicate which ones are optional and get rid of the some are required and others are optional sentences.---\u003e The examples in the configuration topics use cURL, a commonly available tool on both Windows and Linux. You can configure the adapter with any programming language or tool that supports making REST calls or with the EdgeCmd utility. For more information, see the EdgeCmd utility documentation . To validate successful configurations, you can perform data retrieval ( GET commands) with a browser, if available, on your device. For more information on PI Adapter configuration tools, see Configuration tools . Quick Start This Quick Start guides you through setup of each configuration file available for the PI Adapter for Structured Data Files. As you complete each step, perform each Required configuration to establish a data flow from a data source to one or more endpoints. Some configurations are optional. Important: If you want to complete the optional configurations in each step below, complete those tasks before the required tasks. Configure system components. Configuration Description Required Optional system components Defines each component instance on the system host. ??? Configure egress endpoints. Configuration Description Required Optional network proxy If there is a proxy between the adapter and your egress endpoints, define it using this configuration. ??? egress endpoints Defines the final locations that the adapter sends OMF data. ??? Optional: Configure health endpoints. Configuration Description Required Optional general Defines whether diagnostic information is included in health data. Requires a health endpoint. ??? health endpoint Defines endpoint where PI adapters produce and store health data. ??? Optional: Complete buffering and logging configurations. Configuration Description Required Optional buffering Defines whether data buffering is enabled, the volume of data buffered, and the location of buffered files. ??? logging Defines custom logging options. ??? Complete data source configurations. The adapter starts collecting data immediately after you complete the data source configuration. Important: Complete all other adapter configuration steps before data source configuration. This practice ensures a complete history of data collection. Configuration Description Required Optional data filter A reusable file that defines what data within the data source files are received. Can be used in conjunction with data selection configurations. ??? data selection Defines what data within the data source files are received. ??? data source Defines the source that the adapter receives data files from. ???"
                                                 },
    "content/configuration/configuration-examples.html":  {
                                                              "href":  "content/configuration/configuration-examples.html",
                                                              "title":  "PI Adapter for Structured Data Files configuration examples",
                                                              "keywords":  "PI Adapter for Structured Data Files configuration examples The following JSON samples provide examples for all configurations available for PI Adapter for Structured Data Files. System components configuration with two Structured Data Files adapter instances [ { \"ComponentId\": \"StructuredDataFiles1\", \"ComponentType\": \"StructuredDataFiles\" }, { \"ComponentId\": \"StructuredDataFiles2\", \"ComponentType\": \"StructuredDataFiles\" }, { \"ComponentId\": \"OmfEgress\", \"ComponentType\": \"OmfEgress\" } ] Adapter configuration The following example is a complete configuration for the PI Adapter for Structured Data Files. { \"StructuredDataFiles1\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"DataSource\": { \"FriendlyName\": \"Weather\", \"InputDirectory\": \"C:\\\\InputDirectory\", \"FileNameFilter\": \"*.csv\", \"OutputDirectory\": \"C:\\\\OutputDirectory\", \"HasHeader\": true, \"Culture\": \"fr-FR\", \"TimeZone\": \"Europe/Paris\", \"Europe Paris\", \"Compression\": \"Zip\", \"FieldSeparator\": \"|\" }, \"DataSelection\": [ { \"Selected\": true, \"Name\": \"Name\", \"StreamId\": \"StreamId\", \"ValueField\": \"FanSpeed\", \"TimeField\": \"FileCreationTime\", \"TimeFormat\": \"mm/dd/yy\", \"mm dd yy\", \"DataType\": \"Int32\" } ] }, \"System\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"HealthEndpoints\": [], \"Diagnostics\": { \"enableDiagnostics\": true }, \"Components\": [ { \"componentId\": \"Egress\", \"componentType\": \"OmfEgress\" }, { \"componentId\": \"StructuredDataFiles1\", \"componentType\": \"StructuredDataFiles\" } ], \"Buffering\": { \"BufferLocation\": \"C:/ProgramData/OSIsoft/Adapters/StructuredDataFiles/Buffers\", \"C: ProgramData OSIsoft Adapters StructuredDataFiles Buffers\", \"MaxBufferSizeMB\": -1, \"EnablePersistentBuffering\": true }, \"General\": { \"enableDiagnostics\": true, \"metadataLevel\": \"Medium\" } }, \"OmfEgress\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"DataEndpoints\": [ { \"id\": \"WebAPI EndPoint\", \"endpoint\": \"https://PIWEBAPIServer/piwebapi/omf\", \"https:  PIWEBAPIServer piwebapi omf\", \"userName\": \"USERNAME\", \"password\": \"PASSWORD\" }, { \"id\": \"OCS Endpoint\", \"endpoint\": \"https://OCSEndpoint/omf\", \"https:  OCSEndpoint omf\", \"clientId\": \"CLIENTID\", \"clientSecret\": \"CLIENTSECRET\" }, { \"Id\": \"EDS\", \"Endpoint\": \"http://localhost:/api/v1/tenants/default/namespaces/default/omf\", \"http:  localhost: api v1 tenants default namespaces default omf\", \"UserName\": \"eds\", \"Password\": \"eds\" } ] }, \"DataFilters\": [ { \"id\": \"DuplicateData\", \"absoluteDeadband\": 0, \"percentChange\": null, \"expirationPeriod\": \"1:00:00\" } ] } Data source configuration The following are representations of data source configurations for the Structured Data Files adapter. Data source configuration with required parameters { \"InputDirectory\": \"C:\\\\InputDirectory\", \"OutputDirectory\": \"C:\\\\OutputDirectory\" } Data source configuration with optional parameters { \"FriendlyName\": \"Weather\", \"InputDirectory\": \"C:\\\\InputDirectory\", \"FileNameFilter\": \"*.csv\", \"OutputDirectory\": \"C:\\\\OutputDirectory\", \"HasHeader\": true, \"Culture\": \"fr-FR\", \"TimeZone\": \"Europe/Paris\", \"Europe Paris\", \"Compression\": \"Zip\", \"FieldSeparator\": \"|\" } Data selection configuration The following are representations of data selection configurations for the Structured Data Files adapter. Data selection configuration with custom TimeFormat [ { \"Selected\": true, \"Name\": \"Name\", \"StreamId\": \"StreamId\", \"ValueField\": \"FanSpeed\", \"TimeField\": \"FileCreationTime\", \"TimeFormat\": \"mm/dd/yy\", \"mm dd yy\", \"DataType\": \"Int32\" } ] Data selection configuration with unselected item [ { \"Selected\": false, \"Name\": \"Name\", \"StreamId\": \"StreamId\", \"ValueField\": \"Pressure\", \"TimeField\": \"PressureTimeStamp\", \"DataType\": \"Int16\" } ] Data selection configuration with XPath ValueField [ { \"Selected\": true, \"Name\": \"Name\", \"StreamId\": \"StreamId\", \"ValueField\": \"/bookstore/book/title\", \" bookstore book title\", \"TimeField\": \"bookStoreTimeStamp\", \"DataType\": \"String\" } ]"
                                                          },
    "content/configuration/pi-adapter-for-structured-data-files-data-selection-configuration.html":  {
                                                                                                         "href":  "content/configuration/pi-adapter-for-structured-data-files-data-selection-configuration.html",
                                                                                                         "title":  "PI Adapter for Structured Data Files data selection configuration",
                                                                                                         "keywords":  "PI Adapter for Structured Data Files data selection configuration In addition to the data source configuration, you need to provide a data selection configuration to specify the data you want the adapter to collect from the data source. Note: This document uses cURL commands to demonstrate data selection configuration, but other options are available. For more information, see Configuration tools . Configure Structured Data Files data selection Complete the following steps to configure Structured Data Files data selection. Use the PUT method in conjunction with the api/v1/configuration/\u003cComponentId\u003e/DataSelection api v1 configuration \u003cComponentId\u003e DataSelection REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for data selection into the file. For sample JSON, see Structured Data Files data selection examples . Update the example JSON parameters for your environment. For a table of all available parameters, see Structured Data Files data selection parameters . Save the file as ConfigureDataSelection.json . Open a command line session and change the working directory to the location of ConfigureDataSelection.json . Enter the following cURL command (which uses the PUT method) to initialize the data selection configuration. curl -d \"@ConfigureDataSelection.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/StructuredDataFiles1/DataSelection\" \"http:  localhost:5590 api v1 configuration StructuredDataFiles1 DataSelection\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. If you use a component ID other than StructuredDataFiles1 , update the endpoint with the component ID. For a list of other REST operations you can perform, like updating or deleting a data selection configuration, see REST URLs . Structured Data Files data selection schema The full schema definition for the Structured Data Files data selection configuration is in the StructuredDataFiles_DataSelection_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\StructuredDataFiles\\Schemas Linux: /opt/OSIsoft/Adapters/StructuredDataFiles/Schemas  opt OSIsoft Adapters StructuredDataFiles Schemas Structured Data Files data selection parameters Parameter Required Type Description Name Optional string The optional friendly name of the data item collected from the data source. If not configured, the default value is null and results in the StreamId value being used also as a Name . DataFilterId Optional string The identifier of a data filter defined in the Data filters configuration . By default, no filter is applied. Selected Optional boolean If true , data for this item is collected and sent to one or more configured OMF endpoint. Allowed value: true or false Default value: true StreamId Optional string The custom identifier used to create the streams. If not specified, the adapter generates a default value based on the DefaultStreamIdPattern in the PI Adapter for Structured Data Files data source configuration . A properly configured custom stream ID follows these rules: Is not case-sensitive. Can contain spaces. Cannot start with two underscores __ . Can contain a maximum of 100 characters. Cannot use the following characters: /   : ? # [ ] @ ! $ \u0026 \u0027 ( ) \\ * + , ; = % \u003c \u003e ` ValueField Required string Name of the value field. JSONPath, XPath, and CSV are supported 1 . Example: \"FanSpeed\". Allowed Values: Any name to represent the value. TimeField Required string Name of the time field. JSONPath, XPath, and CSV are supported 1 . If no timestamp is provided in the file, the following reserved values can be used to specify the timestamp: - FileCreationTime - UTC time when the file was created. - FileModifiedTime - UTC time when the file was modified. - AdapterGeneratedTime - UTC time the file is processed by the adapter. Example: \"FanSpeedTimeStamp\" DataType Required string Data type of the values specified in the ValueField parameter. Example: \"Int32\" Allowed values: SByte, Byte, Int16, UInt16, Int32, UInt32, Int64, UInt64, Single, Double, Decimal, Boolean, DateTime, String TimeFormat Optional string Time format of the timestamp specified in the TimeField parameter. Default value: null. Example: \"mm/dd/yy\". \"mm dd yy\". 1 Note : For full examples of how to enter JSONPath, XPath, or CSV syntax, see the following topics: JSONPath syntax for value retrieval XPath and CSV syntax for value retrieval Structured Data Files data selection examples The following are examples of valid Structured Data Files data selection configurations: Structured Data Files data selection configuration example with custom TimeFormat [ { \"Selected\": true, \"Name\": \"Name\", \"StreamId\": \"StreamId\", \"ValueField\": \"FanSpeed\", \"TimeField\": \"FileCreationTime\", \"TimeFormat\": \"mm/dd/yy\", \"mm dd yy\", \"DataType\": \"Int32\" } ] Structured Data Files data selection configuration example with unselected item [ { \"Selected\": false, \"Name\": \"Name\", \"StreamId\": \"StreamId\", \"ValueField\": \"Pressure\", \"TimeField\": \"PressureTimeStamp\", \"DataType\": \"Int16\" } ] Structured Data Files data selection configuration example using XPath in the ValueField [ { \"Selected\": true, \"Name\": \"Name\", \"StreamId\": \"StreamId\", \"ValueField\": \"/bookstore/book/title\", \" bookstore book title\", \"TimeField\": \"bookStoreTimeStamp\", \"DataType\": \"String\" } ] REST URLs Relative URL HTTP verb Action api/v1/configuration/\u003cComponentId\u003e/DataSelection api v1 configuration \u003cComponentId\u003e DataSelection GET Retrieves the Structured Data Files data selection configuration api/v1/configuration/\u003cComponentId\u003e/DataSelection api v1 configuration \u003cComponentId\u003e DataSelection PUT Configures or updates the Structured Data Files data selection configuration api/v1/configuration/\u003cComponentId\u003e/DataSelection api v1 configuration \u003cComponentId\u003e DataSelection DELETE Deletes the Structured Data Files data selection configuration api/v1/configuration/\u003cComponentId\u003e/DataSelection api v1 configuration \u003cComponentId\u003e DataSelection PATCH Allows partial updating of configured data selection items. Note: The request must be an array containing one or more data selection items. Each data selection item in the array must include its StreamId . api/v1/configuration/\u003cComponentId\u003e/DataSelection/\u003cStreamId\u003e api v1 configuration \u003cComponentId\u003e DataSelection \u003cStreamId\u003e PUT Updates or creates a new data selection with the specified StreamId . api/v1/configuration/\u003cComponentId\u003e/DataSelection/\u003cStreamId\u003e api v1 configuration \u003cComponentId\u003e DataSelection \u003cStreamId\u003e DELETE Deletes a specific data selection item of the Structured Data Files data selection configuration. Note: Replace \u003cComponentId\u003e with the ID of your Structured Data Files component, for example StructuredDataFiles1 ."
                                                                                                     },
    "content/configuration/pi-adapter-for-structured-data-files-data-source-configuration.html":  {
                                                                                                      "href":  "content/configuration/pi-adapter-for-structured-data-files-data-source-configuration.html",
                                                                                                      "title":  "PI Adapter for Structured Data Files data source configuration",
                                                                                                      "keywords":  "PI Adapter for Structured Data Files data source configuration To use the adapter, you must configure it to receive data from a data source. Note: This document uses cURL commands to demonstrate data selection configuration, but other options are available. For more information, see Configuration tools . Configure Structured Data Files data source Complete the following steps to configure a Structured Data Files data source. Use the PUT method in conjunction with the api/v1/configuration/\u003cComponentId\u003e/DataSource api v1 configuration \u003cComponentId\u003e DataSource REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for a structured data files data source into the file. For sample JSON, see Structured Data Files data source examples . Update the example JSON parameters for your environment. For a table of all available parameters, see Structured Data Files data source parameters . Save the file as ConfigureDataSource.json . Open a command line session and change the working directory to the location of ConfigureDataSelection.json . Enter the following cURL command (which uses the PUT method) to initialize the data source configuration. curl -d \"@ConfigureDataSource.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/StructuredDataFiles1/DataSource\" \"http:  localhost:5590 api v1 configuration StructuredDataFiles1 DataSource\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. If you use a component ID other than StructuredDataFiles1 , update the endpoint with the component ID. For a list of other REST operations you can perform, like updating or deleting a data source configuration, see REST URLs . Postrequisite: Configure data selection. For more information, see PI Adapter for Structured Data Files data selection configuration . Structured Data Files data source schema The full schema definition for the Structured Data Files data source configuration is in the StructuredDataFiles_DataSource_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\StructuredDataFiles\\Schemas Linux: /opt/OSIsoft/Adapters/StructuredDataFiles/Schemas  opt OSIsoft Adapters StructuredDataFiles Schemas Structured Data Files data source parameters The following parameters are available for configuring a Structured Data Files data source: Parameter Required Type Description FriendlyName Optional string The label to use for the data source. InputDirectory Required string Location of the source files to process. FTP servers are not supported. Example: C:\\\\InputDirectory OutputDirectory Required string Location for the files to be moved to after being processed. FTP servers are not supported. Example: C:\\\\OutputDirectory FileNameFilter Optional string Pattern used to match files in the InputDirectory for processing. If no filter is specified, the adapter will attempt to process all files in the InputDirectory . Use * as the wildcard character. Example: *.csv HasHeader Optional bool Indicates if a header line is present in the file. Only applies to CSV files. Default value: false Culture Optional string Locale setting for the input files. Example: en-US Default value: local culture TimeZone Optional string Time zone of timestamps in the input files. If specified, the value must be a valid entry from the IANA time zone database. Example: America/Los_Angeles America Los_Angeles Default value: Etc/UTC Etc UTC Format Optional string Input file format. Allowed values: Csv , Json , Xml Default value: Csv Compression Optional string Input file compression format. Allowed value: None , Zip , Tar , GZip , TarGZip Default value: None Encoding Optional string Character encoding used in the input files. Allowed value: UTF8 , ASCII , Unicode Default value: UTF8 FieldSeparator Optional string Character used to delineate fields in the input files. Only applies to CSV files. Default value: , LineSeparator Optional string Character(s) used to separate lines in the input files. Only applies to CSV files. Default value: \\n StreamIdPrefix Optional string The stream ID prefix applied to all data items collected from the data source. Default value: {ComponentId} DefaultStreamIdPattern Optional string Specifies the default stream ID pattern to use. Possible parameters: {FriendlyName} , {ValueField} Default pattern: {FriendlyName}.{ValueField} Structured Data Files data source examples The following are examples of valid Structured Data Files data source configurations: Note: When copy and pasting the examples below, validate the InputDirectory and OutputDirectory path for the data source host operating system: Windows or Linux. Structured Data Files data source configuration #1 { \"InputDirectory\": \"C:\\\\InputDirectory\", \"OutputDirectory\": \"C:\\\\OutputDirectory\" } Structured Data Files data source configuration #2 { \"FriendlyName\": \"Weather\", \"InputDirectory\": \"C:\\\\InputDirectory\", \"FileNameFilter\": \"*.csv\", \"OutputDirectory\": \"C:\\\\OutputDirectory\", \"HasHeader\": true, \"Culture\": \"fr-FR\", \"TimeZone\": \"Europe/Paris\", \"Europe Paris\", \"Compression\": \"Zip\", \"FieldSeparator\": \"|\" } REST URLs Relative URL HTTP verb Action api/v1/configuration/\u003cComponentId\u003e/DataSource api v1 configuration \u003cComponentId\u003e DataSource GET Retrieves the Structured Data Files data source configuration api/v1/configuration/\u003cComponentId\u003e/DataSource api v1 configuration \u003cComponentId\u003e DataSource POST Creates the Structured Data Files data source configuration api/v1/configuration/\u003cComponentId\u003e/DataSource api v1 configuration \u003cComponentId\u003e DataSource PUT Configures or updates the Structured Data Files data source configuration api/v1/configuration/\u003cComponentId\u003e/DataSource api v1 configuration \u003cComponentId\u003e DataSource DELETE Deletes the Structured Data Files data source configuration Note: Replace \u003cComponentId\u003e with the ID of your Structured Data Files component, for example StructuredDataFiles1 ."
                                                                                                  },
    "content/health-and-diagnostics/health/adapter-health-for-structured-data-files.html":  {
                                                                                                "href":  "content/health-and-diagnostics/health/adapter-health-for-structured-data-files.html",
                                                                                                "title":  "Adapter health",
                                                                                                "keywords":  "Adapter health PI Adapters produce different kinds of health data that can be egressed to different health endpoints. Available health data Dynamic data is sent every minute to configured health endpoints. The following health data are available: Device status Next Health Message Expected AF structure With a health endpoint configured to a PI server, you can use PI System Explorer to view the health of a given adapter. The element hierarchy is shown in the following image."
                                                                                            },
    "content/health-and-diagnostics/health/device-status-for-structured-data-files.html":  {
                                                                                               "href":  "content/health-and-diagnostics/health/device-status-for-structured-data-files.html",
                                                                                               "title":  "Device status",
                                                                                               "keywords":  "Device status The device status indicates the health of this component and if it is currently communicating properly with the data source. This time-series data is stored within a PI point or OCS stream, depending on the endpoint type. During healthy steady-state operation, a value of Good is expected. Property Type description Time string Timestamp of the event DeviceStatus string The value of the DeviceStatus The possible statuses: Status Meaning Good The component is connected to the data source and is successfully processing files. ConnectedNoData The component is connected to the data source, but there was an error parsing a data file or a file contained no data for selected items. AttemptingFailover The component is attempting to fail over. Starting The component is currently in the process of starting up and is not yet connected to the data source. DeviceInError The component encountered an error while connecting to the data source. Shutdown The component is in the process of shutting down or has finished. Removed The component has been removed and will no longer collect data. NotConfigured The component has been created but is not yet configured."
                                                                                           },
    "content/health-and-diagnostics/health/health-and-diagnostics-for-structured-data-files.html":  {
                                                                                                        "href":  "content/health-and-diagnostics/health/health-and-diagnostics-for-structured-data-files.html",
                                                                                                        "title":  "Health and diagnostics",
                                                                                                        "keywords":  "Health and diagnostics PI Adapters produce various types of health data. You can use health data to ensure that your adapters are running properly and that data flows to the configured OMF endpoints. For more information, see Adapter health . PI Adapters also produce diagnostic data. You can use diagnostic data to find more information about a particular adapter instance. Diagnostic data lives alongside the health data and you can egress it using a health endpoint and setting EnableDiagnostics to true . You can configure EnableDiagnostics in the system\u0027s General configuration . For more information on available diagnostics, see Adapter diagnostics . Health endpoint differences Two OMF endpoints are currently supported for adapter health data: PI Web API OSIsoft Cloud Services There are a few differences in how these two systems treat the associated health and diagnostics data. PI Web API parses the information and sends it to configured PI servers for the OMF endpoint. The static data is used to create a hierarchy on a PI AF server similar to the following example: The dynamic health data is time-series data that is stored in PI points on a PI Data Archive. You can see it in the AF hierarchy as PI point data reference attributes. OSIsoft Cloud Services does not currently provide a way to store the static metadata. For OCS-based adapter health endpoints, only the dynamic data is stored. Each value is its own stream with the timestamp property as the single index."
                                                                                                    },
    "content/health-and-diagnostics/health/next-health-message-expected.html":  {
                                                                                    "href":  "content/health-and-diagnostics/health/next-health-message-expected.html",
                                                                                    "title":  "Next health message expected",
                                                                                    "keywords":  "Next health message expected This property is similar to a heartbeat. A new value for NextHealthMessageExpected is sent by an individual adapter data component on a periodic basis while it is functioning properly. This value is a timestamp that indicates when the next value should be received. When monitoring, if the next value is not received by the indicated time, this likely means that there is an issue. It could be an issue with the adapter, adapter component, network connection between the health endpoint and the adapter, and so on. Property Type Description Time string Timestamp of the event NextHealthMessageExpected string Timestamp when next value is expected"
                                                                                },
    "content/index.html":  {
                               "href":  "content/index.html",
                               "title":  "PI Adapter for Structured Data Files overview",
                               "keywords":  "PI Adapter for Structured Data Files overview PI Adapter for Structured Data Files is a data-collection component that transfers time-series data from source files in a local or remote directory to OMF endpoints in OSIsoft Cloud Services, Edge Data Store, or PI Servers. \u003c!---The conceptual information is very light. What type of files? Where do they come from? What sorts of scenarios would this be used in? I wouldn\u0027t expect to see installation and configuration information in the main overview page. It seems too detailed. I realize this is what is done on the other apater documents, but I would question it there, too.---\u003e Adapter installation You can install the adapter with a download kit that you can obtain from the OSIsoft Customer Portal. You can install the adapter on devices running either Windows or Linux operating systems. Adapter configuration Using REST API, you can configure all functions of the adapter. The configurations are stored in JSON files. For data ingress, you must define an adapter component in the system components configuration for each data source to which the adapter will connect. You configure each adapter component with the connection information for the data source and the data to collect. For data egress, you must specify destinations for the data, including security for the outgoing connection. Additional configurations are available to egress health and diagnostics data, add buffering configuration to protect against data loss, and record logging information for troubleshooting purposes. Once you have configured the adapter and it is sending data, you can use administration functions to manage the adapter or individual ingress components of the adapter. Health and diagnostics functions monitor the status of connected devices, adapter system functions, the number of active data streams, the rate of data ingress, the rate of errors, and the rate of data egress. EdgeCmd utility OSIsoft also provides the EdgeCmd utility, a proprietary command line tool to configure and administer an adapter on both Linux and Windows operating systems. EdgeCmd utility is installed separately from the adapter."
                           },
    "content/installation/install-pi-adapter-for-structured-data-files-using-docker.html":  {
                                                                                                "href":  "content/installation/install-pi-adapter-for-structured-data-files-using-docker.html",
                                                                                                "title":  "Install PI Adapter for Structured Data Files using Docker",
                                                                                                "keywords":  "Install PI Adapter for Structured Data Files using Docker Docker is a set of tools that can be used on Linux to manage application deployments. This topic provides examples of how to create a Docker container with PI Adapter for Structured Data Files. Note: If you want to use Docker, you must be familiar with the underlying technology and have determined that it is appropriate for your planned use of the adapter. Docker is not a requirement to use the adapter. Create a startup script for the adapter Use a text editor and create a script similar to one of the following examples: Note: The script varies slightly by processor. ARM32 #!/bin/sh #! bin sh if [ -z $portnum ] ; then exec /StructuredDataFiles_linux-arm/OSIsoft.Data.System.Host  StructuredDataFiles_linux-arm OSIsoft.Data.System.Host else exec /StructuredDataFiles_linux-arm/OSIsoft.Data.System.Host  StructuredDataFiles_linux-arm OSIsoft.Data.System.Host --port:$portnum fi ARM64 #!/bin/sh #! bin sh if [ -z $portnum ] ; then exec /StructuredDataFiles_linux-arm64/OSIsoft.Data.System.Host  StructuredDataFiles_linux-arm64 OSIsoft.Data.System.Host else exec /StructuredDataFiles_linux-arm64/OSIsoft.Data.System.Host  StructuredDataFiles_linux-arm64 OSIsoft.Data.System.Host --port:$portnum fi AMD64 #!/bin/sh #! bin sh if [ -z $portnum ] ; then exec /StructuredDataFiles_linux-x64/OSIsoft.Data.System.Host  StructuredDataFiles_linux-x64 OSIsoft.Data.System.Host else exec /StructuredDataFiles_linux-x64/OSIsoft.Data.System.Host  StructuredDataFiles_linux-x64 OSIsoft.Data.System.Host --port:$portnum fi Name the script sdfdockerstart.sh and save it to the directory where you plan to create the container. Create a Docker container containing the adapter Create the following Dockerfile in the directory where you want to create and run the container. Note: Dockerfile is the required name of the file. Use the variation according to your operating system. ARM32 FROM ubuntu WORKDIR /   RUN apt-get update \u0026\u0026 DEBIAN_FRONTEND=noninteractive apt-get install -y ca-certificates libicu60 libssl1.1 curl COPY sdfdockerstart.sh /   RUN chmod +x /sdfdockerstart.sh  sdfdockerstart.sh ADD ./StructuredDataFiles_linux-arm.tar.gz . StructuredDataFiles_linux-arm.tar.gz . ENTRYPOINT [\"/sdfdockerstart.sh\"] [\" sdfdockerstart.sh\"] ARM64 FROM ubuntu WORKDIR /   RUN apt-get update \u0026\u0026 DEBIAN_FRONTEND=noninteractive apt-get install -y ca-certificates libicu66 libssl1.1 curl COPY sdfdockerstart.sh /   RUN chmod +x /sdfdockerstart.sh  sdfdockerstart.sh ADD ./StructuredDataFiles_linux-arm64.tar.gz . StructuredDataFiles_linux-arm64.tar.gz . ENTRYPOINT [\"/sdfdockerstart.sh\"] [\" sdfdockerstart.sh\"] AMD64 (x64) FROM ubuntu WORKDIR /   RUN apt-get update \u0026\u0026 DEBIAN_FRONTEND=noninteractive apt-get install -y ca-certificates libicu66 libssl1.1 curl COPY sdfdockerstart.sh /   RUN chmod +x /sdfdockerstart.sh  sdfdockerstart.sh ADD ./StructuredDataFiles_linux-x64.tar.gz . StructuredDataFiles_linux-x64.tar.gz . ENTRYPOINT [\"/sdfdockerstart.sh\"] [\" sdfdockerstart.sh\"] Copy the StructuredDataFiles_linux-\\\u003cplatform\u003e.tar.gz file to the same directory as the Dockerfile . Copy the sdfdockerstart.sh script to the same directory as the Dockerfile . Run the following command line in the same directory (you may need to use the sudo command): docker build -t sdfadapter . Run the adapter Docker container REST access from the local host to the Docker container Complete the following steps to run the container: Use the docker container image sdfadapter that you created previously. Type the following command line (you may need to use the sudo command): docker run -d --network host sdfadapter The default port 5590 is accessible from the host, and you can make REST calls to the adapter from applications on the local host computer. In this example, all data stored by the adapter is stored in the container itself. When the container is deleted, the data stored is also deleted. Provide persistent storage for the Docker container If you have a file share directory /sdf/InputDirectory  sdf InputDirectory and you want to move the files to /sdf/OutputDirectory  sdf OutputDirectory after processing, for the Docker container to access these directories and the storage on the host machine, complete the following steps to run the container: Use the docker container image sdfadapter created previously. Enter the following command line (you may need to use the sudo command): docker run -d --network host -v /sdf:/usr/share/OSIsoft/  sdf: usr share OSIsoft  sdfadapter Update the InputDirectory and OutputDirectory of your data source configuration to following settings: { \"InputDirectory\": \"/usr/share/OSIsoft/InputDirectory\", \" usr share OSIsoft InputDirectory\", \"OutputDirectory\": \"/usr/share/OSIsoft/OutputDirectory\" \" usr share OSIsoft OutputDirectory\" } Note: /sdf  sdf is replaced by /usr/share/OSIsoft  usr share OSIsoft , the target directory inside the container. The default port 5590 is accessible from the host and you can make REST calls to the adapter from applications on the local host computer. The data is written to a host directory on the local machine /sdf  sdf rather than the container. Port number change To use a different port other than the default 5590 , you can specify a portnum variable on the docker run command line. For example, to start the adapter using port 6000 instead of 5590 , use the following command line: docker run -d -e portnum=6000 --network host sdfadapter This command accesses the REST API with port 6000 instead of port 5590 . The following curl command returns the configuration for the container. curl http://localhost:6000/api/v1/configuration http:  localhost:6000 api v1 configuration Remove REST access to the Docker container If you remove the --network host option from the docker run command, REST access is not possible from outside the container. This can be valuable when you want to host another application in the same container as this adapter but do not want to have external REST access enabled."
                                                                                            },
    "content/installation/system-requirements.html":  {
                                                          "href":  "content/installation/system-requirements.html",
                                                          "title":  "System requirements",
                                                          "keywords":  "System requirements PI Adapter for Structured Data Files is supported on a variety of platforms and processors. Installation kits are available for the following platforms: Operating System Platform Installation Kit Processor(s) Windows 10 Enterprise Windows 10 IoT Enterprise x64 SDF_win10-x64.msi Intel/AMD Intel AMD 64-bit processors Debian 9, 10 Ubuntu 18.04, 20.04 x64 SDF_linux-x64.deb Intel/AMD Intel AMD 64-bit processors Debian 9, 10 Ubuntu 18.04, 20.04 ARM32 SDF_linux-arm.deb Arm 32-bit processors Debian 9, 10 Ubuntu 18.04, 20.04 ARM64 SDF_linux-arm64.deb Arm 64-bit processors Alternatively, you can use tar.gz files with binaries to build your own custom installers for containers on Linux. For more information on installation of the PI Adapter for Structured Data Files with a Docker container, see Install PI Adapter for Structured Data Files using Docker ."
                                                      },
    "content/main/README.html":  {
                                     "href":  "content/main/README.html",
                                     "title":  "PI Adapter Docs",
                                     "keywords":  "PI Adapter Docs"
                                 },
    "content/main/shared-content/administration/administration.html":  {
                                                                           "href":  "content/main/shared-content/administration/administration.html",
                                                                           "title":  "Administration",
                                                                           "keywords":  "Administration With the PI adapter administration level functions, you can start and stop an adapter service and the individual adapter ingress components. You can also retrieve product version information and delete an adapter. The examples in the administration topics use curl , a commonly available tool on both Windows and Linux. You can use the same operations with any programming language or tool that supports making REST calls. You can also configure PI adapters with the EdgeCmd utility. For more information, see the EdgeCmd utility documentation (https://osisoft.github.io/Edgecmd-Docs/content/edgecmd-utility.html) (https:  osisoft.github.io Edgecmd-Docs content edgecmd-utility.html) . To validate successful configurations, you can accomplish data retrieval steps ( GET commands) using a browser, if available on your device. For more information on PI adapter configuration tools, see Configuration tools ."
                                                                       },
    "content/main/shared-content/administration/delete-an-adapter-component.html":  {
                                                                                        "href":  "content/main/shared-content/administration/delete-an-adapter-component.html",
                                                                                        "title":  "Delete an adapter component",
                                                                                        "keywords":  "Delete an adapter component When you remove an adapter component, the configuration and log files are saved into a sub-directory in case they are needed later. Any associated types, streams, and data remain on the respective endpoints. Complete the following steps to delete an adapter component: Start any of the Configuration tools capable of making HTTP requests. Run a DELETE command to the following endpoint: http://localhost:5590/api/v1/configuration/system/components/\u003cComponentId\u003e http:  localhost:5590 api v1 configuration system components \u003cComponentId\u003e Note: You must make an empty DELETE command against the Id of the component you want to delete. 5590 is the default port number. If you selected a different port number, replace it with that value. Example using curl : Delete an adapter component curl -X DELETE \"http://localhost:5590/api/v1/configuration/system/components/\u003cComponentId\u003e\" \"http:  localhost:5590 api v1 configuration system components \u003cComponentId\u003e\" File relocation All configuration and log files are renamed and moved. The files are renamed according to the timestamp of removal, for example, FileName.json_removed_yyyy-MM-dd--hh-mm-ss . Configuration files are moved to the following location: Windows: %programdata%\\OSIsoft\\Adapters\\AdapterName\\Configuration\\Removed Linux: /usr/share/OSIsoft/Adapters/AdapterName/Configuration/Removed  usr share OSIsoft Adapters AdapterName Configuration Removed Log files are moved to the following location: Windows: %programdata%\\OSIsoft\\Adapters\\AdapterName\\Logs\\Removed Linux: /usr/share/OSIsoft/Adapters/AdapterName/Logs/Removed  usr share OSIsoft Adapters AdapterName Logs Removed In the following example, one adapter service is installed on a particular Windows node with the name \u003cAdapter\u003eService1 . An adapter component with the name \u003cAdapter\u003eDeviceX was added and configured to this adapter and later removed. Linux follows a similar behavior. This is the resulting relocation and renaming scheme after deletion: REST URLs Relative URL HTTP verb Action api/v1/configuration/system/components/ api v1 configuration system components  ComponentId DELETE Deletes specified component Note: Replace ComponentId with the Id of the component that you want to delete."
                                                                                    },
    "content/main/shared-content/administration/retrieve-product-version-information.html":  {
                                                                                                 "href":  "content/main/shared-content/administration/retrieve-product-version-information.html",
                                                                                                 "title":  "Retrieve product version information",
                                                                                                 "keywords":  "Retrieve product version information The product version information includes the application version, the version of the underlying .NET Core framework, and the operating system that the adapter is running on. Complete the following steps to retrieve the product version information of a PI adapter: Use any of the Configuration tools capable of making HTTP requests. Run a GET command to the following endpoint: http://localhost:5590/api/v1/Diagnostics/ProductInformation http:  localhost:5590 api v1 Diagnostics ProductInformation Note: 5590 is the default port number. If you selected a different port number, replace it with that value. Example using curl : Get product information for adapter hosted on port 5590 curl -X GET \"http://localhost:5590/api/v1/Diagnostics/ProductInformation \"http:  localhost:5590 api v1 Diagnostics ProductInformation Example result: { \"Application Version\":\"1.2.0.37\", \".Net Core Version\":\".NET Core 3.1.5\", \"Operating System\":\"Linux 4.15.0-106-generic #107-Ubuntu SMP Thu Jun 4 11:27:52 UTC 2020\" }"
                                                                                             },
    "content/main/shared-content/administration/start-and-stop-an-adapter.html":  {
                                                                                      "href":  "content/main/shared-content/administration/start-and-stop-an-adapter.html",
                                                                                      "title":  "Start and stop an adapter",
                                                                                      "keywords":  "Start and stop an adapter Complete the procedure appropriate for your operating system to start or stop an adapter service: Windows Open Windows services. Select PI Adapter for \u003cAdapterName\u003e . Depending on whether your adapter is running or not, click either Start or Stop . Linux Open command line. Depending on whether your adapter is running or not, type one of the following commands: Example: Start PI Adapter for \u003cAdapterName\u003e sudo systemctl start pi.adapter.\u003cadapterName\u003e Example: Stop PI Adapter for \u003cAdapterName\u003e sudo systemctl stop pi.adapter.\u003cadapterName\u003e Press Enter."
                                                                                  },
    "content/main/shared-content/administration/start-and-stop-ingress-component.html":  {
                                                                                             "href":  "content/main/shared-content/administration/start-and-stop-ingress-component.html",
                                                                                             "title":  "Start and stop ingress component",
                                                                                             "keywords":  "Start and stop ingress component To control data ingress, you can start and stop the ingress components of an adapter whenever necessary. By default, all currently configured ingress components are started. Start an ingress component Complete the following steps to start an individual ingress component: Use any of the Configuration tools capable of making HTTP requests. Run a POST command to the following endpoint, replacing \u003cComponentId\u003e with the ingress component that you want to start: http://localhost:5590/api/v1/Administration/\u003cComponentId\u003e/Start http:  localhost:5590 api v1 Administration \u003cComponentId\u003e Start Note: 5590 is the default port number. If you selected a different port number, replace it with that value. Example using curl : Start the adapter ingress component curl -d \"\" -X POST \"http://localhost:5590/api/v1/Administration/\u003cComponentId\u003e/Start\" \"http:  localhost:5590 api v1 Administration \u003cComponentId\u003e Start\" Stop an ingress component Complete the following steps to stop an individual ingress component: Start any configuration tool capable of making HTTP requests. Run a POST command to the following endpoint, replacing \u003cComponentId\u003e with the ingress component that you want to stop: http://localhost:5590/api/v1/Administration/\u003cComponentId\u003e/Stop http:  localhost:5590 api v1 Administration \u003cComponentId\u003e Stop Note: 5590 is the default port number. If you selected a different port number, replace it with that value. Example using curl : Stop the adapter ingress component curl -d \"\" -X POST \"http://localhost:5590/api/v1/Administration/\u003cComponentId\u003e/Stop\" \"http:  localhost:5590 api v1 Administration \u003cComponentId\u003e Stop\""
                                                                                         },
    "content/main/shared-content/configuration/buffering-configuration.html":  {
                                                                                   "href":  "content/main/shared-content/configuration/buffering-configuration.html",
                                                                                   "title":  "Buffering configuration",
                                                                                   "keywords":  "Buffering configuration You can configure PI adapters to buffer data egressed from the adapter to endpoints. Buffering is configured through the buffering configuration parameters in the system configuration. Note: OSIsoft recommends that you do not modify the default buffering location unless it is necessary. Changes to the buffering configuration parameters only take effect during adapter service startup. Configure buffering Complete the following steps to configure buffering. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/system/buffering http:  localhost:5590 api v1 configuration system buffering REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for buffering into the file. For sample JSON, see Examples - Retrieve the buffering configuration . Update the example JSON parameters for your environment. For a table of all available parameters, see Buffering parameters . Save the file. For example, as ConfigureBuffering.json . Open a command line session. Change directory to the location of ConfigureBuffering.json . Enter the following cURL command (which uses the PUT method) to initialize the buffering configuration. curl -d \"@ConfigureBuffering.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/system/buffering\" \"http:  localhost:5590 api v1 configuration system buffering\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or replacing a buffering configuration, see REST URLs . Buffering schema The full schema definition for the system buffering is in the System_Buffering_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux: /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas Buffering parameters The following parameters are available for configuring buffering: Parameter Required Type Description EnablePersistentBuffering Optional boolean Enables or disables on-disk buffering Allowed value: true or false Default value: true Note: If you disable persistent buffering, in-memory buffering is used. In-memory buffering is limited by value in the MaxBufferSizeMB property. MaxBufferSizeMB Optional integer Defines the maximum size of the buffer files that are persisted on disk or used in memory when EnablePersistentBuffering is set to false per configured endpoint. The unit is specified in MB (1 Megabyte = 1048576 bytes). Consider the capacity and the type of storage medium to determine a suitable value for this parameter. Minimum value: 1 Maximum value: 2147483647 Default value: 1024 BufferLocation Required string Defines the location of the buffer files. Absolute paths are required. Consider the access-control list (ACL) when you set this parameter. Allowed value: Valid path to a folder location in the file system Default value: Windows: %ProgramData%\\OSIsoft\\Adapters\\{AdapterInstance}\\Buffers Linux: /usr/share/OSIsoft/Adapters/{AdapterInstance}/Buffers  usr share OSIsoft Adapters {AdapterInstance} Buffers Examples The following examples are buffering configurations made through the curl REST client. Retrieve the buffering configuration curl -X GET \"http://localhost:5590/api/v1/configuration/system/buffering\" \"http:  localhost:5590 api v1 configuration system buffering\" Sample output: { \"bufferLocation\": \"C:/ProgramData/OSIsoft/Adapters/\u003cAdapterName\u003e/Buffers\", \"C: ProgramData OSIsoft Adapters \u003cAdapterName\u003e Buffers\", \"maxBufferSizeMB\": 1024, \"enablePersistentBuffering\": true } 200 OK response indicates success. Update MaxBufferSizeMb parameter curl -d \"{ \\\"MaxBufferSizeMB\\\": 100 }\" -H \"Content-Type: application/json\" application json\" -X PATCH \"http://localhost:5590/api/v1/configuration/system/buffering\" \"http:  localhost:5590 api v1 configuration system buffering\" 204 No Content response indicates success. REST URLs Relative URL HTTP verb Action api/v1/configuration/system/buffering api v1 configuration system buffering GET Gets the buffering configuration api/v1/configuration/system/buffering api v1 configuration system buffering PUT Replaces the existing buffering configuration api/v1/configuration/system/buffering api v1 configuration system buffering PATCH Update parameter, partial configuration"
                                                                               },
    "content/main/shared-content/configuration/configuration-tools.html":  {
                                                                               "href":  "content/main/shared-content/configuration/configuration-tools.html",
                                                                               "title":  "Configuration tools",
                                                                               "keywords":  "Configuration tools You can configure PI adapters with the EdgeCmd utility, OSIsoft\u0027s proprietary tool for configuring adapters, or a commonly-used REST tool. EdgeCmd utility The EdgeCmd utility enables adapter configuration on both Linux and Windows operating systems. For more information on using the EdgeCmd utility, see the EdgeCmd utility documentation . REST tools The following tools are available to make REST calls: curl curl is a command line tool used to make HTTP calls and is supported on both Windows and Linux operating systems. You can script curl with Bash or PowerShell on Linux or Windows and you can use it to perform adapter administrative and programming tasks. curl commands are used in configuration and management examples throughout this document. For more information, see curl (https://curl.haxx.se/) (https:  curl.haxx.se ) . Postman Postman is a REST tool for systems with GUI components. PI adapters are supported on platforms without GUIs. Postman is particularly useful for learning more about PI Adapter REST APIs. For more information, see Postman (https://www.postman.com/) (https:  www.postman.com ) ."
                                                                           },
    "content/main/shared-content/configuration/data-filters-configuration.html":  {
                                                                                      "href":  "content/main/shared-content/configuration/data-filters-configuration.html",
                                                                                      "title":  "Data filters configuration",
                                                                                      "keywords":  "Data filters configuration PI adapters can be configured to perform data filtering to save network bandwidth. Every data item in the data selection configuration can be assigned the ID of a data filter. The adapter will then filter data for those data items based on the data filter configuration. Configure data filters Complete the following steps to configure data filters. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/DataFilters http:  localhost:5590 api v1 configuration \u003cComponentId\u003e DataFilters REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for data filters into the file. For sample JSON, see Data filters example . Update the example JSON parameters for your environment. For a table of all available parameters, see Data filters parameters . Save the file. For example, as ConfigureDataFilters.json . Open a command line session. Change directory to the location of ConfigureDataFilters.json . Enter the following cURL command (which uses the PUT method) to initialize the data filters configuration. curl -d \"@ConfigureDataFilters.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/DataFilters\" \"http:  localhost:5590 api v1 configuration \u003cComponentId\u003e DataFilters\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or deleting a data filters configuration, see REST URLs . On successful execution, the change that you have made to data filters takes effect immediately during runtime. Data filters schema The full schema definition for the data filters configuration is in the AdapterName_DataFilters_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux: /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas Data filters parameters The following parameters are available for configuring data filters: Parameter Required Type Description Id Required string Unique identifier for the data filter. Allowed value: any string identifier AbsoluteDeadband Optional double Specifies the absolute change in data value that should cause the current value to pass the filter test. Note: You must specify AbsoluteDeadband or PercentChange . Allowed value: double value representing absolute deadband number Default value: null PercentChange Optional double Specifies the percent change from previous value that should cause the current value to pass the filter test. Note: You must specify AbsoluteDeadband or PercentChange . Allowed value: double value representing percent change Default value: null ExpirationPeriod Optional timespan The length in time that can elapse after an event before automatically storing the next event. The expected format is HH:MM:SS.###. * Allowed value: any timespan Default value: null * Note: You can also specify timespans as numbers in seconds. For example, \"ExpirationPeriod\": 25 specifies 25 seconds, or \"ExpirationPeriod\": 125 specifies 2 minutes and 5 seconds. Data filters example [ { \"Id\": \"DuplicateData\", \"AbsoluteDeadband\": 0, \"PercentChange\": null, \"ExpirationPeriod\": \"01:00:00\" } ] REST URLs Relative URL HTTP verb Action api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters GET Gets all configured data filters. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters DELETE Deletes all configured data filters. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters POST Adds an array of data filters or a single data filter. Fails if any data filter already exists. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters PUT Replaces all data. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters  DataFilters PATCH Allows partial updating of configured data filter. api/v1/configuration/ api v1 configuration  ComponentId /DataFilters/  DataFilters  id GET Gets configured data filter by id . api/v1/configuration/ api v1 configuration  ComponentId /DataFilters/  DataFilters  id DELETE Deletes configured data filter by id . api/v1/configuration/ api v1 configuration  ComponentId /DataFilters/  DataFilters  id PUT Replaces data filter by id . Fails if data filter does not exist. Note: Replace ComponentId with the Id of your adapter component."
                                                                                  },
    "content/main/shared-content/configuration/discovery-configuration.html":  {
                                                                                   "href":  "content/main/shared-content/configuration/discovery-configuration.html",
                                                                                   "title":  "Discovery configuration",
                                                                                   "keywords":  "Discovery configuration You can perform a data discovery for existing data pieces on demand. Data discovery is initiated through REST calls and it is tied to a specific discovery ID, which you can either specify or let the adapter generate. Data discovery includes different routes. For example, you can choose to do the following: Retrieve the discovery results Query the discovery status Cancel or delete discoveries Merge discovery results with the data selection item Retrieve results from a current discovery and compare it with results from a previous discovery Retrieve results from a current discovery and compare it with results from a current data selection configuration Configure discovery Start any of the Configuration tools capable of making HTTP requests. Run a POST command with the Id of the discovery and autoSelect set to either true or false to the following endpoint: http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Discoveries http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Discoveries . Note: 5590 is the default port number. If you selected a different port number, replace it with that value. Example using curl : curl -d \"{ \\\"Id\\\":\\\"TestDiscovery\\\", \\\"autoSelect\\\":true }\" -X POST \"http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Discoveries\" \"http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Discoveries\" Discovery parameters Parameter Type Description id string The ID of the discovery. Note: You cannot run multiple discoveries with the same ID. query string A filter that is specific to the data source. The query filter can limit the scope of the discovery. For more information, see the Data source configuration topic of your specific adapter. startTime datetime Time when the discovery started. endTime datetime Time when the discovery ended. progress double Progress of the discovery (number of topics found through the discovery) itemsFound integer Number of data pieces that the discovery found on the data source. newItems integer Number of new data pieces that the discovery found in comparison to the previous discovery. newAssets integer Number of new assets that the discovery found in comparison to the previous discovery. resultUri integer URL at which you can access the results of the discovery. autoSelect boolean When set to true , the result of the discovery gets pushed to the data selection. status reference Status of the discovery, for example Active or Complete . errors string Errors encountered during the discovery. Discoveries status example The following example shows the status of all discoveries. The discovery id in this example was auto-generated. [ { \"id\": \"8ff855f1-a636-490a-bb31-207410a6e607\", \"query\": null, \"startTime\": \"2020-09-30T19:34:01.8180401+02:00\", \"endTime\": \"2020-09-30T19:34:01.8368776+02:00\", \"progress\": 30, \"itemsFound\": 4, \"newItems\": 0, \"newAssets\": 0, \"resultUri\": \"http://127.0.0.1:5590/api/v1/Configuration/\u003cComponentId\u003e/Discoveries/8ff855f1-a636-490a-bb31-207410a6e607/result\", \"http:  127.0.0.1:5590 api v1 Configuration \u003cComponentId\u003e Discoveries 8ff855f1-a636-490a-bb31-207410a6e607 result\", \"autoSelect\": false, \"status\": \"Complete\", \"errors\": null } ] REST URLs Relative URL HTTP verb Action api/v1/configuration/ api v1 configuration  componentId /discoveries  discoveries GET Returns status of all discoveries api/v1/configuration/ api v1 configuration  componentId /discoveries  discoveries POST Initiates a new discovery and returns its Id api/v1/configuration/ api v1 configuration  componentId /discoveries  discoveries DELETE Cancels and deletes all saved discoveries api/v1/configuration/ api v1 configuration  componentId /discoveries/  discoveries  discoveryId GET Gets the status of an individual discovery Note: If a discovery with the specified ID does not exist, you will get an error message api/v1/configuration/ api v1 configuration  componentId /discoveries/  discoveries  discoveryId DELETE Cancels and deletes discovery and result api/v1/configuration/ api v1 configuration  componentId /discoveries/  discoveries  discoveryId /result  result GET Returns the result of a discovery api/v1/configuration/ api v1 configuration  componentId /discoveries/  discoveries  discoveryId /result?diff=  result?diff= previousId GET Returns the difference between the result and the previous result api/v1/configuration/ api v1 configuration  componentId /dataselection?diff=  dataselection?diff= discoveryId GET Returns the difference between the data selection configuration and the discovery results api/v1/configuration/ api v1 configuration  componentId /discoveries/  discoveries  discoveryId /result  result DELETE Cancels and deletes discovery result. Note: The discovery ID is still valid, but the Status property of a discovery query will contain a status of canceled api/v1/configuration/ api v1 configuration  componentId /dataselection/select?discoveryid=  dataselection select?discoveryid= discoveryId POST Adds the discovered items to data selection with selected set to true api/v1/configuration/ api v1 configuration  componentId /dataselection/unselect?discoveryid=  dataselection unselect?discoveryid= discoveryId POST Adds the discovered items to data selection with selected set to false Note: Replace ComponentId with the Id of your adapter component. Replace DiscoveryId with the Id of the discovery for which you want to perform the action."
                                                                               },
    "content/main/shared-content/configuration/egress-endpoint-configuration/configure-a-network-proxy.html":  {
                                                                                                                   "href":  "content/main/shared-content/configuration/egress-endpoint-configuration/configure-a-network-proxy.html",
                                                                                                                   "title":  "Configure a network proxy",
                                                                                                                   "keywords":  "Configure a network proxy Some network architectures may need a network proxy between the PI adapter and the egress endpoint. The process for configuring the adapter to egress data through a network proxy varies depending on the proxy type. HTTPS forward proxy For the adapter to use an HTTPS forward proxy while egressing, configure the https_proxy environment variable. For information on how to configure system environment variables, refer to your platform specific documentation: Windows: setx Ubuntu: EnvironmentVariables Debian: EnvironmentVariables Docker: Environment variables in Compose The value of this environment variable must contain the URL of the proxy server, beginning with http . The format of the string is [user[:password]@]http://hostname[:port] [user[:password]@]http:  hostname[:port] . HTTPS proxy environment variable Parameter Required Description user Optional The user name for the HTTPS forward proxy. password Optional The password for the HTTPS forward proxy specified user name. If you specify user , password remains optional. port Optional If you do not specify port , the default 80 is used. Note: Usage of the https_proxy environment variable may affect other .NET or .NET Core applications. If you set this environment variable, it will affect the adapter egress endpoints and the adapter health endpoints. Examples: myUser@http://192.168.2.2 myUser@http:  192.168.2.2 myUser:myPassword@http://proxymachine.domain:3128 myUser:myPassword@http:  proxymachine.domain:3128 http://proxymachine.domain http:  proxymachine.domain In Windows, this may look something like: Example of an architecture with an https forward proxy: Reverse proxy For the adapter to use a reverse proxy while egressing, you must configure the reverse proxy as an egress endpoint. For information on how to configure an egress endpoint, see Egress endpoints configuration . Example: [{ \"Id\": \"PI Web API Through Proxy\", \"Endpoint\": \"https://\u003creverseProxy\u003e:\u003cport\u003e/piwebapi/omf/\", \"https:  \u003creverseProxy\u003e:\u003cport\u003e piwebapi omf \", \"UserName\": \"\u003cpiWebApiUser\u003e\", \"Password\": \"\u003cpiWebApiPassword\u003e\" }] Example of an architecture with a reverse proxy:"
                                                                                                               },
    "content/main/shared-content/configuration/egress-endpoint-configuration/prepare-egress-destinations.html":  {
                                                                                                                     "href":  "content/main/shared-content/configuration/egress-endpoint-configuration/prepare-egress-destinations.html",
                                                                                                                     "title":  "Prepare egress destinations",
                                                                                                                     "keywords":  "Prepare egress destinations OCS and PI Server destinations may require additional configuration to receive OMF messages. OCS To prepare OCS to receive OMF messages from the adapter, create an OMF connection in OCS. Creating an OMF connection results in an available OMF endpoint that can be used by the adapter egress mechanism. Complete the following steps to create an OMF connection: Create a Client . The Client Id and Client Secret will be used for the corresponding properties in the egress configuration. Create an OMF type Connection . The connection should link the created client to an existing namespace where the data will be stored. The OMF Endpoint URL for the connection will be used as the egress configuration Endpoint property. PI Server To prepare a PI Server to receive OMF messages from the adapter, a PI Web API OMF endpoint must be available. Complete the following steps: Install PI Web API and enable the OSIsoft Message Format (OMF) Services feature. During configuration, choose an AF database and PI Data Archive where metadata and data will be stored. The account used in an egress configuration needs permissions to create AF elements, element templates, and PI points. Configure PI Web API to use Basic authentication. For complete steps, as well as best practices and recommendations, see the PI Web API User Guide on Live Library. Note: The certificate used by PI Web API must be trusted by the device running the adapter, otherwise the egress configuration ValidateEndpointCertificate property needs to be set to false (this can be the case with a self-signed certificate but should only be used for testing purposes). Note: To continue to send OMF egress messages to the PI Web API endpoint after upgrading PI Web API, restart the adapter service."
                                                                                                                 },
    "content/main/shared-content/configuration/egress-endpoints-configuration.html":  {
                                                                                          "href":  "content/main/shared-content/configuration/egress-endpoints-configuration.html",
                                                                                          "title":  "Egress endpoints configuration",
                                                                                          "keywords":  "Egress endpoints configuration PI adapters collect time series data, which they can send to a permanent data store (endpoint). This operation is called data egress. The following endpoints are available for data egress: OSIsoft Cloud Services (OCS) PI servers through PI Web API Edge Data Store (EDS) For long term storage and analysis, you can configure any adapter to send time series data to one or several of these endpoints in any combination. An egress endpoint is comprised of the properties specified under Egress endpoint parameters . Data egress to a PI server creates a PI point in the PI adapter configuration. Data egress to OCS or EDS creates a stream in the PI adapter configuration. Note: Egress to EDS requires the adapter to be on the same server as EDS. The name of the PI point or OCS or EDS stream is a combination of the StreamIdPrefix specified in the adapter data source configuration and the StreamId specified in the adapter data selection configuration. Configure egress endpoints Complete the following steps to configure egress endpoints. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/OmfEgress/dataendpoints http:  localhost:5590 api v1 configuration OmfEgress dataendpoints REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for egress endpoints into the file. For sample JSON, see Examples . Update the example JSON parameters for your environment. For a table of all available parameters, see Egress endpoint parameters . Save the file. For example, as ConfigureEgressEndpoints.json . Open a command line session. Change directory to the location of ConfigureEgressEndpoints.json . Enter the following cURL command (which uses the PUT method) to initialize the egress endpoints configuration. curl -d \"@ConfigureEgressEndpoints.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/OmfEgress/dataendpoints\" \"http:  localhost:5590 api v1 configuration OmfEgress dataendpoints\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or replacing an egress endpoints configuration, see REST URLs . Egress endpoint configuration schema The full schema definition for the egress endpoint configuration is in the OmfEgress_DataEndpoints_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux: /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas Egress endpoint parameters The following parameters are available for configuring egress endpoints: Parameter Required Type Description Id Optional string Unique identifier Allowed value: any string identifier Default value: new GUID Endpoint Required string Destination that accepts OMF v1.2 messages. Supported destinations include OCS, PI Server, and EDS. Allowed value: well-formed http or https endpoint string Default: null Username Required for PI server and EDS endpoint string Basic authentication to the PI Web API OMF or EDS endpoint PI server: Allowed value: any string Default: null Note: If your username contains a backslash, you must add an escape character, for example, type OilCompany\\TestUser as OilCompany\\\\TestUser . EDS: Allowed value: any string, can be null if the endpoint URL schema is HTTP Password Required for PI server and EDS endpoint string Basic authentication to the PI Web API OMF or EDS endpoint PI server: Allowed value: any string Default: null EDS: Allowed value: any string, can be null if the endpoint URL schema is HTTP ClientId Required for OCS endpoint string Authentication with the OCS OMF endpoint Allowed value: any string, can be null if the endpoint URL schema is HTTP Default: null ClientSecret Required for OCS endpoint string Authentication with the OCS OMF endpoint Allowed value: any string, can be null if the endpoint URL schema is HTTP Default: null TokenEndpoint Optional for OCS endpoint string Retrieves an OCS token from an alternative endpoint Allowed value: well-formed http or https endpoint string Default value: null ValidateEndpointCertificate Optional boolean Disables verification of destination certificate. Note: Only use for testing with self-signed certificates. Allowed value: true or false Default value: true Note: If the URL schema for the egress endpoint is HTTP, which is typical for EDS, then you do not need to specify credentials. Examples The following examples are valid egress configurations: Egress data to OCS [{ \"Id\": \"OCS\", \"Endpoint\": \"https://\u003cOCS \"https:  \u003cOCS OMF endpoint\u003e\", \"ClientId\": \"\u003cclientid\u003e\", \"ClientSecret\": \"\u003cclientsecret\u003e\" }] Egress data to PI Web API [{ \"Id\": \"PI Web API\", \"Endpoint\": \"https://\u003cpi \"https:  \u003cpi web api server\u003e:\u003cport\u003e/piwebapi/omf/\", server\u003e:\u003cport\u003e piwebapi omf \", \"UserName\": \"\u003cusername\u003e\", \"Password\": \"\u003cpassword\u003e\" }] Egress data to EDS [{ \"Id\": \"EDS\", \"Endpoint\": \"http://localhost:\u003cport\u003e/api/v1/tenants/default/namespaces/default/omf\", \"http:  localhost:\u003cport\u003e api v1 tenants default namespaces default omf\", \"UserName\": \"eds\", \"Password\": \"eds\" }] REST URLs Relative URL HTTP verb Action api/v1/configuration/omfegress/DataEndpoints api v1 configuration omfegress DataEndpoints GET Gets all configured egress endpoints api/v1/configuration/omfegress/DataEndpoints api v1 configuration omfegress DataEndpoints DELETE Deletes all configured egress endpoints api/v1/configuration/omfegress/DataEndpoints api v1 configuration omfegress DataEndpoints POST Adds an array of egress endpoints or a single endpoint. Fails if any endpoint already exists api/v1/configuration/omfegress/DataEndpoints api v1 configuration omfegress DataEndpoints PUT Replaces all egress endpoints api/v1/configuration/omfegress/DataEndpoints api v1 configuration omfegress DataEndpoints PATCH Allows partial updating of configured endpoints. Note: The request must be an array containing one or more endpoints. Each endpoint in the array must include its Id . api/v1/configuration/omfegress/DataEndpoints/{Id} api v1 configuration omfegress DataEndpoints {Id} GET Gets configured endpoint by Id api/v1/configuration/omfegress/DataEndpoints/{Id} api v1 configuration omfegress DataEndpoints {Id} DELETE Deletes configured endpoint by Id api/v1/configuration/omfegress/DataEndpoints/{Id} api v1 configuration omfegress DataEndpoints {Id} PUT Updates or creates a new endpoint with the specified Id api/v1/configuration/omfegress/DataEndpoints/{Id} api v1 configuration omfegress DataEndpoints {Id} PATCH Allows partial updating of configured endpoint by Id Egress execution details After configuring an egress endpoint, egress is immediately run for that endpoint. Egress is handled individually per configured endpoint. When data is egressed for the first time, types and containers are egressed to the configured endpoint. After that only new or changed types or containers are egressed. Type creation must be successful in order to create containers. Container creation must be successful in order to egress data. If you delete an egress endpoint, data flow immediately stops for that endpoint. Buffered data in a deleted endpoint is permanently lost. Type, container, and data items are batched into one or more OMF messages when egressing. As per the requirements defined in OMF, a single message payload will not exceed 192KB in size. Compression is automatically applied to outbound egress messages. On the egress destination, failure to add a single item results in the message failing. Types, containers, and data are egressed as long as the destination continues to respond to HTTP requests."
                                                                                      },
    "content/main/shared-content/configuration/general-configuration.html":  {
                                                                                 "href":  "content/main/shared-content/configuration/general-configuration.html",
                                                                                 "title":  "General configuration",
                                                                                 "keywords":  "General configuration You can configure PI adapters to produce and store diagnostics data at a designated health endpoint, and to send metadata for created streams. For more information about available diagnostics data, see Adapter diagnostics and Egress diagnostics . For more information about available metadata and what metadata are sent per metadata level, see Adapter Metadata . Configure general Start any of the Configuration tools capable of making HTTP requests. Run a PUT command to the following endpoint, setting enableDiagnostics to either true or false and MetadataLevel to None , Low , Medium , or High : http://localhost:5590/api/v1/configuration/system/general http:  localhost:5590 api v1 configuration system general Note: 5590 is the default port number. If you selected a different port number, replace it with that value. Example using curl : curl -d \"{ \\\"enableDiagnostics\\\":true, \\\"enableMetadata\\\":false }\" -X PUT \"http://localhost:5590/api/v1/configuration/system/general\" \"http:  localhost:5590 api v1 configuration system general\" General schema The full schema definition for the general configuration is in the System_General_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux: /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas General parameters The following parameters are available for configuring general: Parameter Required Type Description EnableDiagnostics Optional boolean Determines if diagnostics are enabled Allowed value: true or false Default value: true MetadataLevel Optional reference Defines amount of metadata sent to OMF endpoints. Allowed value: None , Low , Medium , and High Default value: Medium Example Retrieve the general configuration Example using curl : curl -X GET \"http://localhost:{port}/api/v1/configuration/system/general\" \"http:  localhost:{port} api v1 configuration system general\" Sample output: { \"EnableDiagnostics\": true, \"MetadataLevel\": \"Medium\" } REST URLs Relative URL HTTP verb Action api/v1/configuration/system/General api v1 configuration system General GET Gets the general configuration api/v1/configuration/system/General api v1 configuration system General PUT Replaces the existing general configuration api/v1/configuration/system/General api v1 configuration system General PATCH Allows partial updating of general configuration"
                                                                             },
    "content/main/shared-content/configuration/health-endpoint-configuration.html":  {
                                                                                         "href":  "content/main/shared-content/configuration/health-endpoint-configuration.html",
                                                                                         "title":  "Health endpoint configuration",
                                                                                         "keywords":  "Health endpoint configuration You can configure PI adapters to produce and store health data at a designated health endpoint. You can use health data to ensure that your adapters are running properly and that data flows to the configured OMF endpoints. For more information about adapter health, see Adapter health . Configure health endpoint A health endpoint designates an OMF endpoint where adapter health information is sent. You can configure multiple health endpoints. Complete the following steps to configure health endpoints. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/system/healthendpoints http:  localhost:5590 api v1 configuration system healthendpoints REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for health endpoints into the file. For sample JSON, see Examples . Update the example JSON parameters for your environment. For a table of all available parameters, see Health endpoint parameters . Save the file. For example, as ConfigureHealthEndpoints.json . Open a command line session. Change directory to the location of ConfigureHealthEndpoints.json . Enter the following cURL command (which uses the PUT method) to initialize the health endpoint configuration. curl -d \"@ConfigureHealthEndpoints.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/system/healthendpoints\" \"http:  localhost:5590 api v1 configuration system healthendpoints\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or replacing a health endpoints configuration, see REST URLs . Health endpoints schema The full schema definition for the health endpoint configuration is in the System_HealthEndpoints_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux: /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas Health endpoint parameters The following parameters are available for configuring health endpoints: Parameter Required Type Description Id Optional string Uniquely identifies the endpoint. This can be any alphanumeric string. If left blank, a unique value is generated automatically. Allowed value: any string identifier Default value: new GUID Endpoint Required string The URL of the OMF endpoint to receive this health data Allowed value: well-formed http or https endpoint string Default: null Username Required for PI Web API and EDS endpoints string The username used to authenticate with a PI Web API OMF or EDS endpoint PI server: Allowed value: any string Default: null EDS: Allowed value: any string, but cannot be null Password Required for PI Web API endpoints string The password used to authenticate with a PI Web API OMF or EDS endpoint PI server: Allowed value: any string Default: null EDS: Allowed value: any string, but cannot be null ClientId Required for OCS endpoints string The client ID used for authentication with an OSIsoft Cloud Services OMF endpoint Allowed value: any string Default: null ClientSecret Required for OCS endpoints string The client secret used for authentication with an OSIsoft Cloud Services OMF endpoint Allowed value: any string Default: null TokenEndpoint Optional for OCS endpoints string Retrieves an OCS token from an alternative endpoint Allowed value: well-formed http or https endpoint string Default value: null ValidateEndpointCertificate Optional boolean Disables verification of destination security certificate. Use for testing only with self-signed certificates; OSIsoft recommends keeping this set to the default, true, in production environments. Allowed value: true or false Default value: true Examples OCS endpoint { \"Id\": \"OCS\", \"Endpoint\": \"https://\u003cOCS \"https:  \u003cOCS OMF endpoint\u003e\", \"ClientId\": \"\u003cclientid\u003e\", \"ClientSecret\": \"\u003cclientsecret\u003e\" } PI Web API endpoint { \"Id\": \"PI Web API\", \"Endpoint\": \"https://\u003cpi \"https:  \u003cpi web api server\u003e:\u003cport\u003e/piwebapi/omf/\", server\u003e:\u003cport\u003e piwebapi omf \", \"UserName\": \"\u003cusername\u003e\", \"Password\": \"\u003cpassword\u003e\" } EDS endpoint [{ \"Id\": \"EDS\", \"Endpoint\": \"http://localhost:\u003cport\u003e/api/v1/tenants/default/namespaces/default/omf\", \"http:  localhost:\u003cport\u003e api v1 tenants default namespaces default omf\", \"UserName\": \"eds\", \"Password\": \"eds\" }] REST URLs Relative URL HTTP verb Action api/v1/configuration/system/healthEndpoints api v1 configuration system healthEndpoints GET Gets all configured health endpoints api/v1/configuration/system/healthEndpoints api v1 configuration system healthEndpoints DELETE Deletes all configured health endpoints api/v1/configuration/system/healthEndpoints api v1 configuration system healthEndpoints POST Adds an array of health endpoints or a single endpoint. Fails if any endpoint already exists api/v1/configuration/system/healthEndpoints api v1 configuration system healthEndpoints PUT Replaces all health endpoints. Note: Requires an array of endpoints api/v1/configuration/system/healthEndpoints api v1 configuration system healthEndpoints PATCH Allows partial updating of configured health endpoints Note: The request must be an array containing one or more health endpoints. Each health endpoint in the array must include its Id . api/v1/configuration/system/healthEndpoints/ api v1 configuration system healthEndpoints  Id GET Gets configured health endpoint by Id api/v1/configuration/system/healthEndpoints/ api v1 configuration system healthEndpoints  Id DELETE Deletes configured health endpoint by Id api/v1/configuration/system/healthEndpoints/ api v1 configuration system healthEndpoints  Id PUT Updates or creates a new health endpoint with the specified Id api/v1/configuration/system/healthEndpoints/ api v1 configuration system healthEndpoints  Id PATCH Allows partial updating of configured health endpoint by Id Note: Replace Id with the Id of the health endpoint."
                                                                                     },
    "content/main/shared-content/configuration/logging-configuration.html":  {
                                                                                 "href":  "content/main/shared-content/configuration/logging-configuration.html",
                                                                                 "title":  "Logging configuration",
                                                                                 "keywords":  "Logging configuration PI adapters write daily log messages for the adapter, the system, and OMF egress to flat text files in the following locations: ??? Windows: %ProgramData%\\OSIsoft\\Adapters{AdapterInstance}\\Logs ??? Linux: /usr/share/OSIsoft/Adapters/{AdapterInstance}/Logs  usr share OSIsoft Adapters {AdapterInstance} Logs Each message in the log displays the message severity level, timestamp, and the message itself. Configure logging Complete the following steps to configure logging. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Logging http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Logging REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for logging into the file. For sample JSON, see Example . Update the example JSON parameters for your environment. For a table of all available parameters, see Logging parameters . Save the file. For example, as ConfigureLogging.json . Open a command line session. Change directory to the location of ConfigureLogging.json . Enter the following cURL command (which uses the PUT method) to initialize the logging configuration. curl -d \"@ConfigureLogging.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Logging\" \"http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Logging\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or retrieving a logging configuration, see REST URLs . Any parameter not specified in the updated configuration file reverts to the default schema value. On successful execution, the log-level change takes effect immediately during runtime. The other configurations (log file size and file count) are updated after the adapter is restarted. Logging schema The full schema definition for the logging configuration is in the component specific logging file: AdapterName_Logging_schema.json , OmfEgress_Logging_schema.json , or System_Logging_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux: /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas Logging parameters The following parameters are available for configuring logging: Parameter Required Type Description LogLevel Optional reference The logLevel sets the minimum severity for messages to be included in the logs. Messages with a severity below the level set are not included. The log levels in their increasing order of severity are as follows: Trace , Debug , Information , Warning , Error , Critical , and None . Default log level: Information For detailed information about the log levels, see LogLevel . LogFileSizeLimitBytes Optional integer The maximum size (in bytes) of log files that the service will create for the component. The value must be a positive integer. Minimum value: 1000 Maximum value: 9223372036854775807 Default value: 34636833 LogFileCountLimit Optional integer The maximum number of log files that the service will create for the component. The value must be a positive integer. Minimum value: 1 Maximum value: 2147483647 Default value: 31 LogLevel Level Description Trace Logs that contain the most detailed messages. These messages may contain sensitive application data like actual received values, which is why these messages should not be enabled in production environment. Note: Trace is translated to Verbose in the log file. Debug Logs that can be used to troubleshoot data flow issues by recording metrics and detailed flow related information. Information Logs that track the general flow of the application. Any non-repetitive general information like the following can be useful for diagnosing potential application errors: - Version information related to the software at startup - External services used - Data source connection string - Number of measurements - Egress URL - Change of state ???Starting??? or ???Stopping??? - Configuration Warning Logs that highlight an abnormal or unexpected event in the application flow that does not otherwise cause the application execution to stop. Warning messages can indicate an unconfigured data source state, that a communication with backup failover instance has been lost, an insecure communication channel in use, or any other event that could require attention but that does not impact data flow. Error Logs that highlight when the current flow of execution is stopped due to a failure. These should indicate a failure in the current activity and not an application-wide failure. It can indicate an invalid configuration, unavailable external endpoint, internal flow error, and so on. Critical Logs that describe an unrecoverable application or system crash or a catastrophic failure that requires immediate attention. This can indicate application wide failures like beta timeout expired, unable to start self-hosted endpoint, unable to access vital resource (for example, Data Protection key file), and so on. Note: Critical is translated to Fatal in the log file. None Logging is disabled for the given component. Example Default logging configuration By default, logging captures Information, Warning, Error, and Critical messages in the message logs. The following logging configuration is the installation default for a component: { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 } REST URLs Relative URL HTTP verb Action api/v1/configuration/System/Logging api v1 configuration System Logging GET Retrieves the system logging configuration api/v1/configuration/System/Logging api v1 configuration System Logging PUT Updates the system logging configuration api/v1/configuration/ api v1 configuration  ComponentId /Logging  Logging GET Retrieves the logging configuration of the specified adapter component api/v1/configuration/ api v1 configuration  ComponentId /Logging  Logging PUT Updates the logging configuration of the specified adapter component Note: Replace ComponentId with the Id of your adapter component."
                                                                             },
    "content/main/shared-content/configuration/schedules-configuration.html":  {
                                                                                   "href":  "content/main/shared-content/configuration/schedules-configuration.html",
                                                                                   "title":  "Schedules configuration",
                                                                                   "keywords":  "Schedules configuration Complete the following steps to configure schedules. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Schedules http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Schedules REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for schedules into the file. For sample JSON, see Example . Update the example JSON parameters for your environment. For a table of all available parameters, see Schedules parameters . Save the file. For example, as ConfigureSchedules.json . Open a command line session. Change directory to the location of ConfigureSchedules.json . Enter the following cURL command (which uses the PUT method) to initialize the schedules configuration. curl -d \"@ConfigureSchedules.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Schedules\" \"http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Schedules\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or replacing a schedules configuration, see REST URLs . On successful execution, the schedules change takes effect immediately during runtime. Schedules schema The full schema definition for the schedules configuration is in the AdapterName_Schedules_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux: /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas Schedules parameters The following parameters are available for configuring schedules: Parameter Required Type Description Id Required string Unique identifier for the schedule Allowed value: any string identifier Period Required string The data sampling rate of the schedule. The expected format is HH:MM:SS.###. * Invalid input: null , negative timespan, zero Default value: null (must be specified) Offset Optional string The offset from the midnight when the schedule starts. The expected format is HH:MM:SS.### * Invalid input: negative timespan Default value: null * Note: You can also specify timespans as numbers in seconds. For example, \"Period\": 25 specifies 25 seconds, or \"Period\": 125 specifies 2 minutes and 5 seconds. Example [ { \"Id\": \"schedule1\", \"Period\": \"00:00:01.500\", \"Offset\": \"00:02:03\" } ] REST URLs Relative URL HTTP verb Action api/v1/configuration/ api v1 configuration  ComponentId /Schedules  Schedules GET Gets all configured schedules api/v1/configuration/ api v1 configuration  ComponentId /Schedules  Schedules DELETE Deletes all configured schedules api/v1/configuration/ api v1 configuration  ComponentId /Schedules  Schedules POST Adds an array of schedules or a single schedule. Fails if any schedule already exists api/v1/configuration/ api v1 configuration  ComponentId /Schedules  Schedules PUT Replaces all schedules api/v1/configuration/ api v1 configuration  ComponentId /Schedules/  Schedules  id GET Gets configured schedule by id api/v1/configuration/ api v1 configuration  ComponentId /Schedules/  Schedules  id DELETE Deletes configured schedule by id api/v1/configuration/ api v1 configuration  ComponentId /Schedules/  Schedules  id PUT Replaces schedule by id . Fails if schedule does not exist api/v1/configuration/ api v1 configuration  ComponentId /Schedules/  Schedules  id PATCH Allows partial updating of configured schedule by id Note: Replace ComponentId with the Id of your adapter component."
                                                                               },
    "content/main/shared-content/configuration/system-and-adapter-configuration.html":  {
                                                                                            "href":  "content/main/shared-content/configuration/system-and-adapter-configuration.html",
                                                                                            "title":  "System and adapter configuration",
                                                                                            "keywords":  "System and adapter configuration You can configure the system component and adapter component together using a single file. Change system and adapter configuration Complete the following steps to configure system and adapter. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for system and adapter into the file. For sample JSON, see the corresponding adapter configuration examples topic. Save the file. For example, as ConfigureSystemAndAdapter.json . Open a command line session. Change directory to the location of ConfigureSystemAndAdapter.json . Enter the following cURL command (which uses the PUT method) to initialize the system and adapter configuration. curl -d \"@ConfigureSystemAndAdapter.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration\" \"http:  localhost:5590 api v1 configuration\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. In order for some of the adapter specific configurations to take effect, you have to restart the adapter. If the operation fails due to errors in the configuration, the count of the error and suitable error messages are returned in the result. REST URLs Relative URL HTTP verb Action api/v1/configuration/ api v1 configuration  PUT Replaces the configuration for the entire adapter"
                                                                                        },
    "content/main/shared-content/configuration/system-components-configuration.html":  {
                                                                                           "href":  "content/main/shared-content/configuration/system-components-configuration.html",
                                                                                           "title":  "System components configuration",
                                                                                           "keywords":  "System components configuration PI adapters use JSON configuration files in a protected directory on Windows and Linux to store configuration that is read on startup. While the files are accessible to view, OSIsoft recommends that you use REST or the EdgeCmd utility for any changes you make to the files. As part of making adapters as secure as possible, any passwords or secrets that you configure are stored in encrypted form where cryptographic key material is stored separately in a secure location. If you edit the files directly, the adapter may not work as expected. Note: You can edit any single component or facet of the system individually using REST, but you can also configure the system as a whole with a single REST call. Configure system components Complete the following steps to configure system components. Use the PUT method in conjunction with the http://localhost:5590/api/v1/configuration/system/components http:  localhost:5590 api v1 configuration system components REST endpoint to initialize the configuration. Using a text editor, create an empty text file. Copy and paste an example configuration for system components into the file. For sample JSON, see Examples . Update the example JSON parameters for your environment. For a table of all available parameters, see System components parameters . Save the file. For example, as ConfigureComponents.json . Open a command line session. Change directory to the location of ConfigureComponents.json . Enter the following cURL command (which uses the PUT method) to initialize the system components configuration. curl -d \"@ConfigureComponents.json\" -H \"Content-Type: application/json\" application json\" -X PUT \"http://localhost:5590/api/v1/configuration/system/components/\u003ccomponentId\u003e\" \"http:  localhost:5590 api v1 configuration system components \u003ccomponentId\u003e\" Notes: If you installed the adapter to listen on a non-default port, update 5590 to the port number in use. For a list of other REST operations you can perform, like updating or deleting a system components configuration, see REST URLs . System components schema The full schema definition for the system components configuration is in the System_Components_schema.json file located in one of the following folders: Windows: %ProgramFiles%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Schemas Linux: /opt/OSIsoft/Adapters/\u003cAdapterName\u003e/Schemas  opt OSIsoft Adapters \u003cAdapterName\u003e Schemas System components parameters You can configure the following parameters for system components: Parameters Required Type Description ComponentId Required string The ID of the component 1 . It can be any alphanumeric string. A properly configured ComponentID follows these rules: Cannot contain leading or trailing space Cannot use the following characters: \u003e \u003c /   : ? # [ ] @ ! $ \u0026 * \\ \" ( ) \\\\ + , ; = \\| ` { } ComponentType Required string The type of the component. There are two types of components: OmfEgress and the adapter. 1 1 Note: The OmfEgress component is required to run the adapter. Both its ComponentId and ComponentType are reserved and should not be modified. Examples Default system components configuration The default System_Components.json file for the System component contains the following information. [ { \"ComponentId\": \"OmfEgress\", \"ComponentType\": \"OmfEgress\" } ] System components configuration with two adapter instances [ { \"ComponentId\": \"\u003cAdapterName\u003e1\", \"ComponentType\": \"\u003cAdapterName\u003e\" }, { \"ComponentId\": \"\u003cAdapterName\u003e2\", \"ComponentType\": \"\u003cAdapterName\u003e\" }, { \"ComponentId\": \"OmfEgress\", \"ComponentType\": \"OmfEgress\" } ] REST URLs Relative URL HTTP verb Action api/v1/configuration/system/components api v1 configuration system components GET Retrieves the system components configuration api/v1/configuration/system/components api v1 configuration system components POST Adds a new component to the system configuration api/v1/configuration/system/components api v1 configuration system components PUT Updates the system components configuration api/v1/configuration/system/components/ api v1 configuration system components  ComponentId DELETE Deletes a specific component from the system components configuration api/v1/configuration/system/components/ api v1 configuration system components  ComponentId PUT Creates a new component with the specified ComponentId in the system configuration"
                                                                                       },
    "content/main/shared-content/configuration/text-parser/jsonpath-syntax-for-value-retrieval.html":  {
                                                                                                           "href":  "content/main/shared-content/configuration/text-parser/jsonpath-syntax-for-value-retrieval.html",
                                                                                                           "title":  "JSONPath syntax for value retrieval",
                                                                                                           "keywords":  "JSONPath syntax for value retrieval For information on which semantic is used for retrieving values from JSON files, see JSONPath - XPath for JSON . The following syntax is used to extract values from JSON documents. JSON - Simple JSONPath example [ { \"time\": \"2020-08-10T12:10:46.0928791Z\", \"value\": 1.234567890 }, { \"time\": \"2020-08-10T12:10:47.0928791Z\", \"value\": 12.34567890 }, { \"time\": \"2020-08-10T12:10:48.0928791Z\", \"value\": 123.4567890 }, { \"time\": \"2020-08-10T12:10:49.0928791Z\", \"value\": 1234.567890 }, { \"time\": \"2020-08-10T12:10:50.0928791Z\", \"value\": 12345.67890 }, { \"time\": \"2020-08-10T12:10:51.0928791Z\", \"value\": 123456.7890 }, { \"time\": \"2020-08-10T12:10:52.0928791Z\", \"value\": 12345678.90 }, { \"time\": \"2020-08-10T12:10:53.0928791Z\", \"value\": 123456789.0 } ] The following JSONPath configuration reads a series of values: { \"Id\": \"DoubleValue\", \"FieldDefinition\": \"value\", \"DataType\": \"Double\" }, { \"Id\": \"Timestamp\", \"FieldDefinition\": \"time\", \"DataType\": \"DateTime\", \"IsIndex\": true } JSON - Complex JSONPath examples The following example reads specific values from a JSON array: { \"StreamData\": { \"TPPrototype.uflsample.value_time\": [ { \"StreamId\": \"TPPrototype.uflsample.value_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T06:00:00Z\", \"Value\": 339.0 }, { \"StreamId\": \"TPPrototype.uflsample.value_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T07:00:00Z\", \"Value\": 344.0 }, { \"StreamId\": \"TPPrototype.uflsample.value_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T17:00:00Z\", \"Value\": 341.0 } ], \"TPPrototype.uflsample.value_timeString\": [ { \"StreamId\": \"TPPrototype.uflsample.value_timeString\", \"DataType\": \"String\", \"Timestamp\": \"2013-12-01T06:00:00Z\", \"Value\": \"339.0\" }, { \"StreamId\": \"TPPrototype.uflsample.value_timeString\", \"DataType\": \"String\", \"Timestamp\": \"2013-12-01T07:00:00Z\", \"Value\": \"344.0\" }, { \"StreamId\": \"TPPrototype.uflsample.value_timeString\", \"DataType\": \"String\", \"Timestamp\": \"2013-12-01T17:00:00Z\", \"Value\": \"341.0\" } ], \"TPPrototype.uflsample.pressure_time\": [ { \"StreamId\": \"TPPrototype.uflsample.pressure_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T06:00:00Z\", \"Value\": 339.0 }, { \"StreamId\": \"TPPrototype.uflsample.pressure_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T07:00:00Z\", \"Value\": 344.0 }, { \"StreamId\": \"TPPrototype.uflsample.pressure_time\", \"DataType\": \"Double\", \"Timestamp\": \"2013-12-01T17:00:00Z\", \"Value\": 341.0 } ] } } The following JSONPath configuration reads all the TPPrototype.uflsample.value_time values from the JSON above: { \"Id\": \"Value\", \"DataType\": \"Double\", \"FieldDefinition\": \"$[\u0027StreamData\u0027].[\u0027TPPrototype.uflsample.value_time\u0027][*].Value\" }, { \"Id\": \"Time\", \"DataType\": \"DateTime\", \"FieldDefinition\": \"$[\u0027StreamData\u0027].[\u0027TPPrototype.uflsample.value_time\u0027][*].Timestamp\", \"IsIndex\": true } The following example reads specific value from complex nested JSON: { \"success\": true, \"error\": null, \"result\": { \"type\": \"runtime_history\", \"chart\": { \"chart\": { \"type\": \"column\" }, \"title\": { \"text\": \"\" }, \"subtitle\": { \"text\": \"Daily History\" }, \"colors\": [ \"#fee292\", \"#fdc152\", \"#f69638\", \"#f17130\", \"#9f2d26\", \"#8acadc\", \"#184c8e\" ], \"series\": [ { \"name\": \"Stage 3 Aux Heat\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"heat\", \"state\": \"heat_aux_stage3\" }, { \"name\": \"Stage 2 Aux Heat\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"heat\", \"state\": \"heat_aux_stage2\" }, { \"name\": \"Aux Heat\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"heat\", \"state\": \"heat_aux\" }, { \"name\": \"Stage 2 Heat\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"heat\", \"state\": \"heat_stage2\" }, { \"name\": \"Heat\", \"data\": [ 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.3, 0.2, 0.0 ], \"stack\": \"heat\", \"state\": \"heat\" }, { \"name\": \"Stage 2 Cool\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"cool\", \"state\": \"cool_stage2\" }, { \"name\": \"Cool\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"cool\", \"state\": \"cool\" } ], \"xAxis\": { \"categories\": [ \"Friday\", \"Saturday\", \"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\" ], \"labels\": { \"rotation\": -45 } }, \"yAxis\": { \"allowDecimals\": false, \"min\": 0, \"max\": 24, \"tickInternval\": 4, \"title\": { \"text\": \"Runtime (Hours)\" } }, \"legend\": { \"layout\": \"vertical\", \"align\": \"center\", \"floating\": false, \"shadow\": false, \"itemStyle\": { \"fontSize\": \"1em\" } }, \"tooltip\": { \"shared\": true, \"borderColor\": \"#000000\" }, \"credits\": { \"enabled\": false }, \"plotOptions\": { \"column\": { \"stacking\": \"normal\" }, \"series\": { \"shadow\": false } } }, \"table\": { \"headings\": [ \"Fri\", \"Sat\", \"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\" ], \"series\": [ { \"name\": \"Aux Heat\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": \"heat\", \"state\": \"heat_aux\" }, { \"name\": \"Outdoor High Temp.\", \"data\": [ 72.0, 64.0, 73.0, 72.0, 67.0, 73.0, 77.0, 62.0, 51.0 ], \"stack\": null, \"state\": \"outdoor_high_temperature\" }, { \"name\": \"Outdoor Low Temp.\", \"data\": [ 55.0, 60.0, 62.0, 61.0, 51.0, 43.0, 46.0, 44.0, 35.0 ], \"stack\": null, \"state\": \"outdoor_low_temperature\" }, { \"name\": \"Avg Indoor Temp.\", \"data\": [ 76.0, 77.0, 78.0, 78.0, 77.0, 73.0, 74.0, 75.0, 72.0 ], \"stack\": null, \"state\": \"average_indoor_temperature\" }, { \"name\": \"Avg Indoor Humidity\", \"data\": [ 66.0, 68.0, 70.0, 70.0, 69.0, 67.0, 67.0, 66.0, 61.0 ], \"stack\": null, \"state\": \"average_indoor_humidity\" }, { \"name\": \"Fan Only Runtime\", \"data\": [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], \"stack\": null, \"state\": \"fan_only\" }, { \"name\": \"Vent\", \"data\": [], \"stack\": null, \"state\": \"vent\" } ] }, \"show_monthly_runtime_history\": true } } The following JSONPath configuration reads Sunday Average Indoor Temperature. The timestamp comes from Adapter local time. { \"Id\": \"Temperature\", \"DataType\": \"Double\", \"FieldDefinition\": \"$.result.table.series[3].data[2]\" }, { \"Id\": \"Timestamp\", \"DataType\": \"DateTime\", \"Format\": \"Adapter\", \"IsIndex\": true } Error handling If you encounter text parser related errors that is errors for the ValueField or TimeField , check the StreamId associated with the error message. Possible errors include the following: The JSONPath expression of ValueField or TimeField is pointing to a non-existing value The JSONPath expression of ValueField or TimeField is missing a value altogether DataType does not match the value"
                                                                                                       },
    "content/main/shared-content/configuration/text-parser/text-parser.html":  {
                                                                                   "href":  "content/main/shared-content/configuration/text-parser/text-parser.html",
                                                                                   "title":  "Text parser",
                                                                                   "keywords":  "Text parser The adapter you are using includes the text parser component which ensures consistent parsing of text from different files. For more information on which file types are supported for your adapter, see the topics in this chapter. Designed to be a document parser, the text parser parses a semantically complete document in its entirety. The text parser produces OMF compatible output, which in turn is compatible with the OCS backing SDS (Sequential Data Store) that stores data in streams consisting of multiple values and indexes. Data types supported by the text parser The following data types are supported by the text parser: DateTime DateTimeOffset TimeSpan sbyte byte short ushort int uint long ulong float double decimal bool char string Note: Not all data types supported by the text parser are also supported by OMF. Culture support Some numeric values and datetimes support cultures when they are being parsed. The default culture is en-US (US English) (InvariantCulture). OSIsoft recommends to leave the adapter at the default unless you expect culturally variant input. Note: Installed cultures vary by machine with both Linux and Windows. If the specified culture is not installed, the text parser fails to parse input that requires that culture. Time zone support A time zone or offset specified by a time is always used to convert to UTC time. Time zones are only used if there is no offset or time zone specifier in a text date and time string. If a time zone supports time changes between daylight and standard times, the text file source might contain invalid or ambiguous datetimes. These are usually only possible for a two hour period each year because they occur during time changes. Ambiguous times are reported as standard times."
                                                                               },
    "content/main/shared-content/configuration/text-parser/xpath-and-csv-syntax-for-value-retrieval.html":  {
                                                                                                                "href":  "content/main/shared-content/configuration/text-parser/xpath-and-csv-syntax-for-value-retrieval.html",
                                                                                                                "title":  "XPath and CSV syntax for value retrieval",
                                                                                                                "keywords":  "XPath and CSV syntax for value retrieval For information on which semantics are used for retrieving values from XML and CSV files, see the following documentation: XML - XML Path Language (XPath) CSV - Column Index (1 based) or Header value (if header defined) The following syntaxes are used to extract values from XML or CSV documents. XML - Simple XPath example \u003cvalues\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:46.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:46.0928791Z\u003c time\u003e \u003cvalue\u003e1.234567890\u003c/value\u003e \u003cvalue\u003e1.234567890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:47.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:47.0928791Z\u003c time\u003e \u003cvalue\u003e12.34567890\u003c/value\u003e \u003cvalue\u003e12.34567890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:48.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:48.0928791Z\u003c time\u003e \u003cvalue\u003e123.4567890\u003c/value\u003e \u003cvalue\u003e123.4567890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:49.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:49.0928791Z\u003c time\u003e \u003cvalue\u003e1234.567890\u003c/value\u003e \u003cvalue\u003e1234.567890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:50.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:50.0928791Z\u003c time\u003e \u003cvalue\u003e12345.67890\u003c/value\u003e \u003cvalue\u003e12345.67890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:51.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:51.0928791Z\u003c time\u003e \u003cvalue\u003e123456.7890\u003c/value\u003e \u003cvalue\u003e123456.7890\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:52.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:52.0928791Z\u003c time\u003e \u003cvalue\u003e12345678.90\u003c/value\u003e \u003cvalue\u003e12345678.90\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003cvalue\u003e \u003ctime\u003e2020-08-10T12:10:53.0928791Z\u003c/time\u003e \u003ctime\u003e2020-08-10T12:10:53.0928791Z\u003c time\u003e \u003cvalue\u003e123456789.0\u003c/value\u003e \u003cvalue\u003e123456789.0\u003c value\u003e \u003c/value\u003e \u003c value\u003e \u003c/values\u003e \u003c values\u003e The following XPath configuration reads a series of values: { \"Id\": \"DoubleValue\", \"FieldDefinition\": \"./values/value/value\", \". values value value\", \"DataType\": \"Double\" }, { \"Id\": \"Timestamp\", \"FieldDefinition\": \"./values/value/time\", \". values value time\", \"DataType\": \"DateTime\", \"IsIndex\": true } CSV - Simple CSV column index example 2020-08-10T12:10:46.0928791Z,1.234567890 2020-08-10T12:10:47.0928791Z,12.34567890 2020-08-10T12:10:48.0928791Z,123.4567890 2020-08-10T12:10:49.0928791Z,1234.567890 2020-08-10T12:10:50.0928791Z,12345.67890 2020-08-10T12:10:51.0928791Z,123456.7890 2020-08-10T12:10:52.0928791Z,12345678.90 2020-08-10T12:10:53.0928791Z,123456789.0 The following CSV column index configuration requires the text parser be configured with HasHeader=false . The column indexes are 1 based and configured as strings. { \"Id\": \"DoubleValue\", \"FieldDefinition\": \"2\", \"DataType\": \"Double\" }, { \"Id\": \"Timestamp\", \"FieldDefinition\": \"1\", \"DataType\": \"DateTime\", \"IsIndex\": true } CSV - Simple CSV column header example Date,Value 2020-08-10T12:10:46.0928791Z,1.234567890 2020-08-10T12:10:47.0928791Z,12.34567890 2020-08-10T12:10:48.0928791Z,123.4567890 2020-08-10T12:10:49.0928791Z,1234.567890 2020-08-10T12:10:50.0928791Z,12345.67890 2020-08-10T12:10:51.0928791Z,123456.7890 2020-08-10T12:10:52.0928791Z,12345678.90 2020-08-10T12:10:53.0928791Z,123456789.0 The following CSV column header configuration requires the text parser be configured with HasHeader=true . { \"Id\": \"DoubleValue\", \"FieldDefinition\": \"Value\", \"DataType\": \"Double\" }, { \"Id\": \"Timestamp\", \"FieldDefinition\": \"Date\", \"DataType\": \"DateTime\", \"IsIndex\": true }"
                                                                                                            },
    "content/main/shared-content/diagnostics/adapter-diagnostics.html":  {
                                                                             "href":  "content/main/shared-content/diagnostics/adapter-diagnostics.html",
                                                                             "title":  "Adapter diagnostics",
                                                                             "keywords":  "Adapter diagnostics The adapter and its components produce different kinds of diagnostics data that is sent to all health endpoints. The System_Diagnostics.json file contains a flag that determines whether diagnostics are enabled. You can change this at runtime through REST calls or the EdgeCmd utility. Diagnostics data are collected by default. To egress diagnostics related data, you have to configure an adapter health endpoint first. See Health endpoint configuration . Available diagnostics data Every minute, dynamic data is sent to configured health endpoints. The following diagnostics data are available: System Stream count IO rate Error rate AF structure After running diagnostics with a health endpoint configured to a PI server, you can use PI System Explorer to view the diagnostics for a given adapter. The element hierarchy is shown in the following image. The Elements root contains a link to an Adapters node. This is the root node for all adapter instances. Below Adapters , you will find one or more adapter nodes. Each node\u0027s title is defined by the node\u0027s corresponding computer name and service name in this format: {ComputerName}.{ServiceName} . For example, in the following image, MachineName is the computer name and OpcUa is the service name. To see the System.Diagnostics values, click on an adapter node and set the tab to Attributes . Example values are shown in the image."
                                                                         },
    "content/main/shared-content/diagnostics/egress-diagnostics.html":  {
                                                                            "href":  "content/main/shared-content/diagnostics/egress-diagnostics.html",
                                                                            "title":  "Egress diagnostics",
                                                                            "keywords":  "Egress diagnostics The Egress component of the adapter produces the following diagnostics stream: IO rate The Diagnostics.Egress.IORate dynamic type includes the following values, which are logged in a stream with the Id {machineName}.{serviceName}.OmfEgress.{EndpointId}.IORate . IORate includes only sequential data successfully sent to an egress endpoint. Property Type Description timestamp string Timestamp of event IORate double One-minute rolling average of data rate (streams/second) (streams second)"
                                                                        },
    "content/main/shared-content/diagnostics/error-rate.html":  {
                                                                    "href":  "content/main/shared-content/diagnostics/error-rate.html",
                                                                    "title":  "Error rate",
                                                                    "keywords":  "Error rate The Diagnostics.Adapter.ErrorRate dynamic type includes the following values, which are logged in a stream with the Id {componentid}.ErrorRate . Property Type Description timestamp string Timestamp of event ErrorRate double One-minute rolling average of error rate (streams/second) (streams second)"
                                                                },
    "content/main/shared-content/diagnostics/io-rate.html":  {
                                                                 "href":  "content/main/shared-content/diagnostics/io-rate.html",
                                                                 "title":  "IO rate",
                                                                 "keywords":  "IO rate The Diagnostics.Adapter.IORate dynamic type includes the following values, which are logged in a stream with the Id {componentid}.IORate . IORate includes only sequential data collected from a data source. Property Type Description timestamp string Timestamp of event IORate double One-minute rolling average of data rate (streams/second) (streams second)"
                                                             },
    "content/main/shared-content/diagnostics/stream-count.html":  {
                                                                      "href":  "content/main/shared-content/diagnostics/stream-count.html",
                                                                      "title":  "Stream count",
                                                                      "keywords":  "Stream count The Diagnostics.StreamCountEvent dynamic type includes the following values, which are logged in a stream with the Id {componentid}.StreamCount . The StreamCount and TypeCount include only types and streams created for sequential data received from a data source. Property Type Description timestamp string Timestamp of event StreamCount int Number of streams created by the adapter instance TypeCount int Number of types created by the adapter instance"
                                                                  },
    "content/main/shared-content/diagnostics/system.html":  {
                                                                "href":  "content/main/shared-content/diagnostics/system.html",
                                                                "title":  "System",
                                                                "keywords":  "System The Diagnostics.System dynamic type includes the following values which are logged in a stream with the Id System.Diagnostics . This diagnostic stream contains system level information related to the host platform that the adapter is running on. Property Type Description timestamp string Timestamp of event ProcessIdentifier int Process Id of the host process StartTime string Time at which the host process started WorkingSet long Amount of physical memory in bytes, allocated for the host process TotalProcessorTime double Total processor time for the host process expressed in seconds TotalUserProcessorTime double User processor time for the host process expressed in seconds TotalPrivilegedProcessorTime double Privileged processor time for the host process expressed in seconds ThreadCount int Number of threads in the host process HandleCount int Number of handles opened by the host process ManagedMemorySize double Number of bytes currently thought to be allocated in managed memory Unit of Measure = megabytes PrivateMemorySize double Amount of paged memory in bytes allocated for the host process Unit of Measure = megabytes PeakPagedMemorySize double Maximum amount of memory in the virtual memory paging file in bytes used by the host process. Unit of Measure = megabytes StorageTotalSize double Total size of the storage medium in use by the system Unit of Measure = megabytes StorageFreeSpace double Free space available Unit of Measure = megabytes Each adapter component produces its own diagnostics streams."
                                                            },
    "content/main/shared-content/health/adapter-health.html":  {
                                                                   "href":  "content/main/shared-content/health/adapter-health.html",
                                                                   "title":  "Adapter health",
                                                                   "keywords":  "Adapter health PI Adapters produce different kinds of health data that can be egressed to different health endpoints. To egress health related data, you have to configure an adapter health endpoint first. See Health endpoint configuration . Available health data Dynamic data is sent every minute to configured health endpoints. The following health data is available: Device status Next Health Message Expected AF structure With a health endpoint configured to a PI server, you can use PI System Explorer to view the health of a given adapter. The element hierarchy is shown in the following image."
                                                               },
    "content/main/shared-content/health/device-status.html":  {
                                                                  "href":  "content/main/shared-content/health/device-status.html",
                                                                  "title":  "Device status",
                                                                  "keywords":  "Device status The device status indicates the health of this component and if it is currently communicating properly with the data source. This time-series data is stored within a PI point or OCS stream, depending on the endpoint type. During healthy steady-state operation, a value of Good is expected. Property Type Description Time string Timestamp of the event DeviceStatus string The value of the DeviceStatus The possible statuses are: Status Meaning Good The component is connected to the data source and it is collecting data. ConnectedNoData The component is connected to the data source but it is not receiving data from it. AttemptingFailover The adapter is attempting to fail over. Starting The component is currently in the process of starting up and is not yet connected to the data source. DeviceInError The component encountered an error either while connecting to the data source or attempting to collect data. Shutdown The component is either in the process of shutting down or has finished. Removed The adapter component has been removed and will no longer collect data. NotConfigured The adapter component has been created but is not yet configured."
                                                              },
    "content/main/shared-content/health/health-and-diagnostics.html":  {
                                                                           "href":  "content/main/shared-content/health/health-and-diagnostics.html",
                                                                           "title":  "Health and Diagnostics",
                                                                           "keywords":  "Health and Diagnostics PI Adapters produce various types of health data. You can use health data to ensure that your adapters are running properly and that data flows to the configured OMF endpoints. For more information on available health data, see Adapter health . PI Adapters also produce diagnostic data. You can use diagnostic data to find more information about a particular adapter instance. Diagnostic data lives alongside the health data and you can egress it using a health endpoint and setting EnableDiagnostics to true . You can configure EnableDiagnostics in the system\u0027s General configuration . For more information on available diagnostics data, see Adapter diagnostics . Health endpoint differences Two OMF endpoints are currently supported for adapter health data: PI Web API OSIsoft Cloud Services There are a few differences in how these two systems treat the associated health and diagnostics data. PI Web API parses the information and sends it to configured PI servers for the OMF endpoint. The static data is used to create a hierarchy on a PI AF server similar to the following example: The dynamic health data is time-series data that is stored in PI points on a PI Data Archive. You can see it in the AF hierarchy as PI point data reference attributes. OSIsoft Cloud Services does not currently provide a way to store the static metadata. For OCS-based adapter health endpoints, only the dynamic data is stored. Each value is its own stream with the timestamp property as the single index."
                                                                       },
    "content/main/shared-content/health/next-health-message-expected.html":  {
                                                                                 "href":  "content/main/shared-content/health/next-health-message-expected.html",
                                                                                 "title":  "Next health message expected",
                                                                                 "keywords":  "Next health message expected This property is similar to a heartbeat. A new value for NextHealthMessageExpected is sent by an individual adapter data component on a periodic basis while it is functioning properly. This value is a timestamp that indicates when the next value should be received. When monitoring, if the next value is not received by the indicated time, this likely means that there is an issue. It could be an issue with the adapter, adapter component, network connection between the health endpoint and the adapter, and so on. Property Type Description Time string Timestamp of the event NextHealthMessageExpected string Timestamp when next value is expected"
                                                                             },
    "content/main/shared-content/installation/installation.html":  {
                                                                       "href":  "content/main/shared-content/installation/installation.html",
                                                                       "title":  "Installation",
                                                                       "keywords":  "Installation Adapters are installed on a local machine using an install kit downloaded from the OSIsoft Customer Portal. For instructions on downloading and installing adapters, see Install the adapter . Alternatively, you can build custom installers or containers for Linux. For more information, see the Docker instructions in the documentation of the respective adapter."
                                                                   },
    "content/main/shared-content/installation/install-the-adapter.html":  {
                                                                              "href":  "content/main/shared-content/installation/install-the-adapter.html",
                                                                              "title":  "Install the adapter",
                                                                              "keywords":  "Install the adapter You can install adapters on either a Windows or Linux operating system. Before installing the adapter, see the respective system requirements to ensure your machine is properly configured to provide optimum adapter operation. Windows Complete the following steps to install a PI adapter on a Windows computer: Download the Windows .msi file from the OSIsoft Customer portal (https://customers.osisoft.com/s/products) (https:  customers.osisoft.com s products) . Note: Customer login credentials are required to access the portal. Run the .msi file. Follow the setup wizard. You can change the installation folder or port number during setup. The default port number is 5590 . Optional: To verify the installation, run the following curl command with the port number that you specified during installation: curl http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration If you receive an error, wait a few seconds and try the script again. If the installation was successful, a JSON copy of the default system configuration is returned. Linux Complete the following steps to install a PI adapter on a Linux computer: Download the appropriate Linux distribution file from the OSIsoft Customer portal (https://customers.osisoft.com/s/products) (https:  customers.osisoft.com s products) . Note: Customer login credentials are required to access the portal. Open a terminal. Run the sudo apt install command. Examples : To install the Linux ARM Debian package, run the command sudo apt install ./{AdapterName}_linux-arm.deb . {AdapterName}_linux-arm.deb To install the Linux x64 package, run the command sudo apt install ./{AdapterName}_linux-x64.deb . {AdapterName}_linux-x64.deb Optional: To verify the installation, run the following curl command with the port number that you specified during installation: curl http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration If you receive an error, wait a few seconds and run the command again. If the installation was successful, a JSON copy of the default system configuration is returned."
                                                                          },
    "content/main/shared-content/installation/uninstall-the-adapter.html":  {
                                                                                "href":  "content/main/shared-content/installation/uninstall-the-adapter.html",
                                                                                "title":  "Uninstall the adapter",
                                                                                "keywords":  "Uninstall the adapter Complete the procedure corresponding to your specific operating system to uninstall the adapter: Windows To delete the PI adapter program files from a Windows device, use the Windows Control Panel uninstall application process. Note: The configuration, data, and log files are not deleted by the uninstall process. Optional: To delete data, configuration, and log files, delete the directory %ProgramData%\\OSIsoft\\Adapters\\AdapterName . This deletes all data processed by the adapter, in addition to the configuration and log files. Linux To delete PI Adapter software from a Linux device, open a terminal window and run the following command: sudo apt remove pi.adapter.\u003cAdapterName\u003e Optional: To delete data, configuration, and log files, run the following command: sudo rm -r /usr/share/OSIsoft/Adapters/\u003cAdapterName\u003e  usr share OSIsoft Adapters \u003cAdapterName\u003e This deletes all data processed by the adapter, in addition to the configuration and log files."
                                                                            },
    "content/main/shared-content/metadata/adapter-metadata.html":  {
                                                                       "href":  "content/main/shared-content/metadata/adapter-metadata.html",
                                                                       "title":  "Adapter metadata",
                                                                       "keywords":  "Adapter metadata If the metadataLevel is set to Low or higher in the General configuration , adapter streams created by the ingress components include the following metadata: Datasource: {ComponentId} AdapterType: {ComponentType} ComponentId corresponds to the adapter components\u0027 data source configured in the Components configuration . ComponentType corresponds to the adapter type. Metadata for health and diagnostics streams If you configure a health endpoint and enable metadata, they are included in the health streams ( Device status and Next health message expected ) together with ComponentId and ComponentType . If you enable diagnostics in General configuration , metadata are included in the diagnostics streams ( Stream count , IO rate , Error rate ) together with ComponentId and ComponentType . The adapter may also send its own stream metadata not including health and diagnostics streams. For more information about what custom metadata is included in each stream, see the user guide for your adapter. Note: Metadata is only sent for streams created by the ingress components. Currently, the only endpoint that persists sent metadata is OCS (OSIsoft Cloud Services)."
                                                                   },
    "content/main/shared-content/technical-support-and-feedback.html":  {
                                                                            "href":  "content/main/shared-content/technical-support-and-feedback.html",
                                                                            "title":  "Technical support and feedback",
                                                                            "keywords":  "Technical support and feedback OSIsoft provides several ways to report issues and provide feedback on PI Adapters. Technical support For technical assistance with PI Adapters, contact OSIsoft Technical Support through the OSIsoft Customer Portal . We can help you identify the problem, provide workarounds and address any concerns you may have. Remote access to your facilities may be necessary during the session. Note: You must have an account set up in the OSIsoft Customer Portal before you can open a case. If you do not have a portal account, see How to Get a Login to OSIsoft Customer Portal . Alternatively, call OSIsoft Technical Support at +1 510-297-5828. When you contact OSIsoft Technical Support, be prepared to provide this information: Product name, version, and build numbers Details about your computer platform (CPU type, operating system, and version number) Time that the difficulty started Log files at that time Details of any environment changes prior to the start of the issue Summary of the issue, including any relevant log files during the time the issue occurred \u003c!--To view a brief primer on PI Adapters, see the [PI Adapters playbook](https://customers.osisoft.com/s/knowledgearticle?knowledgeArticleUrl=Playbook-PI-adapters) playbook](https:  customers.osisoft.com s knowledgearticle?knowledgeArticleUrl=Playbook-PI-adapters) in the OSIsoft Customer Portal.--\u003e Product feedback To submit product feedback for PI Adapters, visit the PI Adapters feedback page . The product team at OSIsoft regularly monitors the page. Documentation feedback To submit documentation feedback for PI Adapters, send an email to documentation@osisoft.com . Be sure to include the following information with your feedback: Product name and version Documentation topic URL Details of the suggestion or error The technical documentation team at OSIsoft will review and respond to your feedback."
                                                                        },
    "content/main/shared-content/troubleshooting/troubleshoot-the-adapter.html":  {
                                                                                      "href":  "content/main/shared-content/troubleshooting/troubleshoot-the-adapter.html",
                                                                                      "title":  "Troubleshoot the adapter",
                                                                                      "keywords":  "Troubleshoot the adapter PI adapters provide features for troubleshooting issues related to connectivity, data flow, and configuration. Resources include adapter logs and the Wireshark troubleshooting tool . If you are still unable to resolve issues or need additional guidance, contact OSIsoft Technical Support through the OSIsoft Customer Portal . Note: Make sure to also check the troubleshooting information specific to your adapter in this user guide. Logs Messages from the System and OmfEgress logs provide information on the status of the adapter. For example, they show if a connection from the adapter to an egress endpoint exists. Perform the following steps to view the System and OmfEgress logs: Navigate to the logs directory: Windows: %ProgramData%\\OSIsoft\\Adapters\\\u003cAdapterName\u003e\\Logs Linux: /usr/share/OSIsoft/Adapters/\u003cAdapterName\u003e/Logs  usr share OSIsoft Adapters \u003cAdapterName\u003e Logs . Example: A successful connection to a PI Web API egress endpoint displays the following message in the OmfEgress log: 2020-11-02 11:08:51.870 -06:00 [Information] Data will be sent to the following OMF endpoint: Id: \u003comfegress id\u003e Endpoint: \u003cpi web api URL\u003e (note: the pi web api default port is 443) ValidateEndpointCertificate: \u003ctrue or false\u003e Optional: Change the log level of the adapter to receive more information and context. For more information, see Logging configuration . ASP .NET Core platform The ASP .NET Core platform log provides information from the Kestrel web server that hosts the application. The log will only be present in case EDS encounters heartbeat failures or if the adapter throughput is too high. Perform the following steps to spread the load among multiple adapters: Decrease the scan frequency. Lower the amount of data selection items. \u003c!--## PI Web API and OCS user documentation PI Web API and OCS user documentation provides troubleshooting information for \u003cplaceholder\u003e --\u003e Wireshark Wireshark is a protocol-specific troubleshooting tool that supports all current adapter protocols. Perform the following steps if you want to use Wireshark to capture traffic from the data source to the adapter or from the adapter to the OMF destination. Download Wireshark . Familiarize yourself with the tool and read the Wireshark user guide . Health and diagnostics egress to PI Web API The adapter sends health and diagnostics data to PI Web API; in some cases, conflicts may occur that are due to changes or perceived changes in PI Web API. For example, a 409 - Conflict error message displays if you upgrade your adapter version and the PI points do not match in the upgraded version. However, data is continued to be sent as long as containers are created, so buffering only starts if no containers are created. To resolve the conflict, perform the following steps: Stop the adapter. Delete the Health folder inside of the Buffers folder. Stop PI Web API. Delete the relevant adapter created AF structure. Delete the associated health and diagnostics PI points on any or all PI Data Archives created by PI Web API. Start PI Web API. Start the adapter. Adapter connection to egress endpoint Certain egress health information in both PI Web API and OCS show if an adapter connection to an egress endpoint exists. To verify an active connection, perform one of the following procedures: PI Web API connection Perform the following steps to determine if a connection to the PI Web API endpoint exists: Open PI Web API. Select the OmfEgress component of your adapter, for example GVAdapterUbuntu.\u003cAdapterName\u003e.OmfEgress . Make sure that the following PI points have been created for your egress endpoint: DeviceStatus NextHealthMessageExpected IORate OCS connection Perform the following steps to determine if a connection to the OCS endpoint exists: Open OCS. Click Sequential Data Store \u003e Streams . Makes sure that the following streams have been created for your egress endpoint: DeviceStatus NextHealthMessageExpected IORate TCP connection Perform the following steps to see all established TCP sessions in Linux: Open a terminal. Type the following command: ss -o state established -t -p Press Enter."
                                                                                  },
    "content/main/Templates/Markdown Release Notes template-V1.html":  {
                                                                           "href":  "content/main/Templates/Markdown Release Notes template-V1.html",
                                                                           "title":  "Release notes",
                                                                           "keywords":  "Release notes { Product version x.x } Overview \u003c!--Insert a brief description of your product here and a cross-reference for more information. If these release notes cover service packs or patches in addition to the major release numbers, briefly identify each version covered.--\u003e Fixes and enhancements \u003c!--*Remove this section if this is the first release of a product. If multiple releases are covered by this note, for example, if service packs and patches are added, these can either be sectioned by release, or, if lengthy, can have sub-pages per release.* --\u003e Fixes The following items were resolved in release { x.x.x.xxxx } - { mm/dd/yyyy mm dd yyyy } : \u003c!-- Use table style:--\u003e Item Description { work item # } { work item release note } Enhancements The following features were added in release { x.x.x.xxxx } - { mm/dd/yyyy mm dd yyyy } : \u003c!--*Use bullet style:*--\u003e { new feature } { new feature } \u003c!--OR *Use table style:*--\u003e Item Description { work item # } { feature description } Known issues The following problems and enhancements have been deferred until a future release. \u003c!--*Use bullets and tables as necessary (table format below).* --\u003e Item Description { item # } { problem/enhancement problem enhancement description } System requirements \u003c!--*Provide a cross-reference to the system requirements section. For example,*--\u003e Refer to System requirements . Installation and upgrade \u003c!--*Provide a cross-reference to the installation procedure. For example,*--\u003e Refer to Install the adapter . Uninstallation \u003c!--*Provide a cross-reference to the uninstallation procedure. For example,*--\u003e Refer to Uninstall the adapter . Security information and guidance OSIsoft???s commitment Because the PI System often serves as a barrier protecting control system networks and mission-critical infrastructure assets, OSIsoft is committed to (1) delivering a high-quality product and (2) communicating clearly what security issues have been addressed. This release of { product name } is the highest quality and most secure version of the { product name } released to date. OSIsoft\u0027s commitment to improving the PI System is ongoing, and each future version should raise the quality and security bar even further. Vulnerability communication The practice of publicly disclosing internally discovered vulnerabilities is consistent with the Common Industrial Control System Vulnerability Disclosure Framework developed by the Industrial Control Systems Joint Working Group (ICSJWG). Despite the increased risk posed by greater transparency, OSIsoft is sharing this information to help you make an informed decision about when to upgrade to ensure your PI System has the best available protection. For more information, refer to OSIsoft???s Ethical Disclosure Policy (https://www.osisoft.com/ethical-disclosure-policy) (https:  www.osisoft.com ethical-disclosure-policy) . To report a security vulnerability, refer to OSIsoft\u0027s Report a Security Vulnerability (https://www.osisoft.com/report-a-security-vulnerability) (https:  www.osisoft.com report-a-security-vulnerability) . Vulnerability scoring OSIsoft has selected the Common Vulnerability Scoring System (CVSS) to quantify the severity of security vulnerabilities for disclosure. To calculate the CVSS scores, OSIsoft uses the National Vulnerability Database (NVD) calculator maintained by the National Institute of Standards and Technology (NIST). OSIsoft uses Critical, High, Medium and Low categories to aggregate the CVSS Base scores. This removes some of the opinion related errors of CVSS scoring. As noted in the CVSS specification, Base score range from 0 for the lowest severity to 10 for the highest severity. Overview of new vulnerabilities found or fixed This section is intended to provide relevant security-related information to guide your installation or upgrade decision. OSIsoft is proactively disclosing aggregate information about the number and severity of { product name } security vulnerabilities that are fixed in this release. \u003c!--*Provide an overview of the types of security vulnerabilities fixed in this release*--\u003e \u003c!--*NOTE: If NO security vulnerabilities are identified in the current release, please use the following statement:*--\u003e No security-related information is applicable to this release \u003c!--*When vulnerabilities exist, product teams should decide which format works best specific to the release and/or and or is applicable. Two different samples are provided below.*--\u003e Sample A - For this release of the { product name } , { x number } of security vulnerability has been identified and fixed. Based on the CVSS scoring system this issue has been categorized as a High (7.0 ??? 8.9). This high-level security issue is network accessible and has been resolved in the { product release name } . To reduce exposure to this security issue, either limit access to the port used by the PI SQL products, or upgrade to the latest release. Sample A with OSIsoft sub-component - Based on the CVSS scoring system this issue has been categorized as a High (7.0 ??? 8.9). This high-level security issue has been resolved in { OSIsoft???s PI SDK version 1.1 sub-component } which has been packaged in this { PI ProcessBook 1998 release } . To reduce exposure to this security issue upgrade to the latest release. Sample A with 3rd Party sub-component - Based on the CVSS scoring system this issue has been categorized as a High (7.0 ??? 8.9). This high-level security issue has been resolved in the 3rd Party { OpenSSL sub-component } which has been packaged in this { PI Adapter for \u003cAdapterName\u003e 1.2 } release . To reduce exposure to this security issue upgrade to the latest release. Sample B - For this release of the { product name } , {x number} of security vulnerability has been identified and fixed. The table below provides an overview of the types and severity of the security fixes. Security vulnerabilities fixed in this release Severity category CVSS base score range Number of fixed vulnerabilities Critical 9.0-10 { quantity } High 7.0-8.9 { quantity } Medium 4.0-6.9 { quantity } Low 0-3.9 { quantity } Sample B with sub-components - For this release of the {product name} , {x number} of security vulnerability has been identified and fixed. The tables below provide an overview of the types and severity of the security fixes. { Some/All Some All } of the vulnerabilities were associated with {OSIsoft/3rd {OSIsoft 3rd Party} sub-component { name } which is packaged in this release. Summary of security vulnerabilities fixed in this release Severity category CVSS base score range Number of fixed vulnerabilities Critical 9.0-10 { quantity } High 7.0-8.9 { quantity } Medium 4.0-6.9 { quantity } Low 0-3.9 { quantity } Security vulnerabilities fixed in the {OSIsoft/3rd {OSIsoft 3rd Party} sub-component { name } Severity category CVSS base score range Number of fixed vulnerabilities Critical 9.0-10 { quantity } High 7.0-8.9 { quantity } Medium 4.0-6.9 { quantity } Low 0-3.9 { quantity } \u003c!--*Optional ???Microsoft Software Security Defenses topic below??? (applies to C++ projects)*--\u003e Microsoft software security defenses In addition to finding and fixing security bugs within the { product name } , it is equally critical that OSIsoft leverage security defenses provided by the Microsoft Visual C++ compiler that builds it and the Microsoft Windows operating system that runs it. Over the past decade, Microsoft has continually added new defenses and improved existing defenses with successive versions of the compiler and the operating system. To learn more about many of these key defenses, consult the Microsoft whitepaper Mitigating Software Vulnerabilities. Documentation overview \u003c!--*Remove this section if there is no documentation besides the documentation in which these release notes are included. For additional documentation, provide a brief description. Example:*--\u003e PI System Explorer User Guide: Provides an overview and explains the functions of the PI System Explorer interface. Technical support and resources \u003c!--*Provide a cross-reference to the Technical Support and feedback section. For example,*--\u003e Refer to Technical support and feedback ."
                                                                       },
    "content/main/test.html":  {
                                   "href":  "content/main/test.html",
                                   "title":  "",
                                   "keywords":  ""
                               },
    "content/pi-adapter-for-structured-data-files-overview/pi-adapter-for-structured-data-files-principles-of-operation.html":  {
                                                                                                                                    "href":  "content/pi-adapter-for-structured-data-files-overview/pi-adapter-for-structured-data-files-principles-of-operation.html",
                                                                                                                                    "title":  "PI Adapter for Structured Data Files principles of operation",
                                                                                                                                    "keywords":  "PI Adapter for Structured Data Files principles of operation The adapter\u0027s operations focus on data collection and stream creation. Adapter configuration For the adapter to start data collection, you need to configure the adapter by defining the following: Data source : Provide the data source from which the adapter should collect data. \u003c!---is this where you specify the input directory?---\u003e Data selection : Select items for which the adapter should gather data from the files. Logging : Set up the logging attributes to manage the adapter logging behavior. \u003c!---what about buffering, health endpoints, egress, general configuration (Why isn\u0027t that referred to as diagnostic configuration?), data filters? I know these are optional for data collection, but so is logging.---\u003e For more information, see PI Adapter for Structured Data Files data source configuration , PI Adapter for Structured Data Files data selection configuration , and Logging configuration . Data collection When the adapter starts, it scans for all files in the input directory that match the configured file name filter. These files are processed in the order of their creation time \u003c!---oldest first?---\u003e . As new files are added to the input directory while the adapter is running, the files are processed in the order that they are added. Renaming a file will result in it being moved to the end of the processing queue. After a file has been processed, it will be moved to the output directory. Notes: The following situations could adversely affect data collection: Opening a file in the input directory could result in the adapter being unable to open it or move it to the output directory. Therefore, it is recommended that files are not opened after being placed in the input directory. If the adapter is unable to move a file to the output directory after processing it, the file will be processed again on an adapter restart unless it is manually removed from the input directory. If the input directory is deleted while the adapter is running, the adapter will attempt to resume data collection once the directory is recreated. In some cases, such as when the input directory is deleted and recreated within a short period of time, you may need to restart the adapter for data collection to continue. Supported file types The adapter supports files that are in CSV, JSON, or XML format. The raw data files can be uncompressed or compressed as a zip, gzip, tar, or tar.gzip files. The files can have UTF-8, ASCII, or Unicode encoding. Note: Compression is only supported at the individual file level. The adapter does not support compressed archives that contain multiple files. Supported data types The following table lists value data types that the adapter supports for data collection and types of streams that will be created. Value data type Stream data type SByte Int16 Byte Int16 Int16 Int16 UInt16 UInt16 Int32 Int32 UInt32 UInt32 Int64 Int64 UInt64 UInt64 Single Single Double Double Decimal Single DateTime DateTime String String Stream creation The adapter creates a stream with two properties for a selected item. The properties are described in the following table: Property name Data type Description Timestamp DateTime Timestamp of the given item update. Value Specified on the type of incoming value Value of the item update. Note: If streams are deleted from an endpoint while the adapter is running, the ingress component must be restarted to recreate the streams. See Start and stop ingress component . Certain metadata are sent with each stream created. The following metadata are common for every adapter type: ComponentId : Specifies the data source, for example, StructuredDataFiles1 ComponentType : Specifies the type of adapter, for example, StructuredDataFiles Metadata specific to PI Adapter for Structured Data Files: InputDirectory : Location of the source files to process. The metadata level is set in General configuration . For PI Adapter for Structured Data Files, the following metadata is sent for the individual level: None : No metadata Low : AdapterType ( ComponentType ) and DataSource ( ComponentId ) Medium : AdapterType ( ComponentType ) and DataSource ( ComponentId ) High : AdapterType ( ComponentType ), DataSource ( ComponentId ), InputDirectory Each stream created for a given item has a unique identifier (stream ID). If you specify a custom stream ID for the item in data selection configuration, the adapter uses that stream ID to create the stream. Otherwise, the adapter constructs the stream ID with the following format constructed from the data source\u0027s FriendlyName and the item\u0027s ValueField : \u003cAdapter Component ID\u003e.\u003cFriendlyName\u003e.\u003cValueField\u003e Note: The naming convention is affected by StreamIdPrefix and DefaultStreamIdPattern settings in data source configuration. For more information, see PI Adapter for Structured Data Files data source configuration ."
                                                                                                                                },
    "content/release-notes/release-notes.html":  {
                                                     "href":  "content/release-notes/release-notes.html",
                                                     "title":  "Release notes",
                                                     "keywords":  "Release notes PI Adapter for Structured Data Files 0.1.0.56 Adapter framework 1.3 Overview This represents the initial release for PI Adapter for Structured Data Files. This product collects time series data from source files in a local or remote directory to OMF endpoints in OSIsoft Cloud Services, Edge Data Store, or PI Servers. PI Adapter for Structured Data Files can also collect health and diagnostics information. It supports buffering, static and event data collection, and various Windows and Linux-based operating systems as well as containerization. Note: FTP servers are not supported for this version. For more information, see the PI Adapter for Structured Data Files overview . Known issues On Linux installs, the adapter currently cannot use file shares as an input directory. Workaround : Use an external process or script to copy or move the files locally on the machine where the adapter is running. System requirements Refer to System requirements . Installation Refer to Install the adapter . Uninstallation Refer to Uninstall the adapter . Security information and guidance OSIsoft\u0027s commitment Because the PI System often serves as a barrier protecting control system networks and mission-critical infrastructure assets, OSIsoft is committed to (1) delivering a high-quality product and (2) communicating clearly what security issues have been addressed. This release of PI Adapter for Structured Data Files is the highest quality and most secure version of the PI Adapter for Structured Data Files released to date. OSIsoft\u0027s commitment to improving the PI System is ongoing, and each future version should raise the quality and security bar even further. Vulnerability communication The practice of publicly disclosing internally discovered vulnerabilities is consistent with the Common Industrial Control System Vulnerability Disclosure Framework developed by the Industrial Control Systems Joint Working Group (ICSJWG) . Despite the increased risk posed by greater transparency, OSIsoft is sharing this information to help you make an informed decision about when to upgrade to ensure your PI System has the best available protection. For more information, refer to OSIsoft\u0027s Ethical Disclosure Policy (https://www.osisoft.com/ethical-disclosure-policy) (https:  www.osisoft.com ethical-disclosure-policy) . To report a security vulnerability, refer to OSIsoft\u0027s Report a Security Vulnerability (https://www.osisoft.com/report-a-security-vulnerability) (https:  www.osisoft.com report-a-security-vulnerability) . Vulnerability scoring OSIsoft has selected the Common Vulnerability Scoring System (CVSS) to quantify the severity of security vulnerabilities for disclosure. To calculate the CVSS scores, OSIsoft uses the National Vulnerability Database (NVD) calculator maintained by the National Institute of Standards and Technology (NIST). OSIsoft uses Critical, High, Medium and Low categories to aggregate the CVSS Base scores. This removes some of the opinion related errors of CVSS scoring. As noted in the CVSS specification, Base score range from 0 for the lowest severity to 10 for the highest severity. Overview of new vulnerabilities found or fixed No additional security vulnerabilities are applicable to this release. Sub-components of this release contain known vulnerabilities which are not exploitable in PI Adapter for Structured Data Files. The following table lists the known vulnerabilities and their mitigation in this product. Component Version CVE or Reference CVSS Mitigation json.Net 12.0.3 Applications that use Newtonsoft.Json might be exposed to DOS vulnerability 6.8 Limit MaxDepth when parsing to 100. No code paths result in json parsing and subsequent serialization directly back to a string. Technical support and resources Refer to Technical support and feedback ."
                                                 },
    "README.html":  {
                        "href":  "README.html",
                        "title":  "PI-Adapter-for-Structured-Data-Files-Docs",
                        "keywords":  "PI-Adapter-for-Structured-Data-Files-Docs PI Adapter for Structured Data Files is a data-collection component that transfers time-series data from source devices to OMF (OSIsoft Message Format) endpoints in OSIsoft Cloud Services or PI Servers. This repository contains the documentation for PI Adapter for Structured Data Files. License ?? 2020 - 2021 OSIsoft, LLC. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at: http://www.apache.org/licenses/LICENSE-2.0 http:  www.apache.org licenses LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
                    }
}
